#!/usr/bin/env python3
"""
ğŸ§ª GPRO Integration Test: Complete System Test

Tests the entire GPRO pipeline from model initialization to comparison results.
"""

import os
import sys
import logging
from pathlib import Path

# Add gpro to path
sys.path.append(str(Path(__file__).parent))

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_basic_imports():
    """ê¸°ë³¸ ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸"""
    print("ğŸ”§ ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸...")
    
    try:
        from gpro import (
            GPROModel, GPROConfig, initialize_gpro_model,
            OpenAIValidator, ValidationResult,
            HumanFeedbackSimulator, FeedbackData, create_sample_candidates,
            PreferenceOptimizer, OptimizationConfig,
            ModelComparison, PerformanceMetrics
        )
        print("âœ… ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ!")
        return True
    except ImportError as e:
        print(f"âŒ Import ì‹¤íŒ¨: {e}")
        return False


def test_gpro_model():
    """GPRO ëª¨ë¸ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ GPRO ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    
    try:
        from gpro import initialize_gpro_model
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        model = initialize_gpro_model()
        print("âœ… GPRO ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
        
        # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
        test_candidate = """
        ê¹€AIëŠ” ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ ë°•ì‚¬ë¡œ,
        êµ¬ê¸€ì—ì„œ 5ë…„ê°„ AI ì—°êµ¬ì›ìœ¼ë¡œ ê·¼ë¬´í–ˆìŠµë‹ˆë‹¤.
        ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ ë…¼ë¬¸ 50í¸ ë°œí‘œ.
        """
        
        result = model.predict_with_explanation(test_candidate, "AIì •ì±…ê´€")
        
        print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼: {result['prediction']}")
        print(f"âœ… ì‹ ë¢°ë„: {result['confidence']:.3f}")
        print(f"âœ… í—Œë²•ì  ì ìˆ˜: {result['constitutional_score']:.3f}")
        print(f"âœ… ì£¼ìš” ìš”ì†Œ: {result['reasoning']['top_factor']}")
        
        return True, model
        
    except Exception as e:
        print(f"âŒ GPRO ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False, None


def test_human_feedback():
    """ì¸ê°„ í”¼ë“œë°± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ‘¥ ì¸ê°„ í”¼ë“œë°± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
    
    try:
        from gpro import HumanFeedbackSimulator, create_sample_candidates
        
        # ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”
        simulator = HumanFeedbackSimulator()
        print("âœ… í”¼ë“œë°± ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™” ì„±ê³µ")
        
        # ìƒ˜í”Œ í›„ë³´ì
        candidates = create_sample_candidates()
        print(f"âœ… ìƒ˜í”Œ í›„ë³´ì {len(candidates)}ëª… ìƒì„±")
        
        # í”¼ë“œë°± ìƒì„±
        test_ai_rec = {
            'prediction': 'ì¶”ì²œ',
            'confidence': 0.85,
            'reasoning': {'top_factor': 'technical'},
            'detailed_analysis': {
                'technical_score': 0.9,
                'policy_score': 0.7,
                'leadership_score': 0.8,
                'collaboration_score': 0.75
            }
        }
        
        feedback = simulator.generate_expert_feedback(
            candidates[0]['info'],
            candidates[0]['position'],
            test_ai_rec
        )
        
        print(f"âœ… ì „ë¬¸ê°€ í”¼ë“œë°± ìƒì„±: {feedback.expert_id}")
        print(f"âœ… ì„ í˜¸ë„ ê°•ë„: {feedback.preference_strength:.3f}")
        print(f"âœ… ì‹ ë¢°ë„: {feedback.confidence:.3f}")
        
        return True, [feedback]
        
    except Exception as e:
        print(f"âŒ ì¸ê°„ í”¼ë“œë°± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False, None


def test_openai_validator():
    """OpenAI ê²€ì¦ì í…ŒìŠ¤íŠ¸ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)"""
    print("\nğŸ¤– OpenAI ê²€ì¦ì í…ŒìŠ¤íŠ¸...")
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ì¦ì í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True, None
    
    try:
        from gpro import OpenAIValidator
        
        validator = OpenAIValidator(api_key=api_key)
        print("âœ… OpenAI ê²€ì¦ì ì´ˆê¸°í™” ì„±ê³µ")
        
        # ê°„ë‹¨í•œ ê²€ì¦ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ API í˜¸ì¶œ ì—†ì´)
        print("âœ… OpenAI ê²€ì¦ì ì¤€ë¹„ ì™„ë£Œ (ì‹¤ì œ API í…ŒìŠ¤íŠ¸ëŠ” ì„ íƒì‚¬í•­)")
        
        return True, validator
        
    except Exception as e:
        print(f"âŒ OpenAI ê²€ì¦ì í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False, None


def test_model_comparison(gpro_model):
    """ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“Š ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
    
    try:
        from gpro import ModelComparison, create_sample_test_cases
        
        # ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        comparison = ModelComparison(gpro_model)
        print("âœ… ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì„±ê³µ")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€
        test_cases = create_sample_test_cases()
        for case in test_cases:
            comparison.add_test_case(
                case['candidate_info'],
                case['position'],
                case['ground_truth']
            )
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {len(test_cases)}ê°œ ì¶”ê°€")
        
        # ë¹„êµ ì‹¤í–‰
        results = comparison.run_comprehensive_comparison()
        print("âœ… í¬ê´„ì  ë¹„êµ ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“‹ ë¹„êµ ê²°ê³¼:")
        for recommendation in results['recommendations']:
            print(f"  {recommendation}")
        
        return True, results
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False, None


def test_preference_optimization(gpro_model, feedback_data):
    """ì„ í˜¸ë„ ìµœì í™” í…ŒìŠ¤íŠ¸"""
    print("\nâš¡ ì„ í˜¸ë„ ìµœì í™” í…ŒìŠ¤íŠ¸...")
    
    if not feedback_data:
        print("âš ï¸ í”¼ë“œë°± ë°ì´í„°ê°€ ì—†ì–´ ìµœì í™” í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True
    
    try:
        from gpro import PreferenceOptimizer, OptimizationConfig
        
        # ìµœì í™” ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ê°„ë‹¨íˆ)
        config = OptimizationConfig(
            learning_rate=1e-5,
            batch_size=1,
            num_epochs=1,
            logging_steps=1
        )
        
        # ì˜µí‹°ë§ˆì´ì € ìƒì„±
        optimizer = PreferenceOptimizer(gpro_model, config)
        print("âœ… ì„ í˜¸ë„ ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì„±ê³µ")
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        train_dataset = optimizer.prepare_dataset(feedback_data)
        print(f"âœ… í›ˆë ¨ ë°ì´í„°ì…‹ ì¤€ë¹„: {len(train_dataset)}ê±´")
        
        print("âœ… ì„ í˜¸ë„ ìµœì í™” ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
        print("   (ì‹¤ì œ í›ˆë ¨ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ í…ŒìŠ¤íŠ¸ì—ì„œ ì œì™¸)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì„ í˜¸ë„ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


def run_integration_test():
    """ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª GPRO í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    print("=" * 60)
    
    results = {
        'imports': False,
        'gpro_model': False,
        'human_feedback': False,
        'openai_validator': False,
        'model_comparison': False,
        'preference_optimization': False
    }
    
    # 1. ê¸°ë³¸ Import í…ŒìŠ¤íŠ¸
    results['imports'] = test_basic_imports()
    if not results['imports']:
        print("\nâŒ ê¸°ë³¸ importì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜ì¡´ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")
        return results
    
    # 2. GPRO ëª¨ë¸ í…ŒìŠ¤íŠ¸
    success, gpro_model = test_gpro_model()
    results['gpro_model'] = success
    
    # 3. ì¸ê°„ í”¼ë“œë°± í…ŒìŠ¤íŠ¸
    success, feedback_data = test_human_feedback()
    results['human_feedback'] = success
    
    # 4. OpenAI ê²€ì¦ì í…ŒìŠ¤íŠ¸
    success, validator = test_openai_validator()
    results['openai_validator'] = success
    
    # 5. ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸
    if gpro_model:
        success, comparison_results = test_model_comparison(gpro_model)
        results['model_comparison'] = success
    
    # 6. ì„ í˜¸ë„ ìµœì í™” í…ŒìŠ¤íŠ¸
    if gpro_model and feedback_data:
        results['preference_optimization'] = test_preference_optimization(
            gpro_model, feedback_data
        )
    
    print("\n" + "=" * 60)
    print("ğŸ¯ í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    print("=" * 60)
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"{test_name:<25}: {status}")
    
    print(f"\nì´ í…ŒìŠ¤íŠ¸: {total_tests}")
    print(f"í†µê³¼: {passed_tests}")
    print(f"ì‹¤íŒ¨: {total_tests - passed_tests}")
    print(f"ì„±ê³µë¥ : {passed_tests/total_tests*100:.1f}%")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
        print("GPRO ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    elif passed_tests >= total_tests * 0.8:
        print("\nâœ… ëŒ€ë¶€ë¶„ì˜ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
        print("ì¼ë¶€ ê¸°ëŠ¥ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ í™•ì¸í•´ë³´ì„¸ìš”.")
    else:
        print("\nâš ï¸ ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ì˜ì¡´ì„±ì´ë‚˜ ì„¤ì •ì„ í™•ì¸í•´ë³´ì„¸ìš”.")
    
    print("\nğŸ“š ì°¸ê³ ì‚¬í•­:")
    print("- OpenAI ê²€ì¦ì€ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤")
    print("- ì‹¤ì œ ëª¨ë¸ í›ˆë ¨ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤")
    print("- ì „ì²´ ê¸°ëŠ¥ì€ gpro/README.mdë¥¼ ì°¸ì¡°í•˜ì„¸ìš”")
    
    return results


if __name__ == "__main__":
    try:
        test_results = run_integration_test()
        
        # ì¢…ë£Œ ì½”ë“œ ì„¤ì •
        if all(test_results.values()):
            sys.exit(0)  # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ
        else:
            sys.exit(1)  # ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(2)
    except Exception as e:
        print(f"\n\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(3) 
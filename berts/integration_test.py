"""
ğŸš€ TRAS BERT ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
ê°•ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬í˜„ëœ ì™„ì „í•œ ì‹œìŠ¤í…œì˜ í†µí•© í…ŒìŠ¤íŠ¸

ì „ì²´ íŒŒì´í”„ë¼ì¸: í† í°í™” â†’ ì„ë² ë”© â†’ ì–´í…ì…˜ â†’ BERT â†’ ìµœì í™” â†’ ì‹ ë¢°ë„ í‰ê°€
"""

import sys
import os
import time
import torch
import logging
from typing import List, Dict, Any
import numpy as np

# ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from .tokenizer import KoreanTokenizer
    from .embedding import ContextualEmbedding
    from .attention import MultiHeadAttention
    from .bert_model import FastBERT, ReliableBERT
    from .optimizer import BERTOptimizer
    from .evaluator import TrustScoreCalculator
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ ì‹œ
    from tokenizer import KoreanTokenizer
    from embedding import ContextualEmbedding
    from attention import MultiHeadAttention
    from bert_model import FastBERT, ReliableBERT
    from optimizer import BERTOptimizer
    from evaluator import TrustScoreCalculator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TRASIntegrationTester:
    """
    ğŸ§ª TRAS BERT ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤í„°
    ì „ì²´ ì‹œìŠ¤í…œì˜ ë™ì‘ì„ ê²€ì¦í•˜ê³  ì„±ëŠ¥ì„ ì¸¡ì •
    """
    
    def __init__(self):
        self.test_results = {}
        self.performance_metrics = {}
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        self.test_cases = [
            {
                'text': "ê¹€ì² ìˆ˜ë¥¼ AI ì •ì±…ê´€ìœ¼ë¡œ ê°•ë ¥íˆ ì¶”ì²œí•©ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ ê²½í—˜ì´ í’ë¶€í•˜ê³  ì •ë¶€ ì—…ë¬´ì— ì í•©í•©ë‹ˆë‹¤.",
                'expected_position': 'ì •ì±…ê´€',
                'expected_recommendation': 'ê°•ë ¥ì¶”ì²œ'
            },
            {
                'text': "ì´ì˜í¬ëŠ” êµ­ì¥ê¸‰ ê³µë¬´ì›ìœ¼ë¡œ í–‰ì • ì—…ë¬´ì— ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ìœ í•˜ê³  ìˆì–´ ì¶”ì²œí•©ë‹ˆë‹¤.",
                'expected_position': 'êµ­ì¥',
                'expected_recommendation': 'ì¶”ì²œ'
            },
            {
                'text': "ë°•ë¯¼ìˆ˜ëŠ” ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ ë°•ì‚¬í•™ìœ„ë¥¼ ë³´ìœ í•˜ê³  ìˆì–´ ì •ë¶€ ë””ì§€í„¸ ì •ì±… ìˆ˜ë¦½ì— ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                'expected_position': 'ì •ì±…ê´€',
                'expected_recommendation': 'ì¶”ì²œ'
            },
            {
                'text': "ì •ë¶€ ì°¨ê´€ í›„ë³´ë¡œ ìµœê³ ì˜ ìê²©ì„ ê°–ì¶˜ ì „ë¬¸ê°€ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.",
                'expected_position': 'ì°¨ê´€',
                'expected_recommendation': 'ê°•ë ¥ì¶”ì²œ'
            },
            {
                'text': "ì¼ë°˜ì ì¸ ë¯¼ê°„ ê¸°ì—… ì—…ë¬´ ê²½í—˜ë§Œ ìˆì–´ì„œ ì •ë¶€ ì—…ë¬´ì—ëŠ” ì í•©í•˜ì§€ ì•Šì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
                'expected_position': None,
                'expected_recommendation': 'ë¹„ì¶”ì²œ'
            }
        ]
        
        logger.info("ğŸ§ª TRAS í†µí•© í…ŒìŠ¤í„° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def test_tokenizer(self) -> Dict[str, Any]:
        """í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”¤ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        tokenizer = KoreanTokenizer(max_length=128)
        results = {
            'success': True,
            'processing_times': [],
            'confidence_scores': [],
            'cache_performance': {}
        }
        
        try:
            for i, test_case in enumerate(self.test_cases):
                start_time = time.time()
                
                # í† í°í™” ì‹¤í–‰
                tokenization_result = tokenizer.tokenize(test_case['text'])
                
                processing_time = time.time() - start_time
                results['processing_times'].append(processing_time)
                results['confidence_scores'].append(tokenization_result.confidence_score)
                
                logger.info(f"   í…ŒìŠ¤íŠ¸ {i+1}: {processing_time:.4f}ì´ˆ, ì‹ ë¢°ë„: {tokenization_result.confidence_score:.3f}")
            
            # ìºì‹œ ì„±ëŠ¥ í™•ì¸
            results['cache_performance'] = tokenizer.get_cache_stats()
            
            # í†µê³„ ê³„ì‚°
            results['avg_processing_time'] = np.mean(results['processing_times'])
            results['avg_confidence'] = np.mean(results['confidence_scores'])
            
            logger.info(f"âœ… í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì™„ë£Œ - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {results['avg_processing_time']:.4f}ì´ˆ")
            
        except Exception as e:
            logger.error(f"âŒ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        return results
    
    def test_embedding(self) -> Dict[str, Any]:
        """ì„ë² ë”© ëª¨ë“ˆ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“Š ì„ë² ë”© ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        vocab_size = 10000
        d_model = 256
        embedding_model = ContextualEmbedding(vocab_size, d_model)
        
        results = {
            'success': True,
            'processing_times': [],
            'geometric_properties': []
        }
        
        try:
            for i, test_case in enumerate(self.test_cases):
                # ë”ë¯¸ ì…ë ¥ ìƒì„± (ì‹¤ì œë¡œëŠ” í† í¬ë‚˜ì´ì € ì¶œë ¥ ì‚¬ìš©)
                input_ids = torch.randint(0, vocab_size, (1, 50))
                attention_mask = torch.ones(1, 50)
                
                start_time = time.time()
                embedding_result = embedding_model(input_ids, attention_mask)
                processing_time = time.time() - start_time
                
                results['processing_times'].append(processing_time)
                results['geometric_properties'].append(embedding_result.geometric_properties)
                
                logger.info(f"   í…ŒìŠ¤íŠ¸ {i+1}: {processing_time:.4f}ì´ˆ, ì„ë² ë”© ì°¨ì›: {embedding_result.embeddings.shape}")
            
            results['avg_processing_time'] = np.mean(results['processing_times'])
            logger.info(f"âœ… ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {results['avg_processing_time']:.4f}ì´ˆ")
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        return results
    
    def test_attention(self) -> Dict[str, Any]:
        """ì–´í…ì…˜ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ‘ï¸ ì–´í…ì…˜ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        d_model = 256
        num_heads = 8
        attention_model = MultiHeadAttention(d_model, num_heads)
        
        results = {
            'success': True,
            'processing_times': [],
            'attention_stats': []
        }
        
        try:
            for i in range(len(self.test_cases)):
                # í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
                batch_size, seq_len = 1, 32
                test_input = torch.randn(batch_size, seq_len, d_model)
                attention_mask = torch.ones(batch_size, seq_len, seq_len)
                
                start_time = time.time()
                attention_result = attention_model(test_input, test_input, test_input, attention_mask)
                processing_time = time.time() - start_time
                
                results['processing_times'].append(processing_time)
                
                logger.info(f"   í…ŒìŠ¤íŠ¸ {i+1}: {processing_time:.4f}ì´ˆ, ì–´í…ì…˜ í˜•íƒœ: {attention_result.attention_weights.shape}")
            
            results['avg_processing_time'] = np.mean(results['processing_times'])
            logger.info(f"âœ… ì–´í…ì…˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {results['avg_processing_time']:.4f}ì´ˆ")
            
        except Exception as e:
            logger.error(f"âŒ ì–´í…ì…˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        return results
    
    def test_bert_model(self) -> Dict[str, Any]:
        """BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§  BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ì†Œê·œëª¨ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´)
        bert_model = FastBERT(
            vocab_size=5000,
            d_model=128,
            num_layers=2,
            num_heads=4,
            max_length=128
        )
        
        results = {
            'success': True,
            'processing_times': [],
            'confidence_scores': [],
            'predictions': []
        }
        
        try:
            for i, test_case in enumerate(self.test_cases):
                start_time = time.time()
                
                # BERT ëª¨ë¸ ì‹¤í–‰
                bert_output = bert_model(input_texts=[test_case['text']])
                
                processing_time = time.time() - start_time
                results['processing_times'].append(processing_time)
                results['confidence_scores'].append(bert_output.confidence_scores['overall_confidence'])
                results['predictions'].append(bert_output.government_predictions)
                
                # ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
                if bert_output.government_predictions:
                    best_position = max(
                        bert_output.government_predictions['positions'].items(),
                        key=lambda x: x[1]
                    )
                    best_recommendation = max(
                        bert_output.government_predictions['recommendation'].items(),
                        key=lambda x: x[1]
                    )
                    
                    logger.info(f"   í…ŒìŠ¤íŠ¸ {i+1}: {processing_time:.4f}ì´ˆ")
                    logger.info(f"      ì˜ˆì¸¡ ì§ì±…: {best_position[0]} ({best_position[1]:.3f})")
                    logger.info(f"      ì¶”ì²œ ë¶„ë¥˜: {best_recommendation[0]} ({best_recommendation[1]:.3f})")
                    logger.info(f"      ì‹ ë¢°ë„: {bert_output.confidence_scores['overall_confidence']:.3f}")
            
            results['avg_processing_time'] = np.mean(results['processing_times'])
            results['avg_confidence'] = np.mean(results['confidence_scores'])
            
            logger.info(f"âœ… BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {results['avg_processing_time']:.4f}ì´ˆ")
            
        except Exception as e:
            logger.error(f"âŒ BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        return results
    
    def test_reliable_bert(self) -> Dict[str, Any]:
        """ì‹ ë¢°ì„± í–¥ìƒ BERT í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ›¡ï¸ ReliableBERT í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ê¸°ë³¸ ëª¨ë¸ ìƒì„±
        base_bert = FastBERT(
            vocab_size=3000,
            d_model=64,
            num_layers=1,
            num_heads=2,
            max_length=64
        )
        
        # ì‹ ë¢°ì„± í–¥ìƒ ëª¨ë¸
        reliable_bert = ReliableBERT(base_bert, num_ensemble=2)
        
        results = {
            'success': True,
            'processing_times': [],
            'final_confidences': [],
            'decisions': []
        }
        
        try:
            for i, test_case in enumerate(self.test_cases):
                start_time = time.time()
                
                # ReliableBERT ì‹¤í–‰
                reliable_output = reliable_bert(input_texts=[test_case['text']])
                
                processing_time = time.time() - start_time
                results['processing_times'].append(processing_time)
                
                final_confidence = reliable_output['final_confidence']['final_confidence']
                decision = reliable_output['recommendation']['decision']
                
                results['final_confidences'].append(final_confidence)
                results['decisions'].append(decision)
                
                logger.info(f"   í…ŒìŠ¤íŠ¸ {i+1}: {processing_time:.4f}ì´ˆ")
                logger.info(f"      ìµœì¢… ì‹ ë¢°ë„: {final_confidence:.3f}")
                logger.info(f"      ìµœì¢… ê²°ì •: {decision}")
            
            results['avg_processing_time'] = np.mean(results['processing_times'])
            results['avg_final_confidence'] = np.mean(results['final_confidences'])
            
            logger.info(f"âœ… ReliableBERT í…ŒìŠ¤íŠ¸ ì™„ë£Œ - í‰ê·  ìµœì¢… ì‹ ë¢°ë„: {results['avg_final_confidence']:.3f}")
            
        except Exception as e:
            logger.error(f"âŒ ReliableBERT í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        return results
    
    def test_optimizer(self) -> Dict[str, Any]:
        """ìµœì í™” ëª¨ë“ˆ í…ŒìŠ¤íŠ¸"""
        logger.info("âš¡ ìµœì í™” ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ëª¨ë¸
        class SimpleTestModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = torch.nn.Linear(64, 32)
                self.relu = torch.nn.ReLU()
                self.output = torch.nn.Linear(32, 10)
            
            def forward(self, x):
                x = x.float()
                x = self.linear(x)
                x = self.relu(x)
                return self.output(x)
        
        test_model = SimpleTestModel()
        optimizer = BERTOptimizer(test_model)
        
        results = {
            'success': True,
            'optimization_result': None,
            'processing_performance': {}
        }
        
        try:
            # ìµœì í™” ì‹¤í–‰ (ì–‘ìí™” ì œì™¸, ìºì‹±ë§Œ)
            optimization_result = optimizer.optimize_model(['caching'])
            results['optimization_result'] = {
                'speedup_ratio': optimization_result.speedup_ratio,
                'memory_saved': optimization_result.memory_saved,
                'optimization_methods': optimization_result.optimization_methods
            }
            
            # ìµœì í™”ëœ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            test_data = [torch.randint(0, 100, (2, 64)) for _ in range(3)]
            optimized_results = optimizer.process_with_optimization(test_data)
            
            # ìµœì í™” ë¦¬í¬íŠ¸
            report = optimizer.get_optimization_report()
            results['processing_performance'] = {
                'cache_hit_rate': report['cache_stats']['hit_rate'],
                'current_batch_size': report['current_batch_size'],
                'recommendations': report['recommendations']
            }
            
            logger.info(f"âœ… ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            logger.info(f"   ì†ë„ í–¥ìƒ: {optimization_result.speedup_ratio:.2f}x")
            logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {optimization_result.memory_saved:.1%}")
            logger.info(f"   ìºì‹œ ì ì¤‘ë¥ : {report['cache_stats']['hit_rate']:.1%}")
            
            # ì •ë¦¬
            optimizer.cleanup()
            
        except Exception as e:
            logger.error(f"âŒ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        return results
    
    def test_trust_evaluator(self) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ¯ ì‹ ë¢°ë„ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        trust_calculator = TrustScoreCalculator()
        
        # ë”ë¯¸ ëª¨ë¸
        class DummyModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = torch.nn.Linear(32, 16)
            
            def forward(self, x):
                return self.linear(x.float())
        
        dummy_model = DummyModel()
        
        results = {
            'success': True,
            'trust_scores': [],
            'processing_times': [],
            'recommendations': []
        }
        
        try:
            for i, test_case in enumerate(self.test_cases):
                # ë”ë¯¸ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±
                test_predictions = {
                    'government_predictions': {
                        'positions': {
                            'ì •ì±…ê´€': 0.6, 'ê³¼ì¥': 0.2, 'êµ­ì¥': 0.1,
                            'ì°¨ê´€': 0.05, 'ì¥ê´€': 0.03, 'ëŒ€í†µë ¹': 0.01,
                            'ì´ë¦¬': 0.01, 'ë¹„ì„œê´€': 0.0, 'ë³´ì¢Œê´€': 0.0, 'ìˆ˜ì„': 0.0
                        },
                        'recommendation': {
                            'ê°•ë ¥ì¶”ì²œ': 0.5, 'ì¶”ì²œ': 0.4, 'ë¹„ì¶”ì²œ': 0.1
                        }
                    }
                }
                
                start_time = time.time()
                
                # ì‹ ë¢°ë„ í‰ê°€ ì‹¤í–‰
                trust_assessment = trust_calculator.calculate_trust_score(
                    model=dummy_model,
                    input_text=test_case['text'],
                    predictions=test_predictions
                )
                
                processing_time = time.time() - start_time
                
                results['trust_scores'].append(trust_assessment.overall_trust_score)
                results['processing_times'].append(processing_time)
                results['recommendations'].extend(trust_assessment.recommendations)
                
                logger.info(f"   í…ŒìŠ¤íŠ¸ {i+1}: {processing_time:.4f}ì´ˆ")
                logger.info(f"      ì „ì²´ ì‹ ë¢°ë„: {trust_assessment.overall_trust_score:.3f}")
                logger.info(f"      ë„ë©”ì¸ ì í•©ì„±: {trust_assessment.domain_relevance:.3f}")
            
            results['avg_trust_score'] = np.mean(results['trust_scores'])
            results['avg_processing_time'] = np.mean(results['processing_times'])
            
            logger.info(f"âœ… ì‹ ë¢°ë„ í‰ê°€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - í‰ê·  ì‹ ë¢°ë„: {results['avg_trust_score']:.3f}")
            
        except Exception as e:
            logger.error(f"âŒ ì‹ ë¢°ë„ í‰ê°€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results['success'] = False
            results['error'] = str(e)
        
        return results
    
    def run_full_integration_test(self) -> Dict[str, Any]:
        """ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ TRAS BERT ì‹œìŠ¤í…œ ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # ê° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_modules = [
            ('tokenizer', self.test_tokenizer),
            ('embedding', self.test_embedding),
            ('attention', self.test_attention),
            ('bert_model', self.test_bert_model),
            ('reliable_bert', self.test_reliable_bert),
            ('optimizer', self.test_optimizer),
            ('trust_evaluator', self.test_trust_evaluator)
        ]
        
        all_results = {}
        success_count = 0
        
        for module_name, test_func in test_modules:
            logger.info(f"\nğŸ“‹ {module_name.upper()} ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
            logger.info("-" * 40)
            
            try:
                result = test_func()
                all_results[module_name] = result
                
                if result['success']:
                    success_count += 1
                    logger.info(f"âœ… {module_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                else:
                    logger.error(f"âŒ {module_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                logger.error(f"âŒ {module_name} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                all_results[module_name] = {'success': False, 'error': str(e)}
        
        total_time = time.time() - start_time
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ TRAS BERT ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        logger.info(f"âœ… ì„±ê³µí•œ ëª¨ë“ˆ: {success_count}/{len(test_modules)}")
        logger.info(f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = self._generate_performance_summary(all_results)
        logger.info("\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        for metric, value in performance_summary.items():
            logger.info(f"   {metric}: {value}")
        
        # ê¶Œì¥ì‚¬í•­
        recommendations = self._generate_system_recommendations(all_results)
        if recommendations:
            logger.info("\nğŸ’¡ ì‹œìŠ¤í…œ ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(recommendations, 1):
                logger.info(f"   {i}. {rec}")
        
        return {
            'success_rate': success_count / len(test_modules),
            'total_time': total_time,
            'module_results': all_results,
            'performance_summary': performance_summary,
            'recommendations': recommendations
        }
    
    def _generate_performance_summary(self, results: Dict[str, Any]) -> Dict[str, str]:
        """ì„±ëŠ¥ ìš”ì•½ ìƒì„±"""
        summary = {}
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        processing_times = []
        for module_name, result in results.items():
            if result['success'] and 'avg_processing_time' in result:
                processing_times.append(result['avg_processing_time'])
        
        if processing_times:
            summary['í‰ê·  ì²˜ë¦¬ ì‹œê°„'] = f"{np.mean(processing_times):.4f}ì´ˆ"
        
        # ì‹ ë¢°ë„ ê´€ë ¨ ë©”íŠ¸ë¦­
        if 'bert_model' in results and results['bert_model']['success']:
            bert_result = results['bert_model']
            if 'avg_confidence' in bert_result:
                summary['BERT í‰ê·  ì‹ ë¢°ë„'] = f"{bert_result['avg_confidence']:.3f}"
        
        if 'trust_evaluator' in results and results['trust_evaluator']['success']:
            trust_result = results['trust_evaluator']
            if 'avg_trust_score' in trust_result:
                summary['í‰ê·  ì‹ ë¢°ë„ ì ìˆ˜'] = f"{trust_result['avg_trust_score']:.3f}"
        
        # ìµœì í™” ì„±ëŠ¥
        if 'optimizer' in results and results['optimizer']['success']:
            opt_result = results['optimizer']
            if 'optimization_result' in opt_result and opt_result['optimization_result']:
                opt_data = opt_result['optimization_result']
                summary['ìµœì í™” ì†ë„ í–¥ìƒ'] = f"{opt_data['speedup_ratio']:.2f}x"
        
        return summary
    
    def _generate_system_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ì‹œìŠ¤í…œ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì‹¤íŒ¨í•œ ëª¨ë“ˆ í™•ì¸
        failed_modules = [name for name, result in results.items() if not result['success']]
        if failed_modules:
            recommendations.append(f"ì‹¤íŒ¨í•œ ëª¨ë“ˆë“¤ì„ í™•ì¸í•˜ì„¸ìš”: {', '.join(failed_modules)}")
        
        # ì„±ëŠ¥ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        if 'bert_model' in results and results['bert_model']['success']:
            bert_result = results['bert_model']
            if 'avg_processing_time' in bert_result and bert_result['avg_processing_time'] > 1.0:
                recommendations.append("BERT ëª¨ë¸ ì²˜ë¦¬ ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤. ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ì‹ ë¢°ë„ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        if 'trust_evaluator' in results and results['trust_evaluator']['success']:
            trust_result = results['trust_evaluator']
            if 'avg_trust_score' in trust_result and trust_result['avg_trust_score'] < 0.7:
                recommendations.append("ì „ì²´ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ëª¨ë¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return recommendations

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ TRAS BERT ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸")
    print("ê°•ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬í˜„ëœ ì™„ì „í•œ ì‹œìŠ¤í…œì„ ê²€ì¦í•©ë‹ˆë‹¤.")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        "ê¹€ì² ìˆ˜ë¥¼ AI ì •ì±…ê´€ìœ¼ë¡œ ê°•ë ¥íˆ ì¶”ì²œí•©ë‹ˆë‹¤",
        "ì´ì˜í¬ëŠ” êµ­ì¥ê¸‰ ê³µë¬´ì›ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤"
    ]
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {len(test_cases)}ê°œ ì¤€ë¹„")
    print("ğŸ‰ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 
"""
âš¡ BERT ìµœì í™” ëª¨ë“ˆ
ê³ ì† ì¶”ë¡ ê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ìµœì í™” ê¸°ë²•ë“¤

- ëª¨ë¸ ì–‘ìí™” (Quantization)
- ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
- ìºì‹± ìµœì í™”
- ë³‘ë ¬ ì²˜ë¦¬
"""

import torch
import torch.nn as nn
import torch.quantization as quantization
import time
import psutil
import threading
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import logging
from concurrent.futures import ThreadPoolExecutor
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class OptimizationResult:
    """ìµœì í™” ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    original_time: float
    optimized_time: float
    speedup_ratio: float
    memory_saved: float
    accuracy_maintained: float
    optimization_methods: List[str]

class ModelQuantizer:
    """
    ğŸ“Š ëª¨ë¸ ì–‘ìí™”
    float32 â†’ int8 ë³€í™˜ìœ¼ë¡œ ì†ë„ì™€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    """
    
    @staticmethod
    def quantize_model(model: nn.Module, calibration_data: Optional[torch.Tensor] = None) -> nn.Module:
        """
        ëª¨ë¸ ì–‘ìí™” ìˆ˜í–‰
        
        Args:
            model: ì–‘ìí™”í•  ëª¨ë¸
            calibration_data: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°
            
        Returns:
            ì–‘ìí™”ëœ ëª¨ë¸
        """
        logger.info("ğŸ”§ ëª¨ë¸ ì–‘ìí™” ì‹œì‘...")
        
        # 1. ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        
        # 2. ì–‘ìí™” ì„¤ì •
        model.qconfig = quantization.get_default_qconfig('fbgemm')
        
        # 3. ì–‘ìí™” ì¤€ë¹„
        quantization.prepare(model, inplace=True)
        
        # 4. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì„ íƒì )
        if calibration_data is not None:
            with torch.no_grad():
                _ = model(calibration_data)
        
        # 5. ì–‘ìí™” ì ìš©
        quantized_model = quantization.convert(model, inplace=True)
        
        logger.info("âœ… ëª¨ë¸ ì–‘ìí™” ì™„ë£Œ")
        return quantized_model
    
    @staticmethod
    def measure_quantization_impact(
        original_model: nn.Module, 
        quantized_model: nn.Module,
        test_data: torch.Tensor
    ) -> Dict[str, float]:
        """ì–‘ìí™” ì˜í–¥ ì¸¡ì •"""
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        def get_model_size(model):
            param_size = 0
            buffer_size = 0
            for param in model.parameters():
                param_size += param.nelement() * param.element_size()
            for buffer in model.buffers():
                buffer_size += buffer.nelement() * buffer.element_size()
            return (param_size + buffer_size) / 1024 / 1024  # MB
        
        original_size = get_model_size(original_model)
        quantized_size = get_model_size(quantized_model)
        
        # ì¶”ë¡  ì†ë„ ì¸¡ì •
        def measure_inference_time(model, data, num_runs=10):
            model.eval()
            times = []
            with torch.no_grad():
                for _ in range(num_runs):
                    start = time.time()
                    _ = model(data)
                    times.append(time.time() - start)
            return np.mean(times)
        
        original_time = measure_inference_time(original_model, test_data)
        quantized_time = measure_inference_time(quantized_model, test_data)
        
        return {
            'size_reduction': (original_size - quantized_size) / original_size,
            'speed_improvement': (original_time - quantized_time) / original_time,
            'original_size_mb': original_size,
            'quantized_size_mb': quantized_size,
            'original_time_ms': original_time * 1000,
            'quantized_time_ms': quantized_time * 1000
        }

class DynamicBatchManager:
    """
    ğŸ“ˆ ë™ì  ë°°ì¹˜ í¬ê¸° ê´€ë¦¬
    ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ìë™ ì¡°ì •
    """
    
    def __init__(self, initial_batch_size: int = 8, max_batch_size: int = 64):
        self.current_batch_size = initial_batch_size
        self.max_batch_size = max_batch_size
        self.min_batch_size = 1
        self.performance_history = []
        self.memory_threshold = 0.85  # 85% ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        
    def get_optimal_batch_size(self) -> int:
        """í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœì— ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸
        memory_percent = psutil.virtual_memory().percent / 100.0
        
        if memory_percent > self.memory_threshold:
            # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            self.current_batch_size = max(
                self.min_batch_size, 
                self.current_batch_size // 2
            )
            logger.info(f"ğŸ”» ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ: {self.current_batch_size}")
        
        elif memory_percent < 0.6 and len(self.performance_history) > 0:
            # ë©”ëª¨ë¦¬ ì—¬ìœ  ì‹œ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ì‹œë„
            recent_performance = np.mean(self.performance_history[-5:])
            if recent_performance > 0:  # ì„±ëŠ¥ì´ ì¢‹ë‹¤ë©´
                self.current_batch_size = min(
                    self.max_batch_size,
                    self.current_batch_size * 2
                )
                logger.info(f"ğŸ”º ë©”ëª¨ë¦¬ ì—¬ìœ ë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€: {self.current_batch_size}")
        
        return self.current_batch_size
    
    def record_performance(self, processing_time: float, batch_size: int):
        """ì„±ëŠ¥ ê¸°ë¡ ë° í•™ìŠµ"""
        throughput = batch_size / processing_time  # samples/second
        self.performance_history.append(throughput)
        
        # ìµœê·¼ 10ê°œ ê¸°ë¡ë§Œ ìœ ì§€
        if len(self.performance_history) > 10:
            self.performance_history.pop(0)

class CacheManager:
    """
    ğŸ’¾ ì§€ëŠ¥í˜• ìºì‹œ ê´€ë¦¬
    ìì£¼ ì‚¬ìš©ë˜ëŠ” ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹œí•˜ì—¬ ë°˜ë³µ ê³„ì‚° ë°©ì§€
    """
    
    def __init__(self, max_cache_size: int = 1000):
        self.cache = {}
        self.access_count = {}
        self.max_size = max_cache_size
        self.hit_count = 0
        self.miss_count = 0
        
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        if key in self.cache:
            self.hit_count += 1
            self.access_count[key] = self.access_count.get(key, 0) + 1
            return self.cache[key]
        
        self.miss_count += 1
        return None
    
    def put(self, key: str, value: Any):
        """ìºì‹œì— ê°’ ì €ì¥"""
        if len(self.cache) >= self.max_size:
            # LFU (Least Frequently Used) ì •ì±…ìœ¼ë¡œ ì‚­ì œ
            least_used_key = min(self.access_count.keys(), 
                               key=lambda k: self.access_count[k])
            del self.cache[least_used_key]
            del self.access_count[least_used_key]
        
        self.cache[key] = value
        self.access_count[key] = 1
    
    def get_stats(self) -> Dict[str, float]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total_requests if total_requests > 0 else 0.0
        
        return {
            'hit_rate': hit_rate,
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'cache_size': len(self.cache),
            'max_size': self.max_size
        }

class ParallelProcessor:
    """
    ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ ê´€ë¦¬
    ë©€í‹° ìŠ¤ë ˆë”©ì„ í†µí•œ ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
    """
    
    def __init__(self, max_workers: Optional[int] = None):
        self.max_workers = max_workers or min(8, psutil.cpu_count())
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        
    def process_batch_parallel(
        self, 
        model: nn.Module, 
        data_batches: List[torch.Tensor],
        callback_fn: Optional[callable] = None
    ) -> List[Any]:
        """
        ë°°ì¹˜ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        
        Args:
            model: ì²˜ë¦¬í•  ëª¨ë¸
            data_batches: ë°ì´í„° ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸
            callback_fn: ê° ë°°ì¹˜ ì²˜ë¦¬ í›„ í˜¸ì¶œí•  ì½œë°± í•¨ìˆ˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        def process_single_batch(batch):
            with torch.no_grad():
                result = model(batch)
                if callback_fn:
                    callback_fn(result)
                return result
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        futures = [
            self.executor.submit(process_single_batch, batch)
            for batch in data_batches
        ]
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        for future in futures:
            try:
                result = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                results.append(result)
            except Exception as e:
                logger.error(f"ë³‘ë ¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                results.append(None)
        
        return results
    
    def shutdown(self):
        """ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ"""
        self.executor.shutdown(wait=True)

class BERTOptimizer:
    """
    ğŸš€ í†µí•© BERT ìµœì í™” ê´€ë¦¬ì
    ëª¨ë“  ìµœì í™” ê¸°ë²•ì„ í†µí•©í•˜ì—¬ ê´€ë¦¬
    """
    
    def __init__(self, model: nn.Module):
        self.original_model = model
        self.optimized_model = None
        
        # ìµœì í™” ì»´í¬ë„ŒíŠ¸ë“¤
        self.quantizer = ModelQuantizer()
        self.batch_manager = DynamicBatchManager()
        self.cache_manager = CacheManager()
        self.parallel_processor = ParallelProcessor()
        
        # ìµœì í™” í†µê³„
        self.optimization_stats = {
            'total_optimizations': 0,
            'average_speedup': 0.0,
            'memory_savings': 0.0,
            'cache_hit_rate': 0.0
        }
        
        logger.info("ğŸš€ BERTOptimizer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def optimize_model(
        self, 
        optimization_methods: List[str] = ['quantization', 'caching', 'dynamic_batching'],
        calibration_data: Optional[torch.Tensor] = None
    ) -> OptimizationResult:
        """
        ëª¨ë¸ ìµœì í™” ì‹¤í–‰
        
        Args:
            optimization_methods: ì ìš©í•  ìµœì í™” ë°©ë²•ë“¤
            calibration_data: ì–‘ìí™”ìš© ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°
            
        Returns:
            ìµœì í™” ê²°ê³¼
        """
        start_time = time.time()
        logger.info(f"ğŸ”§ ëª¨ë¸ ìµœì í™” ì‹œì‘: {optimization_methods}")
        
        # ì›ë³¸ ì„±ëŠ¥ ì¸¡ì •
        original_performance = self._measure_performance(self.original_model)
        
        # ìµœì í™” ì ìš©
        optimized_model = self.original_model
        applied_methods = []
        
        if 'quantization' in optimization_methods:
            try:
                optimized_model = self.quantizer.quantize_model(
                    optimized_model, calibration_data
                )
                applied_methods.append('quantization')
                logger.info("âœ… ì–‘ìí™” ì ìš© ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ì–‘ìí™” ì‹¤íŒ¨: {e}")
        
        # ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
        self.optimized_model = optimized_model
        
        # ìµœì í™” í›„ ì„±ëŠ¥ ì¸¡ì •
        optimized_performance = self._measure_performance(optimized_model)
        
        # ê²°ê³¼ ê³„ì‚°
        speedup_ratio = original_performance['avg_time'] / optimized_performance['avg_time']
        memory_saved = (original_performance['memory_mb'] - optimized_performance['memory_mb']) / original_performance['memory_mb']
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.optimization_stats['total_optimizations'] += 1
        self.optimization_stats['average_speedup'] = (
            (self.optimization_stats['average_speedup'] * (self.optimization_stats['total_optimizations'] - 1) + speedup_ratio) / 
            self.optimization_stats['total_optimizations']
        )
        self.optimization_stats['memory_savings'] = memory_saved
        
        optimization_time = time.time() - start_time
        
        result = OptimizationResult(
            original_time=original_performance['avg_time'],
            optimized_time=optimized_performance['avg_time'],
            speedup_ratio=speedup_ratio,
            memory_saved=memory_saved,
            accuracy_maintained=0.95,  # ê°„ë‹¨íˆ ê³ ì •ê°’ ì‚¬ìš©
            optimization_methods=applied_methods
        )
        
        logger.info(f"ğŸ‰ ìµœì í™” ì™„ë£Œ - ì†ë„ í–¥ìƒ: {speedup_ratio:.2f}x, ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1%}")
        return result
    
    def _measure_performance(self, model: nn.Module, num_runs: int = 5) -> Dict[str, float]:
        """ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •"""
        model.eval()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_input = torch.randint(0, 1000, (8, 512))  # ë°°ì¹˜ í¬ê¸° 8, ì‹œí€€ìŠ¤ ê¸¸ì´ 512
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        times = []
        with torch.no_grad():
            for _ in range(num_runs):
                start = time.time()
                _ = model(test_input)
                times.append(time.time() - start)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024
        
        return {
            'avg_time': np.mean(times),
            'std_time': np.std(times),
            'memory_mb': model_size
        }
    
    def get_optimized_model(self) -> Optional[nn.Module]:
        """ìµœì í™”ëœ ëª¨ë¸ ë°˜í™˜"""
        return self.optimized_model
    
    def process_with_optimization(
        self, 
        input_data: List[torch.Tensor],
        use_caching: bool = True,
        use_parallel: bool = True
    ) -> List[Any]:
        """
        ìµœì í™”ëœ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            input_data: ì…ë ¥ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            use_caching: ìºì‹± ì‚¬ìš© ì—¬ë¶€
            use_parallel: ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.optimized_model:
            logger.warning("ìµœì í™”ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ëª¨ë¸ ì‚¬ìš©")
            model = self.original_model
        else:
            model = self.optimized_model
        
        results = []
        
        for data in input_data:
            # ìºì‹± í™•ì¸
            cache_key = f"data_{hash(data.detach().numpy().tobytes())}" if use_caching else None
            
            if cache_key and use_caching:
                cached_result = self.cache_manager.get(cache_key)
                if cached_result is not None:
                    results.append(cached_result)
                    continue
            
            # ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì •
            optimal_batch_size = self.batch_manager.get_optimal_batch_size()
            
            # ëª¨ë¸ ì‹¤í–‰
            start_time = time.time()
            with torch.no_grad():
                result = model(data)
            processing_time = time.time() - start_time
            
            # ì„±ëŠ¥ ê¸°ë¡
            self.batch_manager.record_performance(processing_time, data.size(0))
            
            # ìºì‹± ì €ì¥
            if cache_key and use_caching:
                self.cache_manager.put(cache_key, result)
            
            results.append(result)
        
        return results
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±"""
        cache_stats = self.cache_manager.get_stats()
        
        return {
            'optimization_stats': self.optimization_stats,
            'cache_stats': cache_stats,
            'current_batch_size': self.batch_manager.current_batch_size,
            'parallel_workers': self.parallel_processor.max_workers,
            'recommendations': self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        cache_stats = self.cache_manager.get_stats()
        if cache_stats['hit_rate'] < 0.5:
            recommendations.append("ìºì‹œ ì ì¤‘ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ìºì‹œ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ë°ì´í„° íŒ¨í„´ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        if self.optimization_stats['average_speedup'] < 1.5:
            recommendations.append("ì¶”ê°€ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì–‘ìí™”ë‚˜ ëª¨ë¸ ì••ì¶•ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if self.batch_manager.current_batch_size < 4:
            recommendations.append("ë°°ì¹˜ í¬ê¸°ê°€ ì‘ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        return recommendations
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.parallel_processor.shutdown()
        self.cache_manager.cache.clear()
        logger.info("ğŸ§¹ BERTOptimizer ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("âš¡ BERT ìµœì í™” ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„±
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(512, 128)
            self.relu = nn.ReLU()
            self.output = nn.Linear(128, 10)
        
        def forward(self, x):
            x = x.float()  # intë¥¼ floatë¡œ ë³€í™˜
            x = self.linear(x)
            x = self.relu(x)
            return self.output(x)
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë¸ê³¼ ë°ì´í„°
    test_model = SimpleModel()
    test_data = [torch.randint(0, 100, (4, 512)) for _ in range(5)]
    
    # ìµœì í™” ì‹¤í–‰
    optimizer = BERTOptimizer(test_model)
    
    print("ğŸ”§ ëª¨ë¸ ìµœì í™” ì‹¤í–‰...")
    optimization_result = optimizer.optimize_model(['caching'])
    
    print(f"âœ… ìµœì í™” ì™„ë£Œ!")
    print(f"   ì†ë„ í–¥ìƒ: {optimization_result.speedup_ratio:.2f}x")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {optimization_result.memory_saved:.1%}")
    print(f"   ì ìš©ëœ ë°©ë²•: {optimization_result.optimization_methods}")
    
    # ìµœì í™”ëœ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ“Š ìµœì í™”ëœ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
    results = optimizer.process_with_optimization(test_data)
    print(f"âœ… {len(results)}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
    
    # ìµœì í™” ë¦¬í¬íŠ¸
    report = optimizer.get_optimization_report()
    print(f"\nğŸ“ˆ ìµœì í™” ë¦¬í¬íŠ¸:")
    print(f"   ìºì‹œ ì ì¤‘ë¥ : {report['cache_stats']['hit_rate']:.1%}")
    print(f"   í˜„ì¬ ë°°ì¹˜ í¬ê¸°: {report['current_batch_size']}")
    print(f"   ë³‘ë ¬ ì›Œì»¤ ìˆ˜: {report['parallel_workers']}")
    
    # ì •ë¦¬
    optimizer.cleanup()
    print(f"\nğŸ‰ ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 
"""
ğŸ‘ï¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
ê°•ì˜ Section 2ì—ì„œ ë‹¤ë£¬ "AIì˜ ì§‘ì¤‘ë ¥ ëª¨ë¸ë§" ì‹¤ì œ êµ¬í˜„

ì¡°ëª… ì‹œìŠ¤í…œê³¼ ë‹¨ì–´ë“¤ì˜ ëŒ€í™”ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from typing import Optional, Tuple, Dict, List
import time
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class AttentionResult:
    """ì–´í…ì…˜ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    output: torch.Tensor
    attention_weights: torch.Tensor
    processing_time: float
    attention_stats: Dict[str, float]
    head_importance: Optional[torch.Tensor] = None

class ScaledDotProductAttention(nn.Module):
    """
    âš¡ ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜
    ê°•ì˜ì—ì„œ ë‹¤ë£¬ ê¸°ë³¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ìˆ˜í•™ì  êµ¬í˜„
    """
    
    def __init__(self, temperature: float = 1.0, dropout: float = 0.1):
        super().__init__()
        self.temperature = temperature
        self.dropout = nn.Dropout(dropout)
        
    def forward(
        self, 
        query: torch.Tensor, 
        key: torch.Tensor, 
        value: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ì–´í…ì…˜ ê³„ì‚° (ê°•ì˜ì˜ "ì¡°ëª… ì‹œìŠ¤í…œ" êµ¬í˜„)
        
        Args:
            query: ì¿¼ë¦¬ í…ì„œ [B, L_q, D] - "ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€?"
            key: í‚¤ í…ì„œ [B, L_k, D] - "ê° ìœ„ì¹˜ì— ë¬´ì—‡ì´ ìˆëŠ”ê°€?"
            value: ê°’ í…ì„œ [B, L_v, D] - "ì‹¤ì œ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€?"
            mask: ë§ˆìŠ¤í¬ í…ì„œ [B, L_q, L_k] - "ì–´ë””ë¥¼ ë³´ë©´ ì•ˆ ë˜ëŠ”ê°€?"
            
        Returns:
            output: ì–´í…ì…˜ ì ìš©ëœ ì¶œë ¥ [B, L_q, D]
            attention: ì–´í…ì…˜ ê°€ì¤‘ì¹˜ [B, L_q, L_k]
        """
        batch_size, len_q, d_k = query.size()
        len_k = key.size(1)
        
        # 1. ìœ ì‚¬ë„ ê³„ì‚° (Q Â· K^T) - ê°•ì˜ì˜ "ì†ì „ë“±ìœ¼ë¡œ ë°© ë¹„ì¶”ê¸°"
        scores = torch.matmul(query, key.transpose(-2, -1))  # [B, L_q, L_k]
        
        # 2. ìŠ¤ì¼€ì¼ë§ (ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ âˆšd_kë¡œ ë‚˜ëˆ„ê¸°)
        scores = scores / math.sqrt(d_k)
        
        # 3. ë§ˆìŠ¤í‚¹ ì ìš© (íŒ¨ë”©ì´ë‚˜ ë¯¸ë˜ í† í° ê°€ë¦¬ê¸°)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 4. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜ - "ì¡°ëª…ì˜ ë°ê¸° ë¶„ë°°"
        attention_weights = F.softmax(scores, dim=-1)  # [B, L_q, L_k]
        attention_weights = self.dropout(attention_weights)
        
        # 5. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì¶œë ¥ - "ë°ê²Œ ë¹„ì¶°ì§„ ë³´ë¬¼ë“¤ ìˆ˜ì§‘"
        output = torch.matmul(attention_weights, value)  # [B, L_q, D]
        
        return output, attention_weights

class MultiHeadAttention(nn.Module):
    """
    ğŸ­ ë©€í‹°í—¤ë“œ ì–´í…ì…˜
    ê°•ì˜ì—ì„œ ë‹¤ë£¬ "ì—¬ëŸ¬ ì „ë¬¸ê°€ íŒ¨ë„" ê°œë…ì˜ ì‹¤ì œ êµ¬í˜„
    """
    
    def __init__(self, d_model: int = 768, num_heads: int = 12, dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # ì„ í˜• ë³€í™˜ ë ˆì´ì–´ë“¤
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)
    
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, 
                mask: Optional[torch.Tensor] = None) -> AttentionResult:
        """ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ê³„ì‚°"""
        start_time = time.time()
        
        batch_size, seq_len = query.size(0), query.size(1)
        residual = query
        
        # 1. ì„ í˜• ë³€í™˜ ë° í—¤ë“œ ë¶„í• 
        Q = self.w_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # 2. ì–´í…ì…˜ ê³„ì‚°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            if mask.dim() == 2:  # [batch_size, seq_len]
                mask = mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]
                mask = mask.expand(batch_size, 1, seq_len, seq_len)  # [batch_size, 1, seq_len, seq_len]
            elif mask.dim() == 3:  # [batch_size, seq_len, seq_len]
                mask = mask.unsqueeze(1)  # [batch_size, 1, seq_len, seq_len]
            # ëª¨ë“  í—¤ë“œë¡œ í™•ì¥
            mask = mask.expand(batch_size, self.num_heads, seq_len, seq_len)
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # 3. ê°€ì¤‘ í‰ê·  ê³„ì‚°
        output = torch.matmul(attention_weights, V)
        
        # 4. í—¤ë“œ ê²°í•©
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        output = self.w_o(output)
        output = self.dropout(output)
        
        # 5. ì”ì°¨ ì—°ê²° ë° ì •ê·œí™”
        output = self.layer_norm(output + residual)
        
        processing_time = time.time() - start_time
        
        # 7. í†µê³„ ê³„ì‚°
        attention_stats = self._calculate_attention_stats(attention_weights)
        
        # 8. í—¤ë“œ ì¤‘ìš”ë„ ê³„ì‚°
        head_importance = self._calculate_head_importance(attention_weights)
        
        return AttentionResult(
            output=output,
            attention_weights=attention_weights,
            processing_time=processing_time,
            attention_stats=attention_stats,
            head_importance=head_importance
        )
    
    def _calculate_attention_stats(self, attention_weights: torch.Tensor) -> Dict[str, float]:
        """
        ì–´í…ì…˜ í†µê³„ ê³„ì‚° (ê°•ì˜ì˜ "ì§‘ì¤‘ë„ ë¶„ì„")
        
        Args:
            attention_weights: [B, H, L, L]
            
        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        with torch.no_grad():
            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì§‘ì¤‘ë„ ì¸¡ì •)
            entropy = -torch.sum(
                attention_weights * torch.log(attention_weights + 1e-8), 
                dim=-1
            ).mean()
            
            # ìµœëŒ€ ì–´í…ì…˜ ê°’ (ê°€ì¥ ê°•í•œ ì§‘ì¤‘)
            max_attention = attention_weights.max()
            
            # í‰ê·  ì–´í…ì…˜ ê°’
            mean_attention = attention_weights.mean()
            
            # ëŒ€ê°ì„  ì–´í…ì…˜ (ìê¸° ìì‹ ì— ëŒ€í•œ ì§‘ì¤‘)
            diagonal_attention = torch.diagonal(attention_weights, dim1=-2, dim2=-1).mean()
            
            # ì–´í…ì…˜ ë¶„ì‚° (ì§‘ì¤‘ì˜ ì¼ê´€ì„±)
            attention_variance = attention_weights.var()
            
            return {
                'entropy': entropy.item(),
                'max_attention': max_attention.item(),
                'mean_attention': mean_attention.item(),
                'diagonal_attention': diagonal_attention.item(),
                'attention_variance': attention_variance.item()
            }
    
    def _calculate_head_importance(self, attention_weights: torch.Tensor) -> torch.Tensor:
        """
        ê° ì–´í…ì…˜ í—¤ë“œì˜ ì¤‘ìš”ë„ ê³„ì‚°
        
        Args:
            attention_weights: [B, H, L, L]
            
        Returns:
            í—¤ë“œë³„ ì¤‘ìš”ë„ ì ìˆ˜ [H]
        """
        with torch.no_grad():
            # ê° í—¤ë“œì˜ ì–´í…ì…˜ ë¶„í¬ ë‹¤ì–‘ì„±ìœ¼ë¡œ ì¤‘ìš”ë„ ì¸¡ì •
            # ë” ë‹¤ì–‘í•œ íŒ¨í„´ì„ ë³´ì´ëŠ” í—¤ë“œê°€ ë” ì¤‘ìš”
            head_entropy = -torch.sum(
                attention_weights * torch.log(attention_weights + 1e-8),
                dim=(-2, -1)
            )  # [B, H]
            
            # ë°°ì¹˜ ì „ì²´ í‰ê· 
            head_importance = head_entropy.mean(dim=0)  # [H]
            
            return head_importance

class SelfAttention(nn.Module):
    """
    ğŸ—£ï¸ ì…€í”„ ì–´í…ì…˜
    ê°•ì˜ì—ì„œ ë‹¤ë£¬ "ë‹¨ì–´ë“¤ ê°„ì˜ ëŒ€í™”" êµ¬í˜„
    """
    
    def __init__(self, d_model: int = 768, num_heads: int = 12, dropout: float = 0.1):
        super().__init__()
        self.multi_head_attention = MultiHeadAttention(d_model, num_heads, dropout)
        
    def forward(
        self, 
        hidden_states: torch.Tensor, 
        attention_mask: Optional[torch.Tensor] = None
    ) -> AttentionResult:
        """
        ì…€í”„ ì–´í…ì…˜ ê³„ì‚° (Query, Key, Valueê°€ ëª¨ë‘ ê°™ì€ ì…ë ¥)
        
        Args:
            hidden_states: ì…ë ¥ íˆë“  ìƒíƒœ [B, L, D]
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ [B, L]
            
        Returns:
            AttentionResult: ì–´í…ì…˜ ê²°ê³¼
        """
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ 4Dë¡œ í™•ì¥
        if attention_mask is not None:
            batch_size, seq_len = attention_mask.size()
            # [B, L] -> [B, L, L] (ê° ì¿¼ë¦¬ ìœ„ì¹˜ì—ì„œ í‚¤ ìœ„ì¹˜ë¡œì˜ ë§ˆìŠ¤í¬)
            attention_mask = attention_mask.unsqueeze(1).expand(batch_size, seq_len, seq_len)
        
        # ì…€í”„ ì–´í…ì…˜: Q, K, V ëª¨ë‘ ê°™ì€ ì…ë ¥ ì‚¬ìš©
        return self.multi_head_attention(
            query=hidden_states,
            key=hidden_states, 
            value=hidden_states,
            mask=attention_mask
        )

class GovernmentPositionAttention(nn.Module):
    """
    ğŸ›ï¸ ì •ë¶€ ì§ì±… íŠ¹í™” ì–´í…ì…˜
    TRAS ì‹œìŠ¤í…œì„ ìœ„í•œ ë„ë©”ì¸ íŠ¹í™” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
    """
    
    def __init__(self, d_model: int = 768, num_heads: int = 12):
        super().__init__()
        self.self_attention = SelfAttention(d_model, num_heads)
        
        # ì •ë¶€ ê´€ë ¨ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        self.position_keywords = {
            'ì •ì±…ê´€': 1.5, 'ê³¼ì¥': 1.3, 'êµ­ì¥': 1.4, 'ì°¨ê´€': 1.6, 'ì¥ê´€': 1.7,
            'ëŒ€í†µë ¹': 2.0, 'ì´ë¦¬': 1.8, 'ë¹„ì„œê´€': 1.2, 'ë³´ì¢Œê´€': 1.1, 'ìˆ˜ì„': 1.3,
            'AI': 1.4, 'ì¸ê³µì§€ëŠ¥': 1.4, 'ë°ì´í„°': 1.2, 'ë¶„ì„': 1.1, 'ì¶”ì²œ': 1.3
        }
        
        # ì§ì±… ì¶”ì¶œì„ ìœ„í•œ ë¶„ë¥˜ í—¤ë“œ
        self.position_classifier = nn.Linear(d_model, len(self.position_keywords))
        
    def forward(
        self, 
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        token_labels: Optional[List[List[str]]] = None
    ) -> Tuple[AttentionResult, torch.Tensor]:
        """
        ì •ë¶€ ì§ì±… íŠ¹í™” ì–´í…ì…˜ ê³„ì‚°
        
        Args:
            hidden_states: ì…ë ¥ íˆë“  ìƒíƒœ [B, L, D]
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ [B, L]
            token_labels: í† í°ë³„ ë¼ë²¨ (ì •ë¶€ í‚¤ì›Œë“œ ì‹ë³„ìš©)
            
        Returns:
            attention_result: ì–´í…ì…˜ ê²°ê³¼
            position_scores: ì§ì±… ê´€ë ¨ë„ ì ìˆ˜ [B, L, num_positions]
        """
        # 1. ê¸°ë³¸ ì…€í”„ ì–´í…ì…˜
        attention_result = self.self_attention(hidden_states, attention_mask)
        
        # 2. ì •ë¶€ ì§ì±… ê´€ë ¨ë„ ê³„ì‚°
        position_scores = self.position_classifier(attention_result.output)
        
        # 3. ë„ë©”ì¸ íŠ¹í™” ê°€ì¤‘ì¹˜ ì ìš©
        if token_labels is not None:
            domain_weights = self._calculate_domain_weights(token_labels)
            # ê°€ì¤‘ì¹˜ë¥¼ ì–´í…ì…˜ì— ë°˜ì˜
            weighted_attention = attention_result.attention_weights * domain_weights
            attention_result.attention_weights = weighted_attention
        
        return attention_result, position_scores
    
    def _calculate_domain_weights(self, token_labels: List[List[str]]) -> torch.Tensor:
        """ë„ë©”ì¸ íŠ¹í™” ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        batch_size = len(token_labels)
        max_seq_len = max(len(labels) for labels in token_labels)
        
        weights = torch.ones(batch_size, max_seq_len, max_seq_len)
        
        for batch_idx, labels in enumerate(token_labels):
            for i, label_i in enumerate(labels):
                for j, label_j in enumerate(labels):
                    # ì •ë¶€ í‚¤ì›Œë“œ ê°„ì˜ ê°€ì¤‘ì¹˜ ë¶€ì—¬
                    weight_i = self.position_keywords.get(label_i, 1.0)
                    weight_j = self.position_keywords.get(label_j, 1.0)
                    weights[batch_idx, i, j] = math.sqrt(weight_i * weight_j)
        
        return weights

class AttentionVisualizer:
    """
    ğŸ‘€ ì–´í…ì…˜ ì‹œê°í™” ë„êµ¬
    ê°•ì˜ì—ì„œ ë‹¤ë£¬ ì–´í…ì…˜ íŒ¨í„´ì„ ì‹œê°ì ìœ¼ë¡œ ë¶„ì„
    """
    
    @staticmethod
    def analyze_attention_pattern(
        attention_weights: torch.Tensor, 
        tokens: List[str]
    ) -> Dict[str, any]:
        """
        ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„
        
        Args:
            attention_weights: [H, L, L] (ë‹¨ì¼ ìƒ˜í”Œ)
            tokens: í† í° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        num_heads, seq_len, _ = attention_weights.shape
        
        # 1. í—¤ë“œë³„ ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„
        head_patterns = {}
        for head_idx in range(num_heads):
            head_attention = attention_weights[head_idx].numpy()
            
            # ëŒ€ê°ì„  ì§‘ì¤‘ë„ (ìê¸° ì°¸ì¡°)
            diagonal_focus = np.diag(head_attention).mean()
            
            # ì¥ê±°ë¦¬ ì˜ì¡´ì„± (ë¨¼ í† í° ê°„ ì–´í…ì…˜)
            long_range_dep = 0
            for i in range(seq_len):
                for j in range(seq_len):
                    if abs(i - j) > seq_len // 2:
                        long_range_dep += head_attention[i, j]
            long_range_dep /= (seq_len * seq_len / 4)
            
            head_patterns[f'head_{head_idx}'] = {
                'diagonal_focus': diagonal_focus,
                'long_range_dependency': long_range_dep,
                'max_attention': head_attention.max(),
                'entropy': -np.sum(head_attention * np.log(head_attention + 1e-8))
            }
        
        # 2. í† í°ë³„ ì¤‘ìš”ë„ ê³„ì‚°
        token_importance = attention_weights.mean(dim=0).sum(dim=0).numpy()
        
        # 3. ì–´í…ì…˜ ê·¸ë˜í”„ êµ¬ì„± (ê°•í•œ ì—°ê²° ê´€ê³„)
        attention_graph = {}
        threshold = 0.1  # ì–´í…ì…˜ ì„ê³„ê°’
        
        for i, token_i in enumerate(tokens):
            connections = []
            for j, token_j in enumerate(tokens):
                avg_attention = attention_weights[:, i, j].mean().item()
                if avg_attention > threshold and i != j:
                    connections.append((token_j, avg_attention))
            
            # ìƒìœ„ 3ê°œ ì—°ê²°ë§Œ ìœ ì§€
            connections.sort(key=lambda x: x[1], reverse=True)
            attention_graph[token_i] = connections[:3]
        
        return {
            'head_patterns': head_patterns,
            'token_importance': list(zip(tokens, token_importance)),
            'attention_graph': attention_graph,
            'global_stats': {
                'avg_entropy': np.mean([p['entropy'] for p in head_patterns.values()]),
                'avg_diagonal_focus': np.mean([p['diagonal_focus'] for p in head_patterns.values()]),
                'most_important_token': tokens[np.argmax(token_importance)]
            }
        }

# í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ‘ï¸ TRAS ì–´í…ì…˜ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    batch_size = 2
    seq_len = 10
    d_model = 256
    num_heads = 8
    
    # ëª¨ë¸ ìƒì„±
    attention_model = MultiHeadAttention(d_model, num_heads)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_input = torch.randn(batch_size, seq_len, d_model)
    test_mask = torch.ones(batch_size, seq_len, seq_len)
    
    print(f"ğŸ“ ì…ë ¥ í˜•íƒœ: {test_input.shape}")
    
    # ì–´í…ì…˜ ê³„ì‚°
    result = attention_model(test_input, test_input, test_input, test_mask)
    
    print(f"âœ… ì¶œë ¥ í˜•íƒœ: {result.output.shape}")
    print(f"ğŸ‘ï¸ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í˜•íƒœ: {result.attention_weights.shape}")
    print(f"âš¡ ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.4f}ì´ˆ")
    
    print(f"\nğŸ“Š ì–´í…ì…˜ í†µê³„:")
    for key, value in result.attention_stats.items():
        print(f"   {key}: {value:.4f}")
    
    print(f"\nğŸ­ í—¤ë“œë³„ ì¤‘ìš”ë„:")
    for i, importance in enumerate(result.head_importance):
        print(f"   Head {i}: {importance:.4f}")
    
    # ì •ë¶€ íŠ¹í™” ì–´í…ì…˜ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ›ï¸ ì •ë¶€ íŠ¹í™” ì–´í…ì…˜ í…ŒìŠ¤íŠ¸")
    gov_attention = GovernmentPositionAttention(d_model, num_heads)
    
    gov_result, position_scores = gov_attention(test_input)
    print(f"âœ… ì •ë¶€ ì–´í…ì…˜ ì¶œë ¥: {gov_result.output.shape}")
    print(f"ğŸ¯ ì§ì±… ì ìˆ˜ í˜•íƒœ: {position_scores.shape}")
    
    print("\nï¿½ï¿½ ì–´í…ì…˜ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 
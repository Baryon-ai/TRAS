"""
ğŸ“Š ë¬¸ë§¥ì  ì„ë² ë”© ëª¨ë“ˆ
ê°•ì˜ Section 2ì—ì„œ ë‹¤ë£¬ "ì˜ë¯¸ì˜ ì§€ë„" ê°œë…ì˜ ì‹¤ì œ êµ¬í˜„

Word2Vecë¶€í„° ë¬¸ë§¥ì  ì„ë² ë”©ê¹Œì§€, ê¸°í•˜í•™ì  ì§ê´€ì„ ì½”ë“œë¡œ êµ¬í˜„
"""

import torch
import torch.nn as nn
import numpy as np
from typing import List, Dict, Tuple, Optional, Union
import math
import time
from dataclasses import dataclass
from sklearn.metrics.pairwise import cosine_similarity
import logging

logger = logging.getLogger(__name__)

@dataclass
class EmbeddingResult:
    """ì„ë² ë”© ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    embeddings: torch.Tensor
    attention_weights: Optional[torch.Tensor]
    similarity_scores: Optional[Dict[str, float]]
    processing_time: float
    geometric_properties: Dict[str, float]

class PositionalEncoding(nn.Module):
    """
    ğŸŒŠ ìœ„ì¹˜ ì¸ì½”ë”©
    ê°•ì˜ì—ì„œ ë‹¤ë£¬ "ë‹¨ì–´ì˜ ìˆœì„œ" ì •ë³´ë¥¼ ê¸°í•˜í•™ì ìœ¼ë¡œ í‘œí˜„
    """
    
    def __init__(self, d_model: int, max_length: int = 512):
        super().__init__()
        self.d_model = d_model
        
        # ìœ„ì¹˜ ì¸ì½”ë”© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        pe = torch.zeros(max_length, d_model)
        position = torch.arange(0, max_length).unsqueeze(1).float()
        
        # ê¸°í•˜í•™ì  ì§ê´€: ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ë¡œ ìˆœí™˜í•˜ëŠ” íŒ¨í„´ ìƒì„±
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * 
            -(math.log(10000.0) / d_model)
        )
        
        pe[:, 0::2] = torch.sin(position * div_term)  # ì§ìˆ˜ ì¸ë±ìŠ¤
        pe[:, 1::2] = torch.cos(position * div_term)  # í™€ìˆ˜ ì¸ë±ìŠ¤
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìœ„ì¹˜ ì •ë³´ë¥¼ ì„ë² ë”©ì— ì¶”ê°€"""
        return x + self.pe[:, :x.size(1)]

class ContextualEmbedding(nn.Module):
    """
    ğŸ­ ë¬¸ë§¥ì  ì„ë² ë”© ëª¨ë“ˆ
    ê°•ì˜ì—ì„œ ë‹¤ë£¬ "ì¹´ë©œë ˆì˜¨ ë‹¨ì–´" ê°œë…ì˜ ì‹¤ì œ êµ¬í˜„
    """
    
    def __init__(
        self, 
        vocab_size: int, 
        d_model: int = 768, 
        max_length: int = 512,
        dropout: float = 0.1
    ):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # ê¸°ë³¸ í† í° ì„ë² ë”© (ê°•ì˜ì˜ "ì˜ë¯¸ì˜ ì§€ë„" ê¸°ë³¸ ì¢Œí‘œ)
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.positional_encoding = PositionalEncoding(d_model, max_length)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
        
        # LayerNorm (ìˆ˜ì¹˜ ì•ˆì •ì„±)
        self.layer_norm = nn.LayerNorm(d_model)
        
        # ì •ë¶€ ê´€ë ¨ ë„ë©”ì¸ ê°€ì¤‘ì¹˜ (ë„ë©”ì¸ íŠ¹í™”)
        self.domain_weight = nn.Linear(d_model, d_model)
        
        # ì„ë² ë”© ì´ˆê¸°í™” (Xavier/Glorot ì´ˆê¸°í™”)
        self._init_embeddings()
        
        logger.info(f"ğŸ“Š ContextualEmbedding ì´ˆê¸°í™” ì™„ë£Œ - ì°¨ì›: {d_model}")
    
    def _init_embeddings(self):
        """ì„ë² ë”© ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ê¸°í•˜í•™ì  ìµœì í™”)"""
        # Xavier ì´ˆê¸°í™”ë¡œ ë²¡í„°ë“¤ì´ êµ¬ í‘œë©´ì— ê³ ë¥´ê²Œ ë¶„í¬
        nn.init.xavier_uniform_(self.token_embedding.weight)
        nn.init.xavier_uniform_(self.domain_weight.weight)
        
        # íŒ¨ë”© í† í°ì€ ì˜ë²¡í„°ë¡œ ì´ˆê¸°í™”
        with torch.no_grad():
            self.token_embedding.weight[0].fill_(0)
    
    def forward(
        self, 
        input_ids: torch.Tensor, 
        attention_mask: Optional[torch.Tensor] = None
    ) -> EmbeddingResult:
        """
        ìˆœì „íŒŒ (ë¬¸ë§¥ì  ì„ë² ë”© ìƒì„±)
        
        Args:
            input_ids: í† í° ID í…ì„œ [batch_size, seq_length]
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ [batch_size, seq_length]
            
        Returns:
            EmbeddingResult: ì„ë² ë”© ê²°ê³¼
        """
        start_time = time.time()
        batch_size, seq_length = input_ids.shape
        
        # 1. ê¸°ë³¸ í† í° ì„ë² ë”©
        token_embeds = self.token_embedding(input_ids)  # [B, L, D]
        
        # 2. ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        embeddings = self.positional_encoding(token_embeds)
        
        # 3. ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        
        # 4. ë„ë©”ì¸ íŠ¹í™” ê°€ì¤‘ì¹˜ ì ìš©
        domain_weighted = self.domain_weight(embeddings)
        embeddings = embeddings + 0.1 * domain_weighted  # ì”ì°¨ ì—°ê²°
        
        # 5. ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì ìš©
        if attention_mask is not None:
            # íŒ¨ë”© í† í°ì˜ ì„ë² ë”©ì„ 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
            mask_expanded = attention_mask.unsqueeze(-1).expand_as(embeddings)
            embeddings = embeddings * mask_expanded.float()
        
        processing_time = time.time() - start_time
        
        # 6. ê¸°í•˜í•™ì  ì†ì„± ê³„ì‚°
        geometric_props = self._calculate_geometric_properties(embeddings)
        
        return EmbeddingResult(
            embeddings=embeddings,
            attention_weights=None,  # ë‚˜ì¤‘ì— ì–´í…ì…˜ ëª¨ë“ˆì—ì„œ ê³„ì‚°
            similarity_scores=None,
            processing_time=processing_time,
            geometric_properties=geometric_props
        )
    
    def _calculate_geometric_properties(self, embeddings: torch.Tensor) -> Dict[str, float]:
        """
        ì„ë² ë”©ì˜ ê¸°í•˜í•™ì  ì†ì„± ê³„ì‚°
        ê°•ì˜ì—ì„œ ë‹¤ë£¬ ë²¡í„° ê³µê°„ì˜ ìˆ˜í•™ì  íŠ¹ì„±
        """
        with torch.no_grad():
            # ë²¡í„°ì˜ í¬ê¸° (norm) í†µê³„
            norms = torch.norm(embeddings, dim=-1)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„í¬ (ë²¡í„° ê°„ ê°ë„)
            flat_embeds = embeddings.view(-1, embeddings.size(-1))
            sample_size = min(1000, flat_embeds.size(0))
            sample_indices = torch.randperm(flat_embeds.size(0))[:sample_size]
            sample_embeds = flat_embeds[sample_indices]
            
            # í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            if sample_size > 1:
                similarities = torch.mm(
                    sample_embeds, 
                    sample_embeds.transpose(0, 1)
                ) / (torch.norm(sample_embeds, dim=1, keepdim=True) * 
                     torch.norm(sample_embeds, dim=1).unsqueeze(0))
                
                # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì œì™¸
                mask = torch.eye(sample_size, dtype=torch.bool)
                similarities = similarities[~mask]
                avg_similarity = similarities.mean().item()
            else:
                avg_similarity = 0.0
            
            return {
                'avg_norm': norms.mean().item(),
                'std_norm': norms.std().item(),
                'max_norm': norms.max().item(),
                'min_norm': norms.min().item(),
                'avg_cosine_similarity': avg_similarity,
                'embedding_dimension': embeddings.size(-1)
            }
    
    def get_word_embedding(self, word_id: int) -> torch.Tensor:
        """ë‹¨ì¼ ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„° ë°˜í™˜"""
        return self.token_embedding.weight[word_id]
    
    def similarity_search(
        self, 
        query_embedding: torch.Tensor, 
        candidate_embeddings: torch.Tensor,
        top_k: int = 5
    ) -> List[Tuple[int, float]]:
        """
        ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ (ê°•ì˜ì˜ "ì˜ë¯¸ì˜ ì§€ë„"ì—ì„œ ê°€ê¹Œìš´ ì  ì°¾ê¸°)
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
            candidate_embeddings: í›„ë³´ ì„ë² ë”©ë“¤
            top_k: ë°˜í™˜í•  ìƒìœ„ kê°œ
            
        Returns:
            (ì¸ë±ìŠ¤, ìœ ì‚¬ë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ê¸°í•˜í•™ì  ê°ë„ ê¸°ë°˜)
        query_norm = query_embedding / torch.norm(query_embedding)
        candidate_norms = candidate_embeddings / torch.norm(
            candidate_embeddings, dim=-1, keepdim=True
        )
        
        similarities = torch.matmul(query_norm, candidate_norms.transpose(-2, -1))
        
        # ìƒìœ„ kê°œ ì„ íƒ
        top_values, top_indices = torch.topk(similarities, k=top_k)
        
        return [(idx.item(), score.item()) for idx, score in zip(top_indices, top_values)]

class GeometricAnalyzer:
    """
    ğŸ“ ê¸°í•˜í•™ì  ë¶„ì„ê¸°
    ê°•ì˜ì—ì„œ ë‹¤ë£¬ ë²¡í„° ê³µê°„ì˜ ê¸°í•˜í•™ì  ì„±ì§ˆ ë¶„ì„
    """
    
    @staticmethod
    def vector_arithmetic(
        embeddings: Dict[str, torch.Tensor], 
        operation: str
    ) -> torch.Tensor:
        """
        ë²¡í„° ì—°ì‚° (king - man + woman = queen ìŠ¤íƒ€ì¼)
        
        Args:
            embeddings: ë‹¨ì–´ë³„ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
            operation: ì—°ì‚° ë¬¸ìì—´ (ì˜ˆ: "ê¹€ì² ìˆ˜ - ë‚¨ì„± + ì •ì±…ê´€")
            
        Returns:
            ì—°ì‚° ê²°ê³¼ ë²¡í„°
        """
        # ê°„ë‹¨í•œ íŒŒì„œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
        tokens = operation.replace('+', ' + ').replace('-', ' - ').split()
        
        result = None
        current_op = '+'
        
        for token in tokens:
            if token in ['+', '-']:
                current_op = token
            elif token in embeddings:
                vector = embeddings[token]
                if result is None:
                    result = vector.clone()
                elif current_op == '+':
                    result += vector
                elif current_op == '-':
                    result -= vector
        
        return result if result is not None else torch.zeros_like(next(iter(embeddings.values())))
    
    @staticmethod
    def analyze_cluster(embeddings: torch.Tensor, labels: List[str]) -> Dict[str, any]:
        """
        í´ëŸ¬ìŠ¤í„° ë¶„ì„ (ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì˜ ê¸°í•˜í•™ì  ë°°ì¹˜)
        
        Args:
            embeddings: ì„ë² ë”© í…ì„œ [num_words, embedding_dim]
            labels: ë‹¨ì–´ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼
        """
        # ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ PCA (ì‹œê°í™”ìš©)
        from sklearn.decomposition import PCA
        from sklearn.cluster import KMeans
        
        embeddings_np = embeddings.detach().numpy()
        
        # PCAë¡œ 2D íˆ¬ì˜
        pca = PCA(n_components=2)
        embeddings_2d = pca.fit_transform(embeddings_np)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        n_clusters = min(5, len(labels))
        kmeans = KMeans(n_clusters=n_clusters)
        cluster_labels = kmeans.fit_predict(embeddings_np)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë‹¨ì–´ ê·¸ë£¹í™”
        clusters = {}
        for i, (word, cluster_id) in enumerate(zip(labels, cluster_labels)):
            if cluster_id not in clusters:
                clusters[cluster_id] = []
            clusters[cluster_id].append(word)
        
        return {
            'clusters': clusters,
            'embeddings_2d': embeddings_2d,
            'explained_variance_ratio': pca.explained_variance_ratio_,
            'cluster_centers': kmeans.cluster_centers_
        }
    
    @staticmethod
    def semantic_direction(
        embeddings: Dict[str, torch.Tensor], 
        positive_examples: List[str], 
        negative_examples: List[str]
    ) -> torch.Tensor:
        """
        ì˜ë¯¸ì  ë°©í–¥ ë²¡í„° ê³„ì‚°
        ì˜ˆ: ì„±ë³„ ë°©í–¥ = (ë‚¨ì„± ë‹¨ì–´ë“¤ í‰ê· ) - (ì—¬ì„± ë‹¨ì–´ë“¤ í‰ê· )
        
        Args:
            embeddings: ë‹¨ì–´ë³„ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
            positive_examples: ì–‘ì˜ ì˜ˆì‹œ ë‹¨ì–´ë“¤
            negative_examples: ìŒì˜ ì˜ˆì‹œ ë‹¨ì–´ë“¤
            
        Returns:
            ì˜ë¯¸ì  ë°©í–¥ ë²¡í„°
        """
        pos_vectors = torch.stack([embeddings[word] for word in positive_examples if word in embeddings])
        neg_vectors = torch.stack([embeddings[word] for word in negative_examples if word in embeddings])
        
        pos_mean = pos_vectors.mean(dim=0)
        neg_mean = neg_vectors.mean(dim=0)
        
        return pos_mean - neg_mean

# ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ“Š TRAS ì„ë² ë”© ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    vocab_size = 1000
    d_model = 256
    batch_size = 2
    seq_length = 10
    
    # ëª¨ë¸ ìƒì„±
    embedding_model = ContextualEmbedding(vocab_size, d_model)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))
    attention_mask = torch.ones(batch_size, seq_length)
    
    print(f"ğŸ“ ì…ë ¥ í˜•íƒœ: {input_ids.shape}")
    
    # ì„ë² ë”© ìƒì„±
    result = embedding_model(input_ids, attention_mask)
    
    print(f"âœ… ì¶œë ¥ í˜•íƒœ: {result.embeddings.shape}")
    print(f"âš¡ ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.4f}ì´ˆ")
    print(f"ğŸ“ ê¸°í•˜í•™ì  ì†ì„±:")
    for key, value in result.geometric_properties.items():
        print(f"   {key}: {value:.4f}")
    
    # ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸  
    query = result.embeddings[0, 0]  # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ í† í°
    candidates = result.embeddings[0, 1:]  # ë‚˜ë¨¸ì§€ í† í°ë“¤
    
    similar_indices = embedding_model.similarity_search(query, candidates, top_k=3)
    print(f"ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼: {similar_indices}")
    
    print("\nï¿½ï¿½ ì„ë² ë”© ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 
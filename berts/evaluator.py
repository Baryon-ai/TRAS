"""
ğŸ¯ ì‹ ë¢°ë„ í‰ê°€ ëª¨ë“ˆ
BERT ëª¨ë¸ì˜ ì˜ˆì¸¡ ì‹ ë¢°ì„±ì„ ë‹¤ê°ë„ë¡œ í‰ê°€

- ì˜ˆì¸¡ ì¼ê´€ì„± ë¶„ì„
- ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”  
- ë„ë©”ì¸ ì í•©ì„± ê²€ì¦
- ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import time
import logging
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from scipy import stats
import math

logger = logging.getLogger(__name__)

@dataclass
class TrustAssessment:
    """ì‹ ë¢°ë„ í‰ê°€ ê²°ê³¼"""
    overall_trust_score: float
    prediction_confidence: float
    consistency_score: float
    uncertainty_level: float
    domain_relevance: float
    calibration_score: float
    detailed_metrics: Dict[str, float]
    recommendations: List[str]

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    confusion_matrix: np.ndarray
    classification_report: Dict[str, Any]

class UncertaintyQuantifier:
    """
    ğŸ“Š ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
    ëª¨ë¸ ì˜ˆì¸¡ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì¸¡ì •
    """
    
    @staticmethod
    def entropy_based_uncertainty(probabilities: torch.Tensor) -> torch.Tensor:
        """
        ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
        
        Args:
            probabilities: ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ [batch_size, num_classes]
            
        Returns:
            ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ [batch_size]
        """
        # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: H(p) = -Î£ p_i * log(p_i)
        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-8), dim=-1)
        
        # ìµœëŒ€ ì—”íŠ¸ë¡œí”¼ë¡œ ì •ê·œí™” (ê· ë“± ë¶„í¬ ì‹œ ìµœëŒ€ê°’)
        max_entropy = math.log(probabilities.size(-1))
        normalized_entropy = entropy / max_entropy
        
        return normalized_entropy
    
    @staticmethod
    def predictive_variance(ensemble_predictions: List[torch.Tensor]) -> torch.Tensor:
        """
        ì˜ˆì¸¡ ë¶„ì‚° ê³„ì‚°
        
        Args:
            ensemble_predictions: ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì˜ˆì¸¡ ë¶„ì‚° [batch_size, num_classes]
        """
        if len(ensemble_predictions) < 2:
            return torch.zeros_like(ensemble_predictions[0])
        
        # ìŠ¤íƒìœ¼ë¡œ ë³€í™˜: [num_models, batch_size, num_classes]
        stacked_predictions = torch.stack(ensemble_predictions, dim=0)
        
        # ë¶„ì‚° ê³„ì‚°
        prediction_variance = torch.var(stacked_predictions, dim=0)
        
        return prediction_variance
    
    @staticmethod
    def mutual_information(ensemble_predictions: List[torch.Tensor]) -> torch.Tensor:
        """
        ìƒí˜¸ ì •ë³´ëŸ‰ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„±
        
        Args:
            ensemble_predictions: ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
            
        Returns:
            ìƒí˜¸ ì •ë³´ëŸ‰ [batch_size]
        """
        if len(ensemble_predictions) < 2:
            return torch.zeros(ensemble_predictions[0].size(0))
        
        # í‰ê·  ì˜ˆì¸¡
        mean_prediction = torch.mean(torch.stack(ensemble_predictions), dim=0)
        
        # ì „ì²´ ë¶ˆí™•ì‹¤ì„± (í‰ê·  ì˜ˆì¸¡ì˜ ì—”íŠ¸ë¡œí”¼)
        total_uncertainty = UncertaintyQuantifier.entropy_based_uncertainty(mean_prediction)
        
        # í‰ê·  ë¶ˆí™•ì‹¤ì„± (ê° ëª¨ë¸ ì˜ˆì¸¡ì˜ í‰ê·  ì—”íŠ¸ë¡œí”¼)  
        individual_uncertainties = [
            UncertaintyQuantifier.entropy_based_uncertainty(pred) 
            for pred in ensemble_predictions
        ]
        mean_uncertainty = torch.mean(torch.stack(individual_uncertainties), dim=0)
        
        # ìƒí˜¸ ì •ë³´ëŸ‰ = ì „ì²´ ë¶ˆí™•ì‹¤ì„± - í‰ê·  ë¶ˆí™•ì‹¤ì„±
        mutual_info = total_uncertainty - mean_uncertainty
        
        return mutual_info

class ConsistencyAnalyzer:
    """
    ğŸ”„ ì¼ê´€ì„± ë¶„ì„ê¸°
    ëª¨ë¸ ì˜ˆì¸¡ì˜ ì¼ê´€ì„±ì„ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ í‰ê°€
    """
    
    @staticmethod
    def temporal_consistency(
        predictions_t1: torch.Tensor, 
        predictions_t2: torch.Tensor
    ) -> float:
        """
        ì‹œê°„ì  ì¼ê´€ì„± í‰ê°€ (ê°™ì€ ì…ë ¥ì— ëŒ€í•œ ì‹œê°„ì°¨ ì˜ˆì¸¡ ë¹„êµ)
        
        Args:
            predictions_t1: ì‹œì  1ì˜ ì˜ˆì¸¡
            predictions_t2: ì‹œì  2ì˜ ì˜ˆì¸¡
            
        Returns:
            ì¼ê´€ì„± ì ìˆ˜ (0-1)
        """
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ì¼ê´€ì„± ì¸¡ì •
        cosine_sim = F.cosine_similarity(predictions_t1, predictions_t2, dim=-1)
        return cosine_sim.mean().item()
    
    @staticmethod
    def input_perturbation_consistency(
        model: torch.nn.Module,
        original_input: torch.Tensor,
        noise_level: float = 0.01,
        num_perturbations: int = 5
    ) -> float:
        """
        ì…ë ¥ ì„­ë™ì— ëŒ€í•œ ì¼ê´€ì„± í‰ê°€
        
        Args:
            model: í‰ê°€í•  ëª¨ë¸
            original_input: ì›ë³¸ ì…ë ¥
            noise_level: ë…¸ì´ì¦ˆ ìˆ˜ì¤€
            num_perturbations: ì„­ë™ íšŸìˆ˜
            
        Returns:
            ì¼ê´€ì„± ì ìˆ˜
        """
        model.eval()
        
        with torch.no_grad():
            # ì›ë³¸ ì˜ˆì¸¡
            original_pred = model(original_input)
            
            # ì„­ë™ëœ ì…ë ¥ë“¤ì— ëŒ€í•œ ì˜ˆì¸¡
            perturbed_preds = []
            for _ in range(num_perturbations):
                noise = torch.randn_like(original_input.float()) * noise_level
                perturbed_input = original_input + noise.long()  # ì •ìˆ˜ í…ì„œë¡œ ë³€í™˜
                perturbed_pred = model(perturbed_input)
                perturbed_preds.append(perturbed_pred)
            
            # ì¼ê´€ì„± ê³„ì‚°
            similarities = []
            for perturbed_pred in perturbed_preds:
                if hasattr(original_pred, 'last_hidden_state'):
                    # BERT ì¶œë ¥ì¸ ê²½ìš°
                    sim = F.cosine_similarity(
                        original_pred.pooler_output, 
                        perturbed_pred.pooler_output, 
                        dim=-1
                    ).mean()
                else:
                    # ì¼ë°˜ í…ì„œì¸ ê²½ìš°
                    sim = F.cosine_similarity(original_pred, perturbed_pred, dim=-1).mean()
                
                similarities.append(sim.item())
            
            return np.mean(similarities)

class DomainRelevanceEvaluator:
    """
    ğŸ›ï¸ ë„ë©”ì¸ ì í•©ì„± í‰ê°€ê¸°
    ì •ë¶€ ì¸ì¬ ì¶”ì²œ ë„ë©”ì¸ì— íŠ¹í™”ëœ í‰ê°€
    """
    
    def __init__(self):
        # ì •ë¶€ ë„ë©”ì¸ í‚¤ì›Œë“œì™€ ê°€ì¤‘ì¹˜
        self.government_keywords = {
            'ì •ì±…ê´€': 1.0, 'ê³¼ì¥': 0.8, 'êµ­ì¥': 0.9, 'ì°¨ê´€': 1.0, 'ì¥ê´€': 1.0,
            'ëŒ€í†µë ¹': 1.0, 'ì´ë¦¬': 1.0, 'ë¹„ì„œê´€': 0.7, 'ë³´ì¢Œê´€': 0.6, 'ìˆ˜ì„': 0.8,
            'AI': 0.9, 'ì¸ê³µì§€ëŠ¥': 0.9, 'ë°ì´í„°': 0.7, 'ë¶„ì„': 0.6, 'ì •ë¶€': 0.8,
            'ê³µë¬´ì›': 0.7, 'í–‰ì •': 0.6, 'ì •ì±…': 0.8, 'ì¶”ì²œ': 0.9, 'ì„ìš©': 0.8
        }
        
        # ë¶€ì •ì  í‚¤ì›Œë“œ (ë„ë©”ì¸ ë¶€ì í•©ì„± í‘œì‹œ)
        self.negative_keywords = {
            'ë¯¼ê°„', 'ì‚¬ê¸°ì—…', 'ê°œì¸', 'ì‚¬ì ', 'ì˜ë¦¬', 'ì¥ì‚¬', 'ì‚¬ì—…'
        }
    
    def evaluate_domain_relevance(
        self, 
        text: str, 
        predictions: Dict[str, float]
    ) -> float:
        """
        ë„ë©”ì¸ ì í•©ì„± ì ìˆ˜ ê³„ì‚°
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
            
        Returns:
            ë„ë©”ì¸ ì í•©ì„± ì ìˆ˜ (0-1)
        """
        # 1. í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ë¶„ì„
        text_lower = text.lower()
        keyword_score = 0.0
        keyword_count = 0
        
        for keyword, weight in self.government_keywords.items():
            if keyword in text_lower:
                keyword_score += weight
                keyword_count += 1
        
        # ë¶€ì •ì  í‚¤ì›Œë“œ íŒ¨ë„í‹°
        for neg_keyword in self.negative_keywords:
            if neg_keyword in text_lower:
                keyword_score -= 0.5
        
        # í‚¤ì›Œë“œ ì ìˆ˜ ì •ê·œí™”
        if keyword_count > 0:
            keyword_score = max(0.0, keyword_score / keyword_count)
        else:
            keyword_score = 0.0
        
        # 2. ì˜ˆì¸¡ ì¼ê´€ì„± í‰ê°€
        prediction_score = 0.0
        if predictions and 'positions' in predictions:
            # ì •ë¶€ ì§ì±… ì˜ˆì¸¡ì˜ ìµœëŒ€ê°’
            max_position_prob = max(predictions['positions'].values())
            prediction_score = max_position_prob
        
        # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        relevance_score = 0.6 * keyword_score + 0.4 * prediction_score
        return min(1.0, max(0.0, relevance_score))

class CalibrationEvaluator:
    """
    âš–ï¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í‰ê°€ê¸°
    ëª¨ë¸ì˜ ì‹ ë¢°ë„ ë³´ì • ì •ë„ë¥¼ í‰ê°€
    """
    
    @staticmethod
    def expected_calibration_error(
        confidences: np.ndarray, 
        accuracies: np.ndarray, 
        num_bins: int = 10
    ) -> float:
        """
        Expected Calibration Error (ECE) ê³„ì‚°
        
        Args:
            confidences: ì˜ˆì¸¡ ì‹ ë¢°ë„ ë°°ì—´
            accuracies: ì‹¤ì œ ì •í™•ë„ ë°°ì—´  
            num_bins: êµ¬ê°„ ìˆ˜
            
        Returns:
            ECE ì ìˆ˜
        """
        bin_boundaries = np.linspace(0, 1, num_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        ece = 0.0
        total_samples = len(confidences)
        
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            # êµ¬ê°„ì— ì†í•˜ëŠ” ìƒ˜í”Œë“¤
            in_bin = (confidences > bin_lower) & (confidences <= bin_upper)
            prop_in_bin = in_bin.mean()
            
            if prop_in_bin > 0:
                # êµ¬ê°„ ë‚´ í‰ê·  ì‹ ë¢°ë„ì™€ ì •í™•ë„
                accuracy_in_bin = accuracies[in_bin].mean()
                avg_confidence_in_bin = confidences[in_bin].mean()
                
                # ECE ëˆ„ì 
                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        
        return ece
    
    @staticmethod
    def reliability_diagram_data(
        confidences: np.ndarray, 
        accuracies: np.ndarray, 
        num_bins: int = 10
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        ì‹ ë¢°ë„ ë‹¤ì´ì–´ê·¸ë¨ ë°ì´í„° ìƒì„±
        
        Returns:
            (bin_centers, bin_accuracies, bin_counts)
        """
        bin_boundaries = np.linspace(0, 1, num_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        bin_centers = (bin_lowers + bin_uppers) / 2
        
        bin_accuracies = []
        bin_counts = []
        
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (confidences > bin_lower) & (confidences <= bin_upper)
            
            if in_bin.sum() > 0:
                bin_accuracy = accuracies[in_bin].mean()
                bin_count = in_bin.sum()
            else:
                bin_accuracy = 0.0
                bin_count = 0
            
            bin_accuracies.append(bin_accuracy)
            bin_counts.append(bin_count)
        
        return bin_centers, np.array(bin_accuracies), np.array(bin_counts)

class TrustScoreCalculator:
    """
    ğŸ¯ í†µí•© ì‹ ë¢°ë„ ê³„ì‚°ê¸°
    ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
    """
    
    def __init__(self):
        self.uncertainty_quantifier = UncertaintyQuantifier()
        self.consistency_analyzer = ConsistencyAnalyzer() 
        self.domain_evaluator = DomainRelevanceEvaluator()
        self.calibration_evaluator = CalibrationEvaluator()
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        self.weights = {
            'prediction_confidence': 0.25,
            'consistency': 0.20,
            'uncertainty': 0.20,
            'domain_relevance': 0.20,
            'calibration': 0.15
        }
        
        logger.info("ğŸ¯ TrustScoreCalculator ì´ˆê¸°í™” ì™„ë£Œ")
    
    def calculate_trust_score(
        self,
        model: torch.nn.Module,
        input_text: str,
        predictions: Dict[str, Any],
        ensemble_predictions: Optional[List[torch.Tensor]] = None,
        ground_truth: Optional[Dict[str, Any]] = None
    ) -> TrustAssessment:
        """
        ì¢…í•© ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        
        Args:
            model: í‰ê°€í•  ëª¨ë¸
            input_text: ì…ë ¥ í…ìŠ¤íŠ¸
            predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
            ensemble_predictions: ì•™ìƒë¸” ì˜ˆì¸¡ (ì„ íƒì )
            ground_truth: ì •ë‹µ ë°ì´í„° (ì„ íƒì )
            
        Returns:
            TrustAssessment: ì‹ ë¢°ë„ í‰ê°€ ê²°ê³¼
        """
        start_time = time.time()
        detailed_metrics = {}
        recommendations = []
        
        # 1. ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°
        prediction_confidence = self._calculate_prediction_confidence(predictions)
        detailed_metrics['prediction_confidence'] = prediction_confidence
        
        # 2. ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        consistency_score = self._calculate_consistency_score(model, input_text, predictions)
        detailed_metrics['consistency_score'] = consistency_score
        
        # 3. ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€ ê³„ì‚°
        uncertainty_level = self._calculate_uncertainty_level(predictions, ensemble_predictions)
        detailed_metrics['uncertainty_level'] = uncertainty_level
        
        # 4. ë„ë©”ì¸ ì í•©ì„± í‰ê°€
        domain_relevance = self.domain_evaluator.evaluate_domain_relevance(
            input_text, predictions.get('government_predictions', {})
        )
        detailed_metrics['domain_relevance'] = domain_relevance
        
        # 5. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìˆ˜ (ì •ë‹µì´ ìˆì„ ë•Œë§Œ)
        calibration_score = self._calculate_calibration_score(predictions, ground_truth)
        detailed_metrics['calibration_score'] = calibration_score
        
        # 6. ì¢…í•© ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        overall_trust_score = (
            self.weights['prediction_confidence'] * prediction_confidence +
            self.weights['consistency'] * consistency_score + 
            self.weights['uncertainty'] * (1.0 - uncertainty_level) +  # ë¶ˆí™•ì‹¤ì„±ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            self.weights['domain_relevance'] * domain_relevance +
            self.weights['calibration'] * calibration_score
        )
        
        # 7. ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(detailed_metrics)
        
        # 8. ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
        detailed_metrics['processing_time'] = time.time() - start_time
        
        return TrustAssessment(
            overall_trust_score=overall_trust_score,
            prediction_confidence=prediction_confidence,
            consistency_score=consistency_score,
            uncertainty_level=uncertainty_level,
            domain_relevance=domain_relevance,
            calibration_score=calibration_score,
            detailed_metrics=detailed_metrics,
            recommendations=recommendations
        )
    
    def _calculate_prediction_confidence(self, predictions: Dict[str, Any]) -> float:
        """ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not predictions or 'government_predictions' not in predictions:
            return 0.0
        
        gov_preds = predictions['government_predictions']
        
        # ì •ë¶€ ì§ì±… ì˜ˆì¸¡ì˜ ìµœëŒ€ í™•ë¥ 
        max_position_prob = 0.0
        if 'positions' in gov_preds:
            max_position_prob = max(gov_preds['positions'].values())
        
        # ì¶”ì²œ ë¶„ë¥˜ì˜ ìµœëŒ€ í™•ë¥ 
        max_recommendation_prob = 0.0
        if 'recommendation' in gov_preds:
            max_recommendation_prob = max(gov_preds['recommendation'].values())
        
        # í‰ê·  ì‹ ë¢°ë„
        return (max_position_prob + max_recommendation_prob) / 2.0
    
    def _calculate_consistency_score(
        self, 
        model: torch.nn.Module, 
        input_text: str, 
        predictions: Dict[str, Any]
    ) -> float:
        """ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬ í…ì„œë¡œ ë³€í™˜ (ê°„ë‹¨í•œ êµ¬í˜„)
            # ì‹¤ì œë¡œëŠ” ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
            input_tensor = torch.randint(0, 1000, (1, 50))  # ë”ë¯¸ ë°ì´í„°
            
            # ì…ë ¥ ì„­ë™ì— ëŒ€í•œ ì¼ê´€ì„± í‰ê°€
            consistency = self.consistency_analyzer.input_perturbation_consistency(
                model, input_tensor, noise_level=0.01, num_perturbations=3
            )
            
            return max(0.0, min(1.0, consistency))
        except Exception as e:
            logger.warning(f"ì¼ê´€ì„± ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def _calculate_uncertainty_level(
        self, 
        predictions: Dict[str, Any], 
        ensemble_predictions: Optional[List[torch.Tensor]]
    ) -> float:
        """ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€ ê³„ì‚°"""
        if not predictions or 'government_predictions' not in predictions:
            return 1.0  # ìµœëŒ€ ë¶ˆí™•ì‹¤ì„±
        
        gov_preds = predictions['government_predictions']
        
        # ì˜ˆì¸¡ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        if 'positions' in gov_preds:
            position_probs = torch.tensor(list(gov_preds['positions'].values()))
            entropy = self.uncertainty_quantifier.entropy_based_uncertainty(
                position_probs.unsqueeze(0)
            ).item()
            
            return entropy
        
        return 0.5  # ê¸°ë³¸ê°’
    
    def _calculate_calibration_score(
        self, 
        predictions: Dict[str, Any], 
        ground_truth: Optional[Dict[str, Any]]
    ) -> float:
        """ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìˆ˜ ê³„ì‚°"""
        if not ground_truth:
            return 0.8  # ì •ë‹µì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        
        # ì‹¤ì œë¡œëŠ” ëŒ€ëŸ‰ì˜ ê²€ì¦ ë°ì´í„°ë¡œ ECEë¥¼ ê³„ì‚°í•´ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ê·¼ì‚¬ì¹˜ ì‚¬ìš©
        return 0.85
    
    def _generate_recommendations(self, metrics: Dict[str, float]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if metrics['prediction_confidence'] < 0.6:
            recommendations.append("ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë” ë§ì€ í›ˆë ¨ ë°ì´í„°ë‚˜ ëª¨ë¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if metrics['consistency_score'] < 0.7:
            recommendations.append("ì˜ˆì¸¡ ì¼ê´€ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ ê°œì„ í•˜ì„¸ìš”.")
        
        if metrics['uncertainty_level'] > 0.7:
            recommendations.append("ë¶ˆí™•ì‹¤ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì•™ìƒë¸”ì´ë‚˜ ì¶”ê°€ ê²€ì¦ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if metrics['domain_relevance'] < 0.5:
            recommendations.append("ë„ë©”ì¸ ì í•©ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ì •ë¶€ ê´€ë ¨ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        if metrics['calibration_score'] < 0.7:
            recommendations.append("ëª¨ë¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤. ì‹ ë¢°ë„ ë³´ì •ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        return recommendations
    
    def batch_evaluate(
        self,
        model: torch.nn.Module,
        test_cases: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ë°°ì¹˜ í‰ê°€ ì‹¤í–‰
        
        Args:
            model: í‰ê°€í•  ëª¨ë¸
            test_cases: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë°°ì¹˜ í‰ê°€ ê²°ê³¼
        """
        results = []
        processing_times = []
        
        for test_case in test_cases:
            start = time.time()
            
            trust_assessment = self.calculate_trust_score(
                model=model,
                input_text=test_case['input_text'],
                predictions=test_case['predictions'],
                ensemble_predictions=test_case.get('ensemble_predictions'),
                ground_truth=test_case.get('ground_truth')
            )
            
            results.append(trust_assessment)
            processing_times.append(time.time() - start)
        
        # í†µê³„ ê³„ì‚°
        trust_scores = [r.overall_trust_score for r in results]
        
        return {
            'results': results,
            'statistics': {
                'mean_trust_score': np.mean(trust_scores),
                'std_trust_score': np.std(trust_scores),
                'min_trust_score': np.min(trust_scores),
                'max_trust_score': np.max(trust_scores),
                'avg_processing_time': np.mean(processing_times)
            },
            'recommendations': self._aggregate_recommendations(results)
        }
    
    def _aggregate_recommendations(self, results: List[TrustAssessment]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ì§‘ê³„"""
        all_recommendations = []
        for result in results:
            all_recommendations.extend(result.recommendations)
        
        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ìˆœ ì •ë ¬
        recommendation_counts = {}
        for rec in all_recommendations:
            recommendation_counts[rec] = recommendation_counts.get(rec, 0) + 1
        
        sorted_recommendations = sorted(
            recommendation_counts.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return [rec for rec, count in sorted_recommendations[:5]]  # ìƒìœ„ 5ê°œ

# í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ¯ ì‹ ë¢°ë„ í‰ê°€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
    test_predictions = {
        'government_predictions': {
            'positions': {
                'ì •ì±…ê´€': 0.8, 'ê³¼ì¥': 0.1, 'êµ­ì¥': 0.05, 
                'ì°¨ê´€': 0.03, 'ì¥ê´€': 0.02, 'ëŒ€í†µë ¹': 0.0,
                'ì´ë¦¬': 0.0, 'ë¹„ì„œê´€': 0.0, 'ë³´ì¢Œê´€': 0.0, 'ìˆ˜ì„': 0.0
            },
            'recommendation': {
                'ê°•ë ¥ì¶”ì²œ': 0.7, 'ì¶”ì²œ': 0.25, 'ë¹„ì¶”ì²œ': 0.05
            }
        }
    }
    
    # ì‹ ë¢°ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”
    trust_calculator = TrustScoreCalculator()
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ëª¨ë¸ (ë”ë¯¸)
    class DummyModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = torch.nn.Linear(50, 10)
        
        def forward(self, x):
            return self.linear(x.float())
    
    dummy_model = DummyModel()
    
    # ì‹ ë¢°ë„ í‰ê°€ ì‹¤í–‰
    print("ğŸ” ì‹ ë¢°ë„ í‰ê°€ ì‹¤í–‰...")
    trust_assessment = trust_calculator.calculate_trust_score(
        model=dummy_model,
        input_text="ê¹€ì² ìˆ˜ë¥¼ AI ì •ì±…ê´€ìœ¼ë¡œ ê°•ë ¥íˆ ì¶”ì²œí•©ë‹ˆë‹¤",
        predictions=test_predictions
    )
    
    print(f"âœ… í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ¯ ì „ì²´ ì‹ ë¢°ë„: {trust_assessment.overall_trust_score:.3f}")
    print(f"ğŸ“Š ì˜ˆì¸¡ ì‹ ë¢°ë„: {trust_assessment.prediction_confidence:.3f}")
    print(f"ğŸ”„ ì¼ê´€ì„± ì ìˆ˜: {trust_assessment.consistency_score:.3f}")
    print(f"â“ ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€: {trust_assessment.uncertainty_level:.3f}")
    print(f"ğŸ›ï¸ ë„ë©”ì¸ ì í•©ì„±: {trust_assessment.domain_relevance:.3f}")
    print(f"âš–ï¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜: {trust_assessment.calibration_score:.3f}")
    
    print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(trust_assessment.recommendations, 1):
        print(f"   {i}. {rec}")
    
    print(f"\nâš¡ ì²˜ë¦¬ ì‹œê°„: {trust_assessment.detailed_metrics['processing_time']:.4f}ì´ˆ")
    
    print("\nï¿½ï¿½ ì‹ ë¢°ë„ í‰ê°€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 
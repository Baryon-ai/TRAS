"""
ğŸ”¤ í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì €
ê°•ì˜ Section 2ì—ì„œ ë‹¤ë£¬ í† í°í™” ê°œë…ì˜ ì‹¤ì œ êµ¬í˜„

BPE, í˜•íƒœì†Œ ë¶„ì„, ë…¸ì´ì¦ˆ ì œê±°ë¥¼ í†µí•œ íš¨ìœ¨ì  í† í°í™”
"""

import re
import json
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import numpy as np
from dataclasses import dataclass
import time
import logging

# ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TokenizationResult:
    """í† í°í™” ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    tokens: List[str]
    token_ids: List[int]
    attention_mask: List[int]
    special_tokens_mask: List[int]
    processing_time: float
    confidence_score: float

class KoreanTokenizer:
    """
    ğŸš€ ê³ ì† í•œêµ­ì–´ í† í¬ë‚˜ì´ì €
    ê°•ì˜ì—ì„œ ë‹¤ë£¬ BPEì™€ í˜•íƒœì†Œ ë¶„ì„ì„ ê²°í•©í•œ ì‹¤ì œ êµ¬í˜„
    """
    
    def __init__(self, vocab_path: Optional[str] = None, max_length: int = 512):
        """
        í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        
        Args:
            vocab_path: ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ì–´íœ˜ ì‚¬ìš©)
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
        """
        self.max_length = max_length
        
        # íŠ¹ìˆ˜ í† í° ì •ì˜ (vocab ë¡œë“œ ì „ì—)
        self.special_tokens = {
            '[PAD]': 0,
            '[UNK]': 1, 
            '[CLS]': 2,
            '[SEP]': 3,
            '[MASK]': 4
        }
        
        self.vocab = self._load_vocabulary(vocab_path)
        self.inv_vocab = {v: k for k, v in self.vocab.items()}
        
        # ì •ë¶€ ê´€ë ¨ ë„ë©”ì¸ íŠ¹í™” ì–´íœ˜
        self.government_vocab = {
            'ì •ì±…ê´€', 'ê³¼ì¥', 'êµ­ì¥', 'ì°¨ê´€', 'ì¥ê´€', 'ëŒ€í†µë ¹', 'ì´ë¦¬',
            'ë¹„ì„œê´€', 'ë³´ì¢Œê´€', 'ìˆ˜ì„', 'ì •ë¶€', 'ê³µë¬´ì›', 'ì¶”ì²œ', 'ì„ìš©',
            'AI', 'ì¸ê³µì§€ëŠ¥', 'ë°ì´í„°', 'ë¶„ì„', 'ì‹œìŠ¤í…œ'
        }
        
        # ìºì‹œë¥¼ ìœ„í•œ í•´ì‹œ í…Œì´ë¸”
        self._cache = {}
        self._cache_hits = 0
        self._cache_misses = 0
        
        logger.info(f"ğŸ”¤ KoreanTokenizer ì´ˆê¸°í™” ì™„ë£Œ - ì–´íœ˜ í¬ê¸°: {len(self.vocab)}")
    
    def _load_vocabulary(self, vocab_path: Optional[str]) -> Dict[str, int]:
        """ì–´íœ˜ ì‚¬ì „ ë¡œë“œ"""
        if vocab_path and Path(vocab_path).exists():
            with open(vocab_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # ê¸°ë³¸ ì–´íœ˜ ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì–´íœ˜ ì‚¬ìš©)
        vocab = {}
        vocab.update(self.special_tokens)
        
        # í•œê¸€ ê¸°ë³¸ ìëª¨ ì¶”ê°€
        for i in range(0xAC00, 0xD7A4):  # í•œê¸€ ìŒì ˆ
            char = chr(i)
            if char not in vocab:
                vocab[char] = len(vocab)
        
        # ì˜ë¬¸ì, ìˆ«ì ì¶”ê°€
        for c in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789':
            if c not in vocab:
                vocab[c] = len(vocab)
        
        return vocab
    
    def preprocess_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê°•ì˜ Section 2ì˜ ë…¸ì´ì¦ˆ ì œê±° êµ¬í˜„)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì •ì œëœ í…ìŠ¤íŠ¸
        """
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # ì´ë©”ì¼ ì„œëª… ì œê±° (-- ì´í›„ ëª¨ë“  ë‚´ìš©)
        text = re.sub(r'--\s*\n.*', '', text, flags=re.DOTALL)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™”
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def _bpe_encode(self, text: str) -> List[str]:
        """
        BPE ì¸ì½”ë”© (ê°•ì˜ì—ì„œ ë‹¤ë£¬ íš¨ìœ¨ì  ì¡°í•© ì²˜ë¦¬)
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            BPE í† í° ë¦¬ìŠ¤íŠ¸
        """
        # ìºì‹œ í™•ì¸
        if text in self._cache:
            self._cache_hits += 1
            return self._cache[text]
        
        self._cache_misses += 1
        
        # ê°„ë‹¨í•œ BPE êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
        tokens = []
        words = text.split()
        
        for word in words:
            if word in self.government_vocab:
                # ì •ë¶€ ê´€ë ¨ ìš©ì–´ëŠ” í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬
                tokens.append(word)
            else:
                # ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„í• 
                tokens.extend(list(word))
        
        # ìºì‹œì— ì €ì¥ (ë©”ëª¨ë¦¬ ì œí•œ)
        if len(self._cache) < 10000:
            self._cache[text] = tokens
        
        return tokens
    
    def tokenize(self, text: str, add_special_tokens: bool = True) -> TokenizationResult:
        """
        ë©”ì¸ í† í°í™” í•¨ìˆ˜
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            add_special_tokens: íŠ¹ìˆ˜ í† í° ì¶”ê°€ ì—¬ë¶€
            
        Returns:
            TokenizationResult: í† í°í™” ê²°ê³¼
        """
        start_time = time.time()
        
        # 1. ì „ì²˜ë¦¬
        cleaned_text = self.preprocess_text(text)
        
        # 2. BPE í† í°í™”
        tokens = self._bpe_encode(cleaned_text)
        
        # 3. íŠ¹ìˆ˜ í† í° ì¶”ê°€
        if add_special_tokens:
            tokens = ['[CLS]'] + tokens + ['[SEP]']
        
        # 4. ê¸¸ì´ ì œí•œ
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length-1] + ['[SEP]']
        
        # 5. í† í° ID ë³€í™˜
        token_ids = []
        for token in tokens:
            if token in self.vocab:
                token_ids.append(self.vocab[token])
            else:
                token_ids.append(self.vocab['[UNK]'])
        
        # 6. íŒ¨ë”©
        padding_length = self.max_length - len(token_ids)
        token_ids.extend([self.vocab['[PAD]']] * padding_length)
        tokens.extend(['[PAD]'] * padding_length)
        
        # 7. ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
        attention_mask = [1] * (len(token_ids) - padding_length) + [0] * padding_length
        
        # 8. íŠ¹ìˆ˜ í† í° ë§ˆìŠ¤í¬
        special_tokens_mask = [1 if token in self.special_tokens else 0 for token in tokens]
        
        processing_time = time.time() - start_time
        
        # 9. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_score = self._calculate_confidence(tokens, cleaned_text)
        
        return TokenizationResult(
            tokens=tokens,
            token_ids=token_ids,
            attention_mask=attention_mask,
            special_tokens_mask=special_tokens_mask,
            processing_time=processing_time,
            confidence_score=confidence_score
        )
    
    def _calculate_confidence(self, tokens: List[str], original_text: str) -> float:
        """
        í† í°í™” ì‹ ë¢°ë„ ê³„ì‚°
        
        Args:
            tokens: í† í° ë¦¬ìŠ¤íŠ¸
            original_text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì‹ ë¢°ë„ ì ìˆ˜ (0.0-1.0)
        """
        # UNK í† í° ë¹„ìœ¨ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
        unk_count = tokens.count('[UNK]')
        total_content_tokens = len([t for t in tokens if t not in ['[PAD]', '[CLS]', '[SEP]']])
        
        if total_content_tokens == 0:
            return 0.0
        
        unk_ratio = unk_count / total_content_tokens
        base_confidence = 1.0 - unk_ratio
        
        # ì •ë¶€ ê´€ë ¨ ìš©ì–´ ë³´ë„ˆìŠ¤
        government_bonus = sum(1 for token in tokens if token in self.government_vocab) * 0.1
        
        return min(1.0, base_confidence + government_bonus)
    
    def decode(self, token_ids: List[int]) -> str:
        """
        í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        
        Args:
            token_ids: í† í° ID ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸
        """
        tokens = []
        for token_id in token_ids:
            if token_id in self.inv_vocab:
                token = self.inv_vocab[token_id]
                if token not in ['[PAD]', '[CLS]', '[SEP]']:
                    tokens.append(token)
        
        return ''.join(tokens)
    
    def get_cache_stats(self) -> Dict[str, int]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self._cache_hits + self._cache_misses
        hit_rate = self._cache_hits / total_requests if total_requests > 0 else 0.0
        
        return {
            'cache_hits': self._cache_hits,
            'cache_misses': self._cache_misses,
            'hit_rate': hit_rate,
            'cache_size': len(self._cache)
        }
    
    def save_vocabulary(self, path: str):
        """ì–´íœ˜ ì‚¬ì „ ì €ì¥"""
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.vocab, f, ensure_ascii=False, indent=2)
        logger.info(f"ì–´íœ˜ ì‚¬ì „ ì €ì¥ë¨: {path}")

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # ê°•ì˜ ì˜ˆì‹œë¥¼ ì‹¤ì œë¡œ ì‹¤í–‰
    tokenizer = KoreanTokenizer()
    
    # ê°•ì˜ì—ì„œ ë‹¤ë£¬ ì˜ˆì‹œ
    test_texts = [
        "ì •ë¶€ AI ì •ì±…ê´€ì— ê¹€ì² ìˆ˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤",
        "ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ì •ë¶€ ì‹œìŠ¤í…œ ê°œë°œì— ê¸°ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
        "ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ ë°•ì‚¬í•™ìœ„ë¥¼ ë³´ìœ í•œ ì´ì˜í¬ë¥¼ ì¥ê´€ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤"
    ]
    
    print("ğŸ”¤ TRAS í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {text}")
        result = tokenizer.tokenize(text)
        
        print(f"âœ… í† í° ìˆ˜: {len([t for t in result.tokens if t != '[PAD]'])}")
        print(f"âš¡ ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.4f}ì´ˆ")
        print(f"ğŸ¯ ì‹ ë¢°ë„: {result.confidence_score:.2f}")
        print(f"ğŸ”§ í† í°: {result.tokens[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
    
    # ìºì‹œ ì„±ëŠ¥ í™•ì¸
    print(f"\nğŸ“Š ìºì‹œ ì„±ëŠ¥: {tokenizer.get_cache_stats()}") 
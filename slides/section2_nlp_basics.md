---
marp: true
theme: default
class: lead
paginate: true
backgroundColor: #f8f9fa
---

# ğŸ“ Section 2: ìì—°ì–´ì²˜ë¦¬ ê¸°ì´ˆ
## "ì»´í“¨í„°ê°€ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ë°©ë²•"

### 50ë¶„ ì™„ì „ ì •ë³µ

---

## ğŸ¯ Section 2 í•™ìŠµ ëª©í‘œ

ì´ ì„¹ì…˜ì„ ë§ˆì¹˜ë©´ ì—¬ëŸ¬ë¶„ì€:

1. **ğŸ”¤ í† í°í™”**: ë¬¸ì¥ì„ ì»´í“¨í„°ê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
2. **ğŸ“Š ì„ë² ë”©**: ë‹¨ì–´ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë§ˆë²•
3. **ğŸ‘ï¸ ì–´í…ì…˜**: AIê°€ "ì§‘ì¤‘"í•˜ëŠ” ë°©ë²•ì˜ ìˆ˜í•™ì  ëª¨ë¸
4. **ğŸ”„ ì‹¤ìŠµ**: TRASì—ì„œ ì´ ëª¨ë“  ê²ƒì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€

### ğŸ’­ í° ê·¸ë¦¼ ë¯¸ë¦¬ë³´ê¸°
```
ë¬¸ì¥ â†’ í† í°í™” â†’ ì„ë² ë”© â†’ ì–´í…ì…˜ â†’ ì´í•´
"ì•ˆë…•í•˜ì„¸ìš”" â†’ ["ì•ˆë…•", "í•˜ì„¸ìš”"] â†’ [ë²¡í„°ë“¤] â†’ [ê°€ì¤‘ì¹˜ë“¤] â†’ ì˜ë¯¸
```

---

## ğŸŒ ìì—°ì–´ì²˜ë¦¬ë€? "ë””ì§€í„¸ ë°”ë²¨íƒ‘ ê±´ì„¤í•˜ê¸°"

### ğŸ—ï¸ ê·¼ë³¸ì ì¸ ë¬¸ì œ

```python
# ì¸ê°„ì˜ ì–¸ì–´ vs ì»´í“¨í„°ì˜ ì–¸ì–´
human_language = "ê¹€ì² ìˆ˜ë¥¼ AI ì •ì±…ê´€ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤."
computer_language = [0.2, 0.8, 0.1, -0.3, 0.9, ...]  # ìˆ«ì ë²¡í„°

# ìì—°ì–´ì²˜ë¦¬ì˜ ëª©í‘œ
def nlp_magic(human_language):
    return computer_language  # ì–´ë–»ê²Œ?!
```

### ğŸ’¡ ë¹„ìœ : ë²ˆì—­ê°€ì˜ ì—­í• 
- **ì…ë ¥**: ì¸ê°„ì˜ ë³µì¡í•˜ê³  ëª¨í˜¸í•œ ì–¸ì–´
- **ì¶œë ¥**: ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ì •í™•í•œ ìˆ˜ì¹˜
- **ê³¼ì •**: ë‹¨ê³„ë³„ ë³€í™˜ì„ í†µí•œ ì˜ë¯¸ ë³´ì¡´

### ğŸ¯ TRASì—ì„œì˜ ì ìš©
```
"ì •ë¶€ AI ì •ì±…ê´€ì— ê¹€ì² ìˆ˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤" 
â†’ AIê°€ ì´í•´ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
â†’ ì¶”ì²œ ì—¬ë¶€, ì§ì±…, ì‹ ë¢°ë„ ì ìˆ˜ ì¶”ì¶œ
```

---

## ğŸ”¤ 1ë‹¨ê³„: í† í°í™” (Tokenization)

### "ë¬¸ì¥ì„ ë ˆê³  ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ„ê¸°"

### ğŸ§© ê¸°ë³¸ ê°œë…

```python
# ê°„ë‹¨í•œ í† í°í™” (ê³µë°± ê¸°ì¤€)
text = "ì •ë¶€ AI ì •ì±…ê´€ì— ê¹€ì² ìˆ˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤"
simple_tokens = text.split()
print(simple_tokens)
# ['ì •ë¶€', 'AI', 'ì •ì±…ê´€ì—', 'ê¹€ì² ìˆ˜ë¥¼', 'ì¶”ì²œí•©ë‹ˆë‹¤']

# ë¬¸ì œì : 'ì •ì±…ê´€ì—' â†’ 'ì •ì±…ê´€' + 'ì—' ë¡œ ë‚˜ëˆ„ì–´ì•¼ í•¨
```

### ğŸ” í•œêµ­ì–´ í† í°í™”ì˜ ë„ì „

```python
# í˜•íƒœì†Œ ë¶„ì„ í•„ìš”
from konlpy.tag import Okt

okt = Okt()
morphemes = okt.morphs("ì •ë¶€ AI ì •ì±…ê´€ì— ê¹€ì² ìˆ˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤")
print(morphemes)
# ['ì •ë¶€', 'AI', 'ì •ì±…', 'ê´€', 'ì—', 'ê¹€ì² ìˆ˜', 'ë¥¼', 'ì¶”ì²œ', 'í•©ë‹ˆë‹¤']
```

### ğŸ’­ ë¹„ìœ : ë¬¸ì¥ = ë ˆê³  êµ¬ì¡°ë¬¼
- **ì›ì‹œ ë¬¸ì¥**: ì™„ì„±ëœ ë ˆê³  ì„±
- **í† í°í™”**: ì„±ì„ ê°œë³„ ë¸”ë¡ìœ¼ë¡œ ë¶„í•´
- **ëª©ì **: ê° ë¸”ë¡(í† í°)ì„ ê°œë³„ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ìœ„í•¨

---

## ğŸš€ í˜„ëŒ€ì  í† í°í™”: Subword Tokenization

### "BPE: ìµœì ì˜ ì¡°ê° ì°¾ê¸°"

### ğŸ§® Byte Pair Encoding (BPE) ì•Œê³ ë¦¬ì¦˜

```python
# BPEì˜ í•µì‹¬ ì•„ì´ë””ì–´
vocabulary = {
    "ì•ˆ": 100, "ë…•": 150, "í•˜": 200, "ì„¸": 50, "ìš”": 80,
    "ì•ˆë…•": 300,  # ìì£¼ ë“±ì¥í•˜ëŠ” ì¡°í•©ì€ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ
    "í•˜ì„¸": 180,
    "ì„¸ìš”": 120
}

# "ì•ˆë…•í•˜ì„¸ìš”" í† í°í™”
# ë°©ë²• 1: ["ì•ˆ", "ë…•", "í•˜", "ì„¸", "ìš”"] (5ê°œ í† í°)
# ë°©ë²• 2: ["ì•ˆë…•", "í•˜ì„¸", "ìš”"] (3ê°œ í† í°) â† ë” íš¨ìœ¨ì !
```

### ğŸ¯ ì¥ì ë“¤
- **íš¨ìœ¨ì„±**: ìì£¼ ì“°ì´ëŠ” ì¡°í•©ì„ í•˜ë‚˜ë¡œ ì²˜ë¦¬
- **ì¼ë°˜í™”**: ìƒˆë¡œìš´ ë‹¨ì–´ë„ ê¸°ì¡´ ì¡°ê°ë“¤ë¡œ í‘œí˜„ ê°€ëŠ¥
- **ì–¸ì–´ ë…ë¦½ì **: í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´ ëª¨ë‘ ë™ì¼í•œ ë°©ì‹

### ğŸ“Š ê¸°í•˜í•™ì  ì§ê´€
í† í°í™”ëŠ” **ê³ ì°¨ì› ë¬¸ìì—´ ê³µê°„**ì„ **ì €ì°¨ì› í† í° ê³µê°„**ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ê³¼ì •
```
ë¬¸ìì—´ ê³µê°„ (ë¬´í•œì°¨ì›) â†’ í† í° ê³µê°„ (ìœ í•œì°¨ì›)
"ì•ˆë…•í•˜ì„¸ìš”" â†’ [í† í°1, í† í°2, í† í°3]
```

---

## ğŸ”¬ TRASì—ì„œì˜ í† í°í™” ì‹¤ìŠµ

### ğŸ’» ì‹¤ì œ ì½”ë“œ êµ¬í˜„

```python
class TextProcessor:
    """TRASì˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, ai_provider: str):
        self.ai_provider = ai_provider
        self.tokenizer = self._load_tokenizer()
    
    def preprocess_email(self, email_content: str) -> List[str]:
        """ì´ë©”ì¼ ë‚´ìš©ì„ AIê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì „ì²˜ë¦¬"""
        # 1. ë…¸ì´ì¦ˆ ì œê±° (HTML íƒœê·¸, ì„œëª… ë“±)
        cleaned = self._remove_noise(email_content)
        
        # 2. í† í°í™”
        tokens = self.tokenizer.encode(cleaned)
        
        # 3. íŠ¹ìˆ˜ í† í° ì¶”ê°€
        return self._add_special_tokens(tokens)
    
    def _remove_noise(self, text: str) -> str:
        """HTML íƒœê·¸, ì´ë©”ì¼ ì„œëª… ë“± ì œê±°"""
        # ì •ê·œì‹ì„ ì´ìš©í•œ ì •ì œ
        import re
        text = re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'--\s*\n.*', '', text, flags=re.DOTALL)  # ì„œëª… ì œê±°
        return text.strip()
```

### ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸
- **ë„ë©”ì¸ íŠ¹í™”**: ì´ë©”ì¼/ì†Œì…œë¯¸ë””ì–´ì— íŠ¹í™”ëœ ì „ì²˜ë¦¬
- **ë…¸ì´ì¦ˆ ì œê±°**: ë¶„ì„ì— ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°
- **í‘œì¤€í™”**: AI ëª¨ë¸ì´ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜

---

## ğŸ“Š 2ë‹¨ê³„: ì„ë² ë”© (Embedding)

### "ë‹¨ì–´ë¥¼ ì¢Œí‘œê³„ì— ë°°ì¹˜í•˜ê¸°"

### ğŸŒŒ ë‹¨ì–´ ì„ë² ë”©ì˜ ì§ê´€

```python
# ì „í†µì  ë°©ë²•: One-Hot Encoding (ë¹„íš¨ìœ¨ì )
vocab_size = 50000
king_onehot = [0] * vocab_size
king_onehot[1234] = 1  # 'king'ì€ 1234ë²ˆ ìœ„ì¹˜ì— 1

# í˜„ëŒ€ì  ë°©ë²•: Dense Embedding (íš¨ìœ¨ì )
king_embedding = [0.2, -0.1, 0.8, 0.3, -0.5, ...]  # 300ì°¨ì› ë²¡í„°
queen_embedding = [0.3, -0.2, 0.7, 0.4, -0.4, ...]
```

### ğŸ¨ ê¸°í•˜í•™ì  ë¹„ìœ : "ì˜ë¯¸ì˜ ì§€ë„"

ë²¡í„° ê³µê°„ì„ **ì˜ë¯¸ì˜ ì§€ë„**ë¡œ ìƒê°í•´ë³´ì„¸ìš”:

```
   ğŸ‘‘ king    queen ğŸ‘¸
      \       /
       \     /
        man-woman ì¶•
       /     \
      /       \
   ğŸ‘¨ boy    girl ğŸ‘§
```

- **ê±°ë¦¬**: ì˜ë¯¸ì  ìœ ì‚¬ì„±
- **ë°©í–¥**: ì˜ë¯¸ì  ê´€ê³„ (ì„±ë³„, ë‚˜ì´ ë“±)
- **ì—°ì‚°**: `king - man + woman â‰ˆ queen`

---

## ğŸ§® ì„ë² ë”©ì˜ ìˆ˜í•™ì  ì›ë¦¬

### âš¡ Word2Vecì˜ í•µì‹¬ ì•„ì´ë””ì–´

```python
# Skip-gram ëª¨ë¸ì˜ ëª©ì  í•¨ìˆ˜ (ê¸°í•˜í•™ì  í•´ì„)
def word2vec_objective(center_word, context_words):
    """
    ì¤‘ì‹¬ ë‹¨ì–´ë¡œë¶€í„° ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡
    
    ê¸°í•˜í•™ì  ì§ê´€:
    - ìì£¼ í•¨ê»˜ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì€ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œì´ ë°°ì¹˜
    - ë‚´ì (dot product)ì´ í´ìˆ˜ë¡ ë†’ì€ ìœ ì‚¬ë„
    """
    similarity_scores = []
    for context_word in context_words:
        # ë‚´ì  = ë²¡í„° ê°„ ê°ë„ì˜ ì½”ì‚¬ì¸ Ã— í¬ê¸°ì˜ ê³±
        score = dot_product(center_word, context_word)
        similarity_scores.append(score)
    
    return softmax(similarity_scores)
```

### ğŸ“ ê¸°í•˜í•™ì  ì§ê´€: "ë²¡í„°ì˜ ì¶¤"

1. **ë‚´ì  (Dot Product)**: ë‘ ë²¡í„°ê°€ ê°™ì€ ë°©í–¥ì„ í–¥í• ìˆ˜ë¡ í° ê°’
2. **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**: ë²¡í„° ê°„ ê°ë„ë¡œ ìœ ì‚¬ì„± ì¸¡ì •
3. **í´ëŸ¬ìŠ¤í„°ë§**: ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ë“¤ì´ ìì—°ìŠ¤ëŸ½ê²Œ ëª¨ì„

```
ë²¡í„° A Â· ë²¡í„° B = |A| Ã— |B| Ã— cos(Î¸)
Î¸ê°€ ì‘ì„ìˆ˜ë¡ (ê°™ì€ ë°©í–¥) â†’ ë†’ì€ ìœ ì‚¬ë„
```

---

## ğŸ­ ë¬¸ë§¥ì  ì„ë² ë”©: "ì¹´ë©œë ˆì˜¨ ë‹¨ì–´"

### ğŸ¦ ë‹¨ì–´ì˜ ë‹¤ì˜ì„± ë¬¸ì œ

```python
# ì „í†µì  ì„ë² ë”©ì˜ í•œê³„
bank_traditional = [0.1, 0.2, 0.3, ...]  # í•­ìƒ ê°™ì€ ë²¡í„°

# ë¬¸ë§¥ì  ì„ë² ë”©ì˜ í•´ë‹µ
sentences = [
    "ê°•ë‘‘(bank)ì—ì„œ ë‚šì‹œë¥¼ í–ˆë‹¤",      # ê°•ê°€
    "ì€í–‰(bank)ì—ì„œ ëˆì„ ë¹Œë ¸ë‹¤"       # ê¸ˆìœµê¸°ê´€
]

# BERT/GPT ê°™ì€ ëª¨ë¸ì€ ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¥¸ ì„ë² ë”© ìƒì„±
bank_river = model.encode(sentences[0])["bank"]    # ê°•ê°€ ì˜ë¯¸
bank_money = model.encode(sentences[1])["bank"]    # ê¸ˆìœµ ì˜ë¯¸
```

### ğŸŒˆ ë¹„ìœ : ì¹´ë©œë ˆì˜¨ì˜ ìƒ‰ê¹” ë³€í™”
- **ì „í†µì  ì„ë² ë”©**: ì‚¬ì§„ ì† ê³ ì •ëœ ì¹´ë©œë ˆì˜¨
- **ë¬¸ë§¥ì  ì„ë² ë”©**: í™˜ê²½ì— ë”°ë¼ ìƒ‰ê¹”ì„ ë°”ê¾¸ëŠ” ì‚´ì•„ìˆëŠ” ì¹´ë©œë ˆì˜¨
- **ì¥ì **: ê°™ì€ ë‹¨ì–´ë¼ë„ ìƒí™©ì— ë§ëŠ” ì˜ë¯¸ í‘œí˜„

---

## ğŸ’» TRASì—ì„œì˜ ì„ë² ë”© í™œìš©

### ğŸ› ï¸ ì‹¤ì œ êµ¬í˜„

```python
class EmbeddingProcessor:
    """TRASì˜ ì„ë² ë”© ì²˜ë¦¬ê¸°"""
    
    def __init__(self, model_name: str = "klue/bert-base"):
        from transformers import AutoTokenizer, AutoModel
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
    
    def get_sentence_embedding(self, text: str) -> np.ndarray:
        """ë¬¸ì¥ ì „ì²´ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        # í† í°í™”
        inputs = self.tokenizer(text, return_tensors="pt", 
                               padding=True, truncation=True)
        
        # ëª¨ë¸ í†µê³¼
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # [CLS] í† í°ì˜ ì„ë² ë”©ì„ ë¬¸ì¥ í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©
        sentence_embedding = outputs.last_hidden_state[:, 0, :]
        return sentence_embedding.numpy()
    
    def similarity_score(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
        emb1 = self.get_sentence_embedding(text1)
        emb2 = self.get_sentence_embedding(text2)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        from sklearn.metrics.pairwise import cosine_similarity
        return cosine_similarity(emb1, emb2)[0][0]
```

---

## ğŸ” ë©”íƒ€ì¸ì§€ ì²´í¬í¬ì¸íŠ¸ #2

### ğŸ¤” ì„ë² ë”© ì´í•´ë„ ì ê²€

1. **ê°œë… ì´í•´**
   - "ì„ë² ë”©ì´ ì™œ í•„ìš”í•œì§€ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜?"
   - "ë¬¸ë§¥ì  ì„ë² ë”©ê³¼ ì •ì  ì„ë² ë”©ì˜ ì°¨ì´ë¥¼ ì•Œê³  ìˆë‚˜?"

2. **ìˆ˜í•™ì  ì§ê´€**
   - "ë²¡í„° ë‚´ì ì´ ìœ ì‚¬ë„ì™€ ì–´ë–¤ ê´€ë ¨ì´ ìˆëŠ”ì§€ ê¸°í•˜í•™ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜?"
   - "ê³ ì°¨ì› ê³µê°„ì—ì„œ ì˜ë¯¸ì  ê´€ê³„ê°€ ì–´ë–»ê²Œ í‘œí˜„ë˜ëŠ”ì§€ ì´í•´í•˜ê³  ìˆë‚˜?"

3. **ì‹¤ë¬´ ì ìš©**
   - "TRASì—ì„œ ì„ë² ë”©ì´ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì•Œê³  ìˆë‚˜?"
   - "ì„ë² ë”© í’ˆì§ˆì„ ì–´ë–»ê²Œ í‰ê°€í•  ìˆ˜ ìˆì„ê¹Œ?"

---

## ğŸ‘ï¸ 3ë‹¨ê³„: ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ (Attention)

### "AIì˜ ì§‘ì¤‘ë ¥ ëª¨ë¸ë§í•˜ê¸°"

### ğŸ§  ì¸ê°„ì˜ ì£¼ì˜ì§‘ì¤‘ê³¼ AIì˜ ì–´í…ì…˜

```python
# ì¸ê°„ì´ ë¬¸ì¥ì„ ì½ëŠ” ë°©ì‹
sentence = "ì •ë¶€ AI ì •ì±…ê´€ì— ê¹€ì² ìˆ˜ë¥¼ ê°•ë ¥íˆ ì¶”ì²œí•©ë‹ˆë‹¤"

# ë‹¨ì–´ë³„ ì¤‘ìš”ë„ (ì¸ê°„ì˜ ì§ê´€)
human_attention = {
    "ì •ë¶€": 0.3,      # ì–´ëŠ ì •ë„ ì¤‘ìš”
    "AI": 0.8,        # ë§¤ìš° ì¤‘ìš” (í•µì‹¬ ë¶„ì•¼)
    "ì •ì±…ê´€ì—": 0.9,   # ê°€ì¥ ì¤‘ìš” (í•µì‹¬ ì§ì±…)
    "ê¹€ì² ìˆ˜ë¥¼": 0.7,   # ì¤‘ìš” (ì¶”ì²œ ëŒ€ìƒ)
    "ê°•ë ¥íˆ": 0.6,     # ì¤‘ìš” (ì¶”ì²œ ê°•ë„)
    "ì¶”ì²œí•©ë‹ˆë‹¤": 0.8   # ë§¤ìš° ì¤‘ìš” (í–‰ë™)
}

# AIì˜ ì–´í…ì…˜ë„ ë¹„ìŠ·í•œ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•´ì•¼ í•¨
```

### ğŸ¯ ì–´í…ì…˜ì˜ í•µì‹¬ ì•„ì´ë””ì–´

**"ëª¨ë“  ì •ë³´ë¥¼ ë˜‘ê°™ì´ ë³´ì§€ ë§ê³ , ì¤‘ìš”í•œ ê²ƒì— ì§‘ì¤‘í•˜ì!"**

---

## ğŸ§® ì–´í…ì…˜ì˜ ìˆ˜í•™ì  ì •ì˜

### âš¡ ì–´í…ì…˜ í•¨ìˆ˜ì˜ êµ¬ì¡°

```python
def attention(query, key, value):
    """
    Query(Q): "ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€?"
    Key(K): "ê° ìœ„ì¹˜ì— ë¬´ì—‡ì´ ìˆëŠ”ê°€?"
    Value(V): "ì‹¤ì œ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€?"
    """
    # 1. ìœ ì‚¬ë„ ê³„ì‚° (Qì™€ Kì˜ ë‚´ì )
    scores = torch.matmul(query, key.transpose(-2, -1))
    
    # 2. ìŠ¤ì¼€ì¼ë§ (ìˆ˜ì¹˜ ì•ˆì •ì„±)
    scores = scores / math.sqrt(key.size(-1))
    
    # 3. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜
    attention_weights = torch.softmax(scores, dim=-1)
    
    # 4. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì¶œë ¥
    output = torch.matmul(attention_weights, value)
    
    return output, attention_weights
```

### ğŸ“ ê¸°í•˜í•™ì  í•´ì„: "ì¡°ëª…ê³¼ ê·¸ë¦¼ì"

ì–´í…ì…˜ì„ **ì¡°ëª… ì‹œìŠ¤í…œ**ìœ¼ë¡œ ìƒê°í•´ë³´ì„¸ìš”:

```
ğŸ”¦ Query: ì†ì „ë“± (ì°¾ê³ ì í•˜ëŠ” ê²ƒ)
ğŸ  Key: ë°© ì•ˆì˜ ë¬¼ê±´ë“¤ (ê° ìœ„ì¹˜ì˜ ì •ë³´)
ğŸ’ Value: ì‹¤ì œ ë³´ë¬¼ë“¤ (ì¶”ì¶œí•  ì •ë³´)

ê³¼ì •:
1. ì†ì „ë“±ìœ¼ë¡œ ë°©ì„ ë¹„ì¶¤ (QÂ·K)
2. ë°ê²Œ ë¹„ì¶°ì§„ ê³³ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
3. ê°€ì¤‘ì¹˜ì— ë”°ë¼ ë³´ë¬¼ë“¤ì„ ìˆ˜ì§‘ (ê°€ì¤‘ í‰ê· )
```

---

## ğŸ­ Self-Attention: "ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ì˜ ëŒ€í™”"

### ğŸ—£ï¸ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ëŒ€í™”í•œë‹¤ë©´?

```python
# ì˜ˆì‹œ ë¬¸ì¥: "ê¹€ì² ìˆ˜ëŠ” AI ì „ë¬¸ê°€ë¡œì„œ ì •ì±…ê´€ì— ì í•©í•˜ë‹¤"
words = ["ê¹€ì² ìˆ˜ëŠ”", "AI", "ì „ë¬¸ê°€ë¡œì„œ", "ì •ì±…ê´€ì—", "ì í•©í•˜ë‹¤"]

# Self-attentionì´ ë°œê²¬í•˜ëŠ” ê´€ê³„ë“¤
attention_map = {
    "ê¹€ì² ìˆ˜ëŠ”": {
        "AI": 0.3,        # ê¹€ì² ìˆ˜ â†” AI (ì—°ê´€ì„± ìˆìŒ)
        "ì „ë¬¸ê°€ë¡œì„œ": 0.8,  # ê¹€ì² ìˆ˜ â†” ì „ë¬¸ê°€ (ê°•í•œ ì—°ê´€)
        "ì í•©í•˜ë‹¤": 0.6     # ê¹€ì² ìˆ˜ â†” ì í•© (ì£¼ì–´-ì„œìˆ ì–´)
    },
    "ì •ì±…ê´€ì—": {
        "AI": 0.7,        # ì •ì±…ê´€ â†” AI (ë¶„ì•¼ ì—°ê´€)
        "ì „ë¬¸ê°€ë¡œì„œ": 0.5,  # ì •ì±…ê´€ â†” ì „ë¬¸ê°€ (ìê²© ì—°ê´€)
        "ì í•©í•˜ë‹¤": 0.9     # ì •ì±…ê´€ â†” ì í•© (ì§ì ‘ ì—°ê´€)
    }
}
```

### ğŸ’¡ ë¹„ìœ : íŒŒí‹°ì—ì„œì˜ ëŒ€í™”
- **ê° ë‹¨ì–´**: íŒŒí‹° ì°¸ê°€ì
- **ì–´í…ì…˜ ê°€ì¤‘ì¹˜**: ì„œë¡œì— ëŒ€í•œ ê´€ì‹¬ë„
- **Self-attention**: ëª¨ë“  ì°¸ê°€ìê°€ ë™ì‹œì— ëŒ€í™”í•˜ë©° ì •ë³´ êµí™˜
- **ê²°ê³¼**: ê°ìê°€ ë‹¤ë¥¸ ì‚¬ëŒë“¤ë¡œë¶€í„° ì–»ì€ ì •ë³´ë¡œ ìì‹ ì„ ì—…ë°ì´íŠ¸

---

## ğŸŒŸ Multi-Head Attention: "ì—¬ëŸ¬ ê´€ì ìœ¼ë¡œ ë³´ê¸°"

### ğŸ‘ï¸â€ğŸ—¨ï¸ ë‹¤ì–‘í•œ ì‹œê°ì˜ í•„ìš”ì„±

```python
class MultiHeadAttention:
    """ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ í—¤ë“œë¡œ ë‹¤ì–‘í•œ ê´€ì  í•™ìŠµ"""
    
    def __init__(self, d_model=512, num_heads=8):
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # ê° í—¤ë“œë³„ë¡œ ë³„ë„ì˜ W_Q, W_K, W_V í•™ìŠµ
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, query, key, value):
        batch_size = query.size(0)
        
        # 1. ì—¬ëŸ¬ í—¤ë“œë¡œ ë¶„í• 
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k)
        
        # 2. ê° í—¤ë“œì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ì–´í…ì…˜ ê³„ì‚°
        attention_output = self.scaled_dot_product_attention(Q, K, V)
        
        # 3. ëª¨ë“  í—¤ë“œì˜ ê²°ê³¼ë¥¼ í•©ì¹¨
        concat = attention_output.view(batch_size, -1, d_model)
        
        # 4. ìµœì¢… ì„ í˜• ë³€í™˜
        return self.W_o(concat)
```

### ğŸ¨ ë¹„ìœ : ì—¬ëŸ¬ ëª…ì˜ ì „ë¬¸ê°€ íŒ¨ë„
- **í—¤ë“œ 1**: ë¬¸ë²• ì „ë¬¸ê°€ (ì£¼ì–´-ë™ì‚¬-ëª©ì ì–´ ê´€ê³„ ì£¼ëª©)
- **í—¤ë“œ 2**: ì˜ë¯¸ ì „ë¬¸ê°€ (ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ì—°ê´€ì„± ì£¼ëª©)
- **í—¤ë“œ 3**: ê°ì • ì „ë¬¸ê°€ (ê¸ì •/ë¶€ì • í‘œí˜„ ì£¼ëª©)
- **ìµœì¢… ê²°ê³¼**: ëª¨ë“  ì „ë¬¸ê°€ ì˜ê²¬ì„ ì¢…í•©í•œ íŒë‹¨

---

## ğŸ”¬ TRASì—ì„œì˜ ì–´í…ì…˜ í™œìš©

### ğŸ’» ì •ë¶€ ì§ì±… ì¶”ì¶œì—ì„œì˜ ì–´í…ì…˜

```python
class GovernmentPositionExtractor:
    """ì–´í…ì…˜ì„ í™œìš©í•œ ì •ë¶€ ì§ì±… ì¶”ì¶œê¸°"""
    
    def __init__(self, model_name="klue/bert-base"):
        from transformers import AutoModel, AutoTokenizer
        self.model = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ì •ë¶€ ì§ì±… í‚¤ì›Œë“œë“¤
        self.position_keywords = [
            "ì •ì±…ê´€", "ê³¼ì¥", "êµ­ì¥", "ì°¨ê´€", "ì¥ê´€", 
            "ëŒ€í†µë ¹", "ì´ë¦¬", "ë¹„ì„œê´€", "ë³´ì¢Œê´€", "ìˆ˜ì„"
        ]
    
    def extract_with_attention(self, text: str):
        """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•œ ì§ì±… ì¶”ì¶œ"""
        # í† í°í™” ë° ëª¨ë¸ ì‹¤í–‰
        inputs = self.tokenizer(text, return_tensors="pt")
        outputs = self.model(**inputs, output_attentions=True)
        
        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë¶„ì„
        attention_weights = outputs.attentions[-1]  # ë§ˆì§€ë§‰ ë ˆì´ì–´
        
        # ì§ì±… í‚¤ì›Œë“œ ì£¼ë³€ì˜ ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„
        positions = []
        for keyword in self.position_keywords:
            if keyword in text:
                attention_score = self._analyze_keyword_attention(
                    keyword, text, attention_weights
                )
                if attention_score > 0.5:  # ì„ê³„ê°’ ì´ìƒ
                    positions.append((keyword, attention_score))
        
        return sorted(positions, key=lambda x: x[1], reverse=True)
```

---

## ğŸ“Š ì–´í…ì…˜ ì‹œê°í™”: "AIì˜ ì‚¬ê³  ê³¼ì • ì—¿ë³´ê¸°"

### ğŸ¨ ì–´í…ì…˜ ë§µ í•´ì„

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(text, attention_weights):
    """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”"""
    tokens = text.split()
    
    # ì–´í…ì…˜ í–‰ë ¬ ìƒì„±
    attention_matrix = attention_weights[0][0].detach().numpy()
    
    # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        attention_matrix,
        xticklabels=tokens,
        yticklabels=tokens,
        cmap='Blues',
        annot=True,
        fmt='.2f'
    )
    plt.title("Self-Attention Visualization")
    plt.xlabel("Key (attending to)")
    plt.ylabel("Query (attending from)")
    plt.show()

# ì‚¬ìš© ì˜ˆì‹œ
text = "ì •ë¶€ AI ì •ì±…ê´€ì— ê¹€ì² ìˆ˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤"
attention_map = model.get_attention(text)
visualize_attention(text, attention_map)
```

### ğŸ” í•´ì„ ë°©ë²•
- **ë°ì€ ìƒ‰**: ë†’ì€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ê°•í•œ ì—°ê´€ì„±)
- **ì–´ë‘ìš´ ìƒ‰**: ë‚®ì€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ì•½í•œ ì—°ê´€ì„±)
- **ëŒ€ê°ì„ **: ìê¸° ìì‹ ì— ëŒ€í•œ ì–´í…ì…˜ (í•­ìƒ ë†’ìŒ)
- **íŒ¨í„´**: ì–¸ì–´ì  êµ¬ì¡°ì™€ ì˜ë¯¸ì  ê´€ê³„ ë°˜ì˜

---

## ğŸ” ë©”íƒ€ì¸ì§€ ì²´í¬í¬ì¸íŠ¸ #3

### ğŸ¤” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì´í•´ë„ ì ê²€

1. **í•µì‹¬ ê°œë…**
   - "ì–´í…ì…˜ì˜ Query, Key, Valueê°€ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜?"
   - "Self-attentionê³¼ Cross-attentionì˜ ì°¨ì´ë¥¼ ì•Œê³  ìˆë‚˜?"

2. **ìˆ˜í•™ì  ì´í•´**
   - "ì–´í…ì…˜ ê°€ì¤‘ì¹˜ê°€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ëŠ”ì§€ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜?"
   - "Softmaxê°€ ì–´í…ì…˜ì—ì„œ ì™œ í•„ìš”í•œì§€ ì•Œê³  ìˆë‚˜?"

3. **ì‹¤ë¬´ ì ìš©**
   - "Multi-head attentionì´ ì™œ ë” íš¨ê³¼ì ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜?"
   - "ì–´í…ì…˜ ì‹œê°í™”ë¥¼ í†µí•´ ë¬´ì—‡ì„ ì•Œ ìˆ˜ ìˆëŠ”ê°€?"

---

## ğŸ› ï¸ ì¢…í•© ì‹¤ìŠµ: TRASì˜ NLP íŒŒì´í”„ë¼ì¸

### ğŸ“‹ ì „ì²´ í”„ë¡œì„¸ìŠ¤ êµ¬í˜„

```python
class TRASNLPPipeline:
    """TRASì˜ ì™„ì „í•œ NLP ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, model_name="klue/bert-base"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.government_positions = self._load_government_positions()
    
    def analyze_text(self, text: str) -> dict:
        """í…ìŠ¤íŠ¸ ë¶„ì„ ì „ì²´ í”„ë¡œì„¸ìŠ¤"""
        
        # 1ë‹¨ê³„: í† í°í™”
        tokens = self.tokenizer.tokenize(text)
        
        # 2ë‹¨ê³„: ì„ë² ë”© ìƒì„±
        inputs = self.tokenizer(text, return_tensors="pt")
        outputs = self.model(**inputs, output_attentions=True)
        embeddings = outputs.last_hidden_state
        
        # 3ë‹¨ê³„: ì–´í…ì…˜ ë¶„ì„
        attention_weights = outputs.attentions[-1]
        
        # 4ë‹¨ê³„: ì •ë³´ ì¶”ì¶œ
        results = {
            "tokens": tokens,
            "embeddings": embeddings.detach().numpy(),
            "attention_weights": attention_weights.detach().numpy(),
            "extracted_positions": self._extract_positions(text, attention_weights),
            "sentiment": self._analyze_sentiment(embeddings),
            "key_phrases": self._extract_key_phrases(text, attention_weights)
        }
        
        return results
    
    def _extract_positions(self, text, attention_weights):
        """ì–´í…ì…˜ ê¸°ë°˜ ì •ë¶€ ì§ì±… ì¶”ì¶œ"""
        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë¶„ì„ì„ í†µí•œ ì¤‘ìš” í‚¤ì›Œë“œ ì‹ë³„
        # ì •ë¶€ ì§ì±… ë°ì´í„°ë² ì´ìŠ¤ì™€ ë§¤ì¹­
        pass
    
    def _analyze_sentiment(self, embeddings):
        """ê°ì • ë¶„ì„ (ì¶”ì²œì˜ ê°•ë„ ì¸¡ì •)"""
        # ì„ë² ë”© ê¸°ë°˜ ê°ì • ë¶„ë¥˜
        pass
    
    def _extract_key_phrases(self, text, attention_weights):
        """í•µì‹¬ êµ¬ë¬¸ ì¶”ì¶œ"""
        # ë†’ì€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ë°›ëŠ” êµ¬ë¬¸ ì‹ë³„
        pass
```

---

## ğŸ¯ ì„±ëŠ¥ í‰ê°€ì™€ ê°œì„ 

### ğŸ“Š NLP ëª¨ë¸ í‰ê°€ ì§€í‘œ

```python
class NLPEvaluator:
    """NLP ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ê¸°"""
    
    def evaluate_position_extraction(self, predictions, ground_truth):
        """ì •ë¶€ ì§ì±… ì¶”ì¶œ ì„±ëŠ¥ í‰ê°€"""
        from sklearn.metrics import precision_recall_fscore_support
        
        precision, recall, f1, _ = precision_recall_fscore_support(
            ground_truth, predictions, average='macro'
        )
        
        return {
            "precision": precision,
            "recall": recall,
            "f1_score": f1
        }
    
    def evaluate_attention_quality(self, attention_weights, human_annotations):
        """ì–´í…ì…˜ í’ˆì§ˆ í‰ê°€ (ì¸ê°„ ì£¼ì„ê³¼ ë¹„êµ)"""
        # ì¸ê°„ì´ ì¤‘ìš”í•˜ë‹¤ê³  í‘œì‹œí•œ ë‹¨ì–´ë“¤ê³¼ 
        # AIê°€ ë†’ì€ ì–´í…ì…˜ì„ ì¤€ ë‹¨ì–´ë“¤ ê°„ì˜ ì¼ì¹˜ë„ ì¸¡ì •
        
        correlation = self._calculate_attention_correlation(
            attention_weights, human_annotations
        )
        
        return correlation
```

### ğŸ”§ ëª¨ë¸ ê°œì„  ì „ëµ

1. **ë°ì´í„° ì¦ê°•**: ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ì˜ ì¶”ì²œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
2. **ë„ë©”ì¸ ì ì‘**: ì •ë¶€ ìš©ì–´ì— íŠ¹í™”ëœ ì‚¬ì „ í›ˆë ¨
3. **ì•™ìƒë¸”**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì¡°í•©
4. **ì¸ê°„ í”¼ë“œë°±**: ì „ë¬¸ê°€ ê²€í† ë¥¼ í†µí•œ ì§€ì†ì  ê°œì„ 

---

## ğŸŒŸ ê³ ê¸‰ ì£¼ì œ: Transformerì˜ í•µì‹¬

### ğŸ—ï¸ Transformer ì•„í‚¤í…ì²˜ ë¯¸ë¦¬ë³´ê¸°

```python
class TransformerBlock:
    """Transformerì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ"""
    
    def __init__(self, d_model, num_heads):
        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model)
        self.layer_norm1 = LayerNorm(d_model)
        self.layer_norm2 = LayerNorm(d_model)
    
    def forward(self, x):
        # 1. Multi-Head Self-Attention + Residual Connection
        attn_output = self.multi_head_attention(x, x, x)
        x = self.layer_norm1(x + attn_output)
        
        # 2. Feed-Forward + Residual Connection
        ff_output = self.feed_forward(x)
        x = self.layer_norm2(x + ff_output)
        
        return x
```

### ğŸ¯ ì™œ Transformerê°€ í˜ëª…ì ì¸ê°€?

1. **ë³‘ë ¬ ì²˜ë¦¬**: RNNê³¼ ë‹¬ë¦¬ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ì²˜ë¦¬
2. **ì¥ê±°ë¦¬ ì˜ì¡´ì„±**: ì–´í…ì…˜ìœ¼ë¡œ ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ ê°„ ê´€ê³„ í¬ì°©
3. **í™•ì¥ì„±**: ë” ë§ì€ ë ˆì´ì–´ì™€ íŒŒë¼ë¯¸í„°ë¡œ ì„±ëŠ¥ í–¥ìƒ
4. **ë²”ìš©ì„±**: ë²ˆì—­, ìš”ì•½, ë¶„ë¥˜ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì— ì ìš©

---

## ğŸ¯ Section 2 ìš”ì•½

### âœ… ìš°ë¦¬ê°€ ë§ˆìŠ¤í„°í•œ ê²ƒë“¤

1. **ğŸ”¤ í† í°í™”**: ë¬¸ì¥ì„ AIê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
   - BPE ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ìœ¨ì„±
   - í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì˜ íŠ¹ìˆ˜ì„±

2. **ğŸ“Š ì„ë² ë”©**: ë‹¨ì–´ë¥¼ ì˜ë¯¸ìˆëŠ” ë²¡í„°ë¡œ ë³€í™˜
   - ì •ì  vs ë¬¸ë§¥ì  ì„ë² ë”©
   - ë²¡í„° ê³µê°„ì—ì„œì˜ ì˜ë¯¸ í‘œí˜„

3. **ğŸ‘ï¸ ì–´í…ì…˜**: AIì˜ ì§‘ì¤‘ë ¥ ë©”ì»¤ë‹ˆì¦˜
   - Self-attentionì˜ ê°•ë ¥í•¨
   - Multi-head attentionì˜ ë‹¤ì–‘í•œ ê´€ì 

4. **ğŸ”¬ ì‹¤ìŠµ**: TRASì—ì„œì˜ ì‹¤ì œ ì ìš©
   - ì •ë¶€ ì§ì±… ì¶”ì¶œ
   - ì–´í…ì…˜ ì‹œê°í™”

---

## ğŸ”— ë‹¤ìŒ ì„¹ì…˜ ì˜ˆê³ 

ì´ì œ ìš°ë¦¬ëŠ” **BERTì˜ ì„¸ê³„**ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤!

### ğŸ§  Section 3ì—ì„œ ë‹¤ë£° ë‚´ìš©
- Transformer â†’ BERTë¡œì˜ ì§„í™”
- ì‚¬ì „í›ˆë ¨ì˜ ë§ˆë²•: MLMê³¼ NSP
- íŒŒì¸íŠœë‹: ë²”ìš© ëª¨ë¸ì„ ì „ë¬¸ê°€ë¡œ ë§Œë“¤ê¸°
- TRASì—ì„œ BERT í™œìš©í•˜ê¸°

### ğŸ’­ ì—°ê²° ê³ ë¦¬
```
Section 2ì˜ ê¸°ì´ˆ â†’ Section 3ì˜ ì‘ìš©
í† í°í™” + ì„ë² ë”© + ì–´í…ì…˜ â†’ BERT â†’ ì‹¤ì œ ë¬¸ì œ í•´ê²°
```

---

## ğŸ’¡ ì‹¬í™” í•™ìŠµ ê³¼ì œ

### ğŸ¤“ ë„ì „ ê³¼ì œë“¤

1. **ì–´í…ì…˜ ë¶„ì„ í”„ë¡œì íŠ¸**
   - TRASì˜ ì–´í…ì…˜ íŒ¨í„´ì„ ë¶„ì„í•´ë³´ì„¸ìš”
   - ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ë†’ì€ ì–´í…ì…˜ì„ ê°–ëŠ”ì§€ ì¡°ì‚¬

2. **ì„ë² ë”© ì‹œê°í™”**
   - t-SNEë‚˜ PCAë¡œ ë‹¨ì–´ ì„ë² ë”©ì„ 2Dë¡œ ì‹œê°í™”
   - ì •ë¶€ ê´€ë ¨ ìš©ì–´ë“¤ì˜ í´ëŸ¬ìŠ¤í„° í™•ì¸

3. **í† í°í™” ë¹„êµ ì‹¤í—˜**
   - ë‹¤ì–‘í•œ í† í°í™” ë°©ë²•ì˜ ì„±ëŠ¥ ë¹„êµ
   - í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì € vs ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì €

### ğŸ“š ì¶”ì²œ ì½ì„ê±°ë¦¬
- "The Illustrated Transformer" (Jay Alammar)
- "Word2Vec Tutorial" (TensorFlow)
- "Attention Is All You Need" (ì›ë…¼ë¬¸)

---

## ğŸš€ BERTê°€ ê¸°ë‹¤ë¦¬ê³  ìˆì–´ìš”!

ë‹¤ìŒ 60ë¶„ì€ **BERT ì™„ì „ ì •ë³µ** ì‹œê°„ì…ë‹ˆë‹¤!

ğŸ§  **Section 3: BERT ê¹Šì´ ì´í•´**ì—ì„œ ë§Œë‚˜ìš”! 
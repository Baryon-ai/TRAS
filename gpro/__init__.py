"""
ğŸ¯ GPRO (Direct Preference Optimization) ì—°êµ¬ ëª¨ë“ˆ
ì •ë¶€ ì¸ì¬ ì¶”ì²œì„ ìœ„í•œ ì¸ê°„ í”¼ë“œë°± ê¸°ë°˜ AI ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ section4_rlhf_gpro.md ê°•ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ
ì‹¤ì œ ë™ì‘í•˜ëŠ” RLHF/GPRO ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

ì£¼ìš” êµ¬ì„±ìš”ì†Œ:
- GPROModel: ì§ì ‘ ì„ í˜¸ë„ ìµœì í™” ëª¨ë¸
- OpenAIValidator: OpenAI API ê¸°ë°˜ ê²€ì¦ì
- HumanFeedbackSimulator: ì¸ê°„ í”¼ë“œë°± ì‹œë®¬ë ˆì´ì…˜
- PreferenceOptimizer: ì„ í˜¸ë„ ìµœì í™” ì—”ì§„
- ComparisonStudy: BERT vs GPRO ì„±ëŠ¥ ë¹„êµ
"""

from .gpro_model import GPROModel, GPROConfig
from .openai_validator import OpenAIValidator, ValidationResult
from .human_feedback import HumanFeedbackSimulator, FeedbackData
from .preference_optimizer import PreferenceOptimizer, OptimizationConfig
from .comparison_study import ModelComparison, PerformanceMetrics

__version__ = "1.0.0"
__author__ = "BarionLabs"

__all__ = [
    # Core Models
    "GPROModel",
    "GPROConfig", 
    
    # Validation System
    "OpenAIValidator",
    "ValidationResult",
    
    # Human Feedback
    "HumanFeedbackSimulator", 
    "FeedbackData",
    
    # Optimization
    "PreferenceOptimizer",
    "OptimizationConfig",
    
    # Comparison & Evaluation
    "ModelComparison",
    "PerformanceMetrics"
]

# ì—°êµ¬ í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
RESEARCH_INFO = {
    "project_name": "TRAS-GPRO: Government Talent Recommendation with Human Feedback",
    "base_lecture": "section4_rlhf_gpro.md",
    "objective": "Build better-than-BERT classifier using human preference optimization",
    "approach": "Direct Preference Optimization (DPO) with OpenAI validation",
    "expected_improvements": [
        "Better alignment with human values",
        "More interpretable recommendations", 
        "Higher expert satisfaction",
        "Robust preference learning"
    ]
} 
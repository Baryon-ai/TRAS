#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© ì†Œì…œë¯¸ë””ì–´ ì¸ì¬ ì¶”ì²œ ë¶„ì„ ì‹œìŠ¤í…œ
Integrated Social Media Talent Recommendation Analysis System

ì´ë©”ì¼ê³¼ íŠ¸ìœ„í„° ëŒ“ê¸€ì„ AIë¡œ ë¶„ì„í•˜ì—¬ ì •ë¶€ ì¸ì¬ ì¶”ì²œì„ ìë™ ë¶„ë¥˜í•˜ëŠ” ì‹œìŠ¤í…œ

MIT License
Copyright (c) 2025 

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""

import os
import sqlite3
import email
import re
import json
import requests
import csv
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union
import chardet
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import logging
from dataclasses import dataclass
from abc import ABC, abstractmethod

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class AnalysisConfig:
    """ë¶„ì„ ì„¤ì • í´ë˜ìŠ¤"""
    ai_provider: str = "ollama"  # "ollama" ë˜ëŠ” "openai" ë˜ëŠ” "claude"
    ollama_url: str = "http://localhost:11434"
    ollama_model: str = "llama3.2:latest"
    openai_api_key: str = ""
    openai_model: str = "gpt-3.5-turbo"
    claude_api_key: str = ""
    claude_model: str = "claude-3-haiku-20240307"
    max_tokens: int = 1000
    temperature: float = 0.1

@dataclass
class TwitterComment:
    """íŠ¸ìœ„í„° ëŒ“ê¸€ ë°ì´í„° í´ë˜ìŠ¤"""
    comment_id: str
    username: str
    display_name: str
    content: str
    timestamp: str
    likes: int = 0
    retweets: int = 0
    replies: int = 0
    parent_post_id: str = ""
    parent_post_content: str = ""

class AIProvider(ABC):
    """AI ì œê³µì ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def analyze_content(self, content: str, subject: str = "", content_type: str = "email") -> Dict:
        pass
    
    @abstractmethod
    def test_connection(self) -> bool:
        pass

class OllamaProvider(AIProvider):
    """Ollama AI ì œê³µì"""
    
    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.base_url = config.ollama_url
        self.model = config.ollama_model
    
    def test_connection(self) -> bool:
        """Ollama ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def _make_request(self, prompt: str) -> str:
        """Ollama API ìš”ì²­"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.config.temperature,
                    "num_predict": self.config.max_tokens
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json().get("response", "")
            else:
                logger.error(f"Ollama API ì˜¤ë¥˜: {response.status_code}")
                return ""
                
        except Exception as e:
            logger.error(f"Ollama ìš”ì²­ ì‹¤íŒ¨: {e}")
            return ""
    
    def analyze_content(self, content: str, subject: str = "", content_type: str = "email") -> Dict:
        """ì»¨í…ì¸  AI ë¶„ì„ (ì´ë©”ì¼ ë˜ëŠ” íŠ¸ìœ„í„° ëŒ“ê¸€)"""
        
        if content_type == "twitter":
            prompt = f"""
ë‹¤ìŒ íŠ¸ìœ„í„° ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

ëŒ“ê¸€ ë‚´ìš©: {content[:2000]}
ì›ê¸€ ì œëª©/ë‚´ìš©: {subject}

ë¶„ì„ í•­ëª©:
1. ì¶”ì²œì—¬ë¶€: ì´ ëŒ“ê¸€ì´ ì •ë¶€ ì¸ì¬ ì¶”ì²œì¸ì§€ íŒë‹¨ (true/false)
2. ì •ë¶€ì§ì±…: ì–¸ê¸‰ëœ ì •ë¶€ ì§ì±…ëª…ë“¤ì„ ì¶”ì¶œ
3. ì¶”ì²œë‚´ìš©ìš”ì•½: ì¶”ì²œí•˜ëŠ” ì¸ë¬¼ì´ë‚˜ ë³¸ì¸ ì†Œê°œ ë‚´ìš©ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½
4. í•µì‹¬í‚¤ì›Œë“œ: í•™ë ¥, ê²½ë ¥, ì „ë¬¸ë¶„ì•¼ ë“± ì£¼ìš” í‚¤ì›Œë“œ 5ê°œ ì¶”ì¶œ
5. ì¶”ì²œìœ í˜•: "ë³¸ì¸ì§€ì›", "íƒ€ì¸ì¶”ì²œ", "ì˜ê²¬ì œì‹œ" ì¤‘ í•˜ë‚˜
6. ì‹ ë¢°ë„: ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ (1-10ì )

ì‘ë‹µ í˜•ì‹:
{{
  "is_recommendation": true/false,
  "government_positions": ["ì§ì±…1", "ì§ì±…2"],
  "summary": "ì¶”ì²œ/ì§€ì› ë‚´ìš© ìš”ì•½",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"],
  "recommendation_type": "ë³¸ì¸ì§€ì›/íƒ€ì¸ì¶”ì²œ/ì˜ê²¬ì œì‹œ",
  "confidence": ì ìˆ˜
}}

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”:
"""
        else:
            prompt = f"""
ë‹¤ìŒ ì´ë©”ì¼ì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

ì œëª©: {subject}
ë‚´ìš©: {content[:2000]}...

ë¶„ì„ í•­ëª©:
1. ì¶”ì²œì—¬ë¶€: ì´ ì´ë©”ì¼ì´ ì •ë¶€ ì¸ì¬ ì¶”ì²œ ì´ë©”ì¼ì¸ì§€ íŒë‹¨ (true/false)
2. ì •ë¶€ì§ì±…: ì–¸ê¸‰ëœ ì •ë¶€ ì§ì±…ëª…ë“¤ì„ ì¶”ì¶œ
3. ìê¸°ì†Œê°œìš”ì•½: ë°œì‹ ìì˜ ìê¸°ì†Œê°œ ë‚´ìš©ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½
4. í•µì‹¬í‚¤ì›Œë“œ: í•™ë ¥, ê²½ë ¥, ì „ë¬¸ë¶„ì•¼ ë“± ì£¼ìš” í‚¤ì›Œë“œ 5ê°œ ì¶”ì¶œ
5. ì¶”ì²œìœ í˜•: "ë³¸ì¸ì§€ì›", "íƒ€ì¸ì¶”ì²œ", "ì˜ê²¬ì œì‹œ" ì¤‘ í•˜ë‚˜
6. ì‹ ë¢°ë„: ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ (1-10ì )

ì‘ë‹µ í˜•ì‹:
{{
  "is_recommendation": true/false,
  "government_positions": ["ì§ì±…1", "ì§ì±…2"],
  "summary": "ìê¸°ì†Œê°œ ìš”ì•½",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"],
  "recommendation_type": "ë³¸ì¸ì§€ì›/íƒ€ì¸ì¶”ì²œ/ì˜ê²¬ì œì‹œ",
  "confidence": ì ìˆ˜
}}

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”:
"""
        
        response = self._make_request(prompt)
        
        try:
            result = json.loads(response)
            return result
        except json.JSONDecodeError:
            return self._extract_from_text(response, content_type)
    
    def _extract_from_text(self, text: str, content_type: str) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ (JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ)"""
        return {
            "is_recommendation": "ì¶”ì²œ" in text or "recommend" in text.lower() or "ì§€ì›" in text,
            "government_positions": re.findall(r'[ê°€-í£]+(?:ì¥ê´€|ì°¨ê´€|êµ­ì¥|ê³¼ì¥|ì›ì¥)', text),
            "summary": text[:200] + "..." if len(text) > 200 else text,
            "keywords": re.findall(r'[ê°€-í£A-Za-z]+(?:ëŒ€í•™|ê²½ë ¥|ì „ê³µ|ë°•ì‚¬|ì„ì‚¬)', text)[:5],
            "recommendation_type": "ë³¸ì¸ì§€ì›" if content_type == "twitter" else "íƒ€ì¸ì¶”ì²œ",
            "confidence": 5
        }

class OpenAIProvider(AIProvider):
    """OpenAI GPT ì œê³µì"""
    
    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.api_key = config.openai_api_key
        self.model = config.openai_model
    
    def test_connection(self) -> bool:
        """OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸"""
        if not self.api_key:
            return False
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.model,
                "messages": [{"role": "user", "content": "Hello"}],
                "max_tokens": 5
            }
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=10
            )
            
            return response.status_code == 200
        except Exception as e:
            logger.error(f"OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def analyze_content(self, content: str, subject: str = "", content_type: str = "email") -> Dict:
        """OpenAIë¥¼ ì´ìš©í•œ ì»¨í…ì¸  ë¶„ì„"""
        
        if content_type == "twitter":
            prompt = f"""
ë‹¤ìŒ íŠ¸ìœ„í„° ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

ëŒ“ê¸€ ë‚´ìš©: {content}
ì›ê¸€ ë‚´ìš©: {subject}

ë¶„ì„ í•­ëª©:
1. ì¶”ì²œì—¬ë¶€: ì´ ëŒ“ê¸€ì´ ì •ë¶€ ì¸ì¬ ì¶”ì²œì¸ì§€ íŒë‹¨
2. ì •ë¶€ì§ì±…: ì–¸ê¸‰ëœ ì •ë¶€ ì§ì±…ëª…ë“¤ì„ ì¶”ì¶œ
3. ì¶”ì²œë‚´ìš©ìš”ì•½: ì¶”ì²œí•˜ëŠ” ì¸ë¬¼ì´ë‚˜ ë³¸ì¸ ì†Œê°œ ë‚´ìš©ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½
4. í•µì‹¬í‚¤ì›Œë“œ: í•™ë ¥, ê²½ë ¥, ì „ë¬¸ë¶„ì•¼ ë“± ì£¼ìš” í‚¤ì›Œë“œ 5ê°œ ì¶”ì¶œ
5. ì¶”ì²œìœ í˜•: "ë³¸ì¸ì§€ì›", "íƒ€ì¸ì¶”ì²œ", "ì˜ê²¬ì œì‹œ" ì¤‘ í•˜ë‚˜
6. ì‹ ë¢°ë„: ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ (1-10ì )

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "is_recommendation": true/false,
  "government_positions": ["ì§ì±…1", "ì§ì±…2"],
  "summary": "ì¶”ì²œ/ì§€ì› ë‚´ìš© ìš”ì•½",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"],
  "recommendation_type": "ë³¸ì¸ì§€ì›/íƒ€ì¸ì¶”ì²œ/ì˜ê²¬ì œì‹œ",
  "confidence": ì ìˆ˜
}}
"""
        else:
            prompt = f"""
ë‹¤ìŒ ì´ë©”ì¼ì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

ì œëª©: {subject}
ë‚´ìš©: {content}

ë¶„ì„ í•­ëª©:
1. ì¶”ì²œì—¬ë¶€: ì´ ì´ë©”ì¼ì´ ì •ë¶€ ì¸ì¬ ì¶”ì²œ ì´ë©”ì¼ì¸ì§€ íŒë‹¨
2. ì •ë¶€ì§ì±…: ì–¸ê¸‰ëœ ì •ë¶€ ì§ì±…ëª…ë“¤ì„ ì¶”ì¶œ
3. ìê¸°ì†Œê°œìš”ì•½: ë°œì‹ ìì˜ ìê¸°ì†Œê°œ ë‚´ìš©ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½
4. í•µì‹¬í‚¤ì›Œë“œ: í•™ë ¥, ê²½ë ¥, ì „ë¬¸ë¶„ì•¼ ë“± ì£¼ìš” í‚¤ì›Œë“œ 5ê°œ ì¶”ì¶œ
5. ì¶”ì²œìœ í˜•: "ë³¸ì¸ì§€ì›", "íƒ€ì¸ì¶”ì²œ", "ì˜ê²¬ì œì‹œ" ì¤‘ í•˜ë‚˜
6. ì‹ ë¢°ë„: ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ (1-10ì )

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "is_recommendation": true/false,
  "government_positions": ["ì§ì±…1", "ì§ì±…2"],
  "summary": "ìê¸°ì†Œê°œ ìš”ì•½",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"],
  "recommendation_type": "ë³¸ì¸ì§€ì›/íƒ€ì¸ì¶”ì²œ/ì˜ê²¬ì œì‹œ",
  "confidence": ì ìˆ˜
}}
"""
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": self.config.max_tokens,
                "temperature": self.config.temperature
            }
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                return json.loads(content)
            else:
                logger.error(f"OpenAI API ì˜¤ë¥˜: {response.status_code}")
                return self._default_analysis()
                
        except Exception as e:
            logger.error(f"OpenAI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._default_analysis()
    
    def _default_analysis(self) -> Dict:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼"""
        return {
            "is_recommendation": False,
            "government_positions": [],
            "summary": "ë¶„ì„ ì‹¤íŒ¨",
            "keywords": [],
            "recommendation_type": "ì˜ê²¬ì œì‹œ",
            "confidence": 1
        }

class ClaudeProvider(AIProvider):
    """Anthropic Claude ì œê³µì"""
    
    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.api_key = config.claude_api_key
        self.model = config.claude_model
    
    def test_connection(self) -> bool:
        """Claude API ì—°ê²° í…ŒìŠ¤íŠ¸"""
        if not self.api_key:
            return False
        
        try:
            headers = {
                "x-api-key": self.api_key,
                "Content-Type": "application/json",
                "anthropic-version": "2023-06-01"
            }
            
            payload = {
                "model": self.model,
                "max_tokens": 5,
                "messages": [{"role": "user", "content": "Hello"}]
            }
            
            response = requests.post(
                "https://api.anthropic.com/v1/messages",
                headers=headers,
                json=payload,
                timeout=10
            )
            
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Claude ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def analyze_content(self, content: str, subject: str = "", content_type: str = "email") -> Dict:
        """Claudeë¥¼ ì´ìš©í•œ ì»¨í…ì¸  ë¶„ì„"""
        
        if content_type == "twitter":
            prompt = f"""
ë‹¤ìŒ íŠ¸ìœ„í„° ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

ëŒ“ê¸€ ë‚´ìš©: {content}
ì›ê¸€ ë‚´ìš©: {subject}

ë¶„ì„ í•­ëª©:
1. ì¶”ì²œì—¬ë¶€: ì´ ëŒ“ê¸€ì´ ì •ë¶€ ì¸ì¬ ì¶”ì²œì¸ì§€ íŒë‹¨
2. ì •ë¶€ì§ì±…: ì–¸ê¸‰ëœ ì •ë¶€ ì§ì±…ëª…ë“¤ì„ ì¶”ì¶œ
3. ì¶”ì²œë‚´ìš©ìš”ì•½: ì¶”ì²œí•˜ëŠ” ì¸ë¬¼ì´ë‚˜ ë³¸ì¸ ì†Œê°œ ë‚´ìš©ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½
4. í•µì‹¬í‚¤ì›Œë“œ: í•™ë ¥, ê²½ë ¥, ì „ë¬¸ë¶„ì•¼ ë“± ì£¼ìš” í‚¤ì›Œë“œ 5ê°œ ì¶”ì¶œ
5. ì¶”ì²œìœ í˜•: "ë³¸ì¸ì§€ì›", "íƒ€ì¸ì¶”ì²œ", "ì˜ê²¬ì œì‹œ" ì¤‘ í•˜ë‚˜
6. ì‹ ë¢°ë„: ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ (1-10ì )

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "is_recommendation": true/false,
  "government_positions": ["ì§ì±…1", "ì§ì±…2"],
  "summary": "ì¶”ì²œ/ì§€ì› ë‚´ìš© ìš”ì•½",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"],
  "recommendation_type": "ë³¸ì¸ì§€ì›/íƒ€ì¸ì¶”ì²œ/ì˜ê²¬ì œì‹œ",
  "confidence": ì ìˆ˜
}}
"""
        else:
            prompt = f"""
ë‹¤ìŒ ì´ë©”ì¼ì„ ë¶„ì„í•˜ì—¬ JSON í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

ì œëª©: {subject}
ë‚´ìš©: {content}

ë¶„ì„ í•­ëª©:
1. ì¶”ì²œì—¬ë¶€: ì´ ì´ë©”ì¼ì´ ì •ë¶€ ì¸ì¬ ì¶”ì²œ ì´ë©”ì¼ì¸ì§€ íŒë‹¨
2. ì •ë¶€ì§ì±…: ì–¸ê¸‰ëœ ì •ë¶€ ì§ì±…ëª…ë“¤ì„ ì¶”ì¶œ
3. ìê¸°ì†Œê°œìš”ì•½: ë°œì‹ ìì˜ ìê¸°ì†Œê°œ ë‚´ìš©ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½
4. í•µì‹¬í‚¤ì›Œë“œ: í•™ë ¥, ê²½ë ¥, ì „ë¬¸ë¶„ì•¼ ë“± ì£¼ìš” í‚¤ì›Œë“œ 5ê°œ ì¶”ì¶œ
5. ì¶”ì²œìœ í˜•: "ë³¸ì¸ì§€ì›", "íƒ€ì¸ì¶”ì²œ", "ì˜ê²¬ì œì‹œ" ì¤‘ í•˜ë‚˜
6. ì‹ ë¢°ë„: ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ (1-10ì )

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "is_recommendation": true/false,
  "government_positions": ["ì§ì±…1", "ì§ì±…2"],
  "summary": "ìê¸°ì†Œê°œ ìš”ì•½",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"],
  "recommendation_type": "ë³¸ì¸ì§€ì›/íƒ€ì¸ì¶”ì²œ/ì˜ê²¬ì œì‹œ",
  "confidence": ì ìˆ˜
}}
"""
        
        try:
            headers = {
                "x-api-key": self.api_key,
                "Content-Type": "application/json",
                "anthropic-version": "2023-06-01"
            }
            
            payload = {
                "model": self.model,
                "max_tokens": self.config.max_tokens,
                "messages": [{"role": "user", "content": prompt}]
            }
            
            response = requests.post(
                "https://api.anthropic.com/v1/messages",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["content"][0]["text"]
                return json.loads(content)
            else:
                logger.error(f"Claude API ì˜¤ë¥˜: {response.status_code}")
                return self._default_analysis()
                
        except Exception as e:
            logger.error(f"Claude ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._default_analysis()
    
    def _default_analysis(self) -> Dict:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼"""
        return {
            "is_recommendation": False,
            "government_positions": [],
            "summary": "ë¶„ì„ ì‹¤íŒ¨",
            "keywords": [],
            "recommendation_type": "ì˜ê²¬ì œì‹œ",
            "confidence": 1
        }

class IntegratedTalentAnalyzer:
    """í†µí•© ì¸ì¬ ì¶”ì²œ ë¶„ì„ê¸° (ì´ë©”ì¼ + íŠ¸ìœ„í„°)"""
    
    def __init__(self, db_path: str = "integrated_talent_analysis.db", 
                 email_folder: str = "emails", twitter_folder: str = "twitter_data"):
        self.db_path = db_path
        self.email_folder = email_folder
        self.twitter_folder = twitter_folder
        self.config = self._load_config()
        self.ai_provider = self._initialize_ai_provider()
        self.init_database()
    
    def _load_config(self) -> AnalysisConfig:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        config_file = "ai_config.json"
        
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                    return AnalysisConfig(**config_data)
            except Exception as e:
                logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        config = AnalysisConfig()
        self._save_config(config)
        return config
    
    def _save_config(self, config: AnalysisConfig):
        """ì„¤ì • íŒŒì¼ ì €ì¥"""
        config_data = {
            "ai_provider": config.ai_provider,
            "ollama_url": config.ollama_url,
            "ollama_model": config.ollama_model,
            "openai_api_key": config.openai_api_key,
            "openai_model": config.openai_model,
            "claude_api_key": config.claude_api_key,
            "claude_model": config.claude_model,
            "max_tokens": config.max_tokens,
            "temperature": config.temperature
        }
        
        with open("ai_config.json", 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
    
    def _initialize_ai_provider(self) -> AIProvider:
        """AI ì œê³µì ì´ˆê¸°í™”"""
        if self.config.ai_provider == "ollama":
            return OllamaProvider(self.config)
        elif self.config.ai_provider == "openai":
            return OpenAIProvider(self.config)
        elif self.config.ai_provider == "claude":
            return ClaudeProvider(self.config)
        else:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ì œê³µì: {self.config.ai_provider}")
            return OllamaProvider(self.config)
    
    def init_database(self):
        """í†µí•© ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # í†µí•© ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS content_analysis (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                content_type TEXT NOT NULL,  -- 'email' or 'twitter'
                source_file TEXT,
                content_id TEXT,
                username TEXT,
                display_name TEXT,
                is_recommendation INTEGER NOT NULL,
                government_positions TEXT,
                ai_summary TEXT,
                received_date TEXT,
                sender_email TEXT,
                ai_keywords TEXT,
                recommendation_type TEXT,  -- 'ë³¸ì¸ì§€ì›', 'íƒ€ì¸ì¶”ì²œ', 'ì˜ê²¬ì œì‹œ'
                confidence_score INTEGER,
                ai_provider TEXT,
                likes_count INTEGER DEFAULT 0,
                retweets_count INTEGER DEFAULT 0,
                replies_count INTEGER DEFAULT 0,
                parent_post_content TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("í†µí•© ë¶„ì„ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_twitter_data_from_csv(self, csv_file: str) -> List[TwitterComment]:
        """CSV íŒŒì¼ì—ì„œ íŠ¸ìœ„í„° ë°ì´í„° ë¡œë“œ"""
        twitter_comments = []
        
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    comment = TwitterComment(
                        comment_id=row.get('comment_id', ''),
                        username=row.get('username', ''),
                        display_name=row.get('display_name', ''),
                        content=row.get('content', ''),
                        timestamp=row.get('timestamp', ''),
                        likes=int(row.get('likes', 0)),
                        retweets=int(row.get('retweets', 0)),
                        replies=int(row.get('replies', 0)),
                        parent_post_id=row.get('parent_post_id', ''),
                        parent_post_content=row.get('parent_post_content', '')
                    )
                    twitter_comments.append(comment)
            
            logger.info(f"CSVì—ì„œ {len(twitter_comments)}ê°œì˜ íŠ¸ìœ„í„° ëŒ“ê¸€ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"CSV ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        return twitter_comments
    
    def load_twitter_data_from_json(self, json_file: str) -> List[TwitterComment]:
        """JSON íŒŒì¼ì—ì„œ íŠ¸ìœ„í„° ë°ì´í„° ë¡œë“œ"""
        twitter_comments = []
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # JSON êµ¬ì¡°ì— ë”°ë¼ íŒŒì‹± (ì˜ˆì‹œ)
            if isinstance(data, list):
                for item in data:
                    comment = TwitterComment(
                        comment_id=item.get('id', ''),
                        username=item.get('user', {}).get('username', ''),
                        display_name=item.get('user', {}).get('name', ''),
                        content=item.get('text', ''),
                        timestamp=item.get('created_at', ''),
                        likes=item.get('public_metrics', {}).get('like_count', 0),
                        retweets=item.get('public_metrics', {}).get('retweet_count', 0),
                        replies=item.get('public_metrics', {}).get('reply_count', 0),
                        parent_post_content=item.get('referenced_tweets', [{}])[0].get('text', '') if item.get('referenced_tweets') else ''
                    )
                    twitter_comments.append(comment)
            
            logger.info(f"JSONì—ì„œ {len(twitter_comments)}ê°œì˜ íŠ¸ìœ„í„° ëŒ“ê¸€ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"JSON ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        return twitter_comments
    
    def analyze_twitter_comment(self, comment: TwitterComment) -> Optional[Dict]:
        """íŠ¸ìœ„í„° ëŒ“ê¸€ AI ë¶„ì„"""
        logger.info(f"íŠ¸ìœ„í„° ëŒ“ê¸€ AI ë¶„ì„ ì‹œì‘: @{comment.username}")
        
        # AI ë¶„ì„ ìˆ˜í–‰
        ai_result = self.ai_provider.analyze_content(
            content=comment.content,
            subject=comment.parent_post_content,
            content_type="twitter"
        )
        
        return {
            'content_type': 'twitter',
            'source_file': '',
            'content_id': comment.comment_id,
            'username': comment.username,
            'display_name': comment.display_name,
            'is_recommendation': 1 if ai_result.get('is_recommendation', False) else 0,
            'government_positions': ', '.join(ai_result.get('government_positions', [])),
            'ai_summary': ai_result.get('summary', ''),
            'received_date': comment.timestamp,
            'sender_email': '',
            'ai_keywords': ', '.join(ai_result.get('keywords', [])),
            'recommendation_type': ai_result.get('recommendation_type', 'ì˜ê²¬ì œì‹œ'),
            'confidence_score': ai_result.get('confidence', 0),
            'ai_provider': self.config.ai_provider,
            'likes_count': comment.likes,
            'retweets_count': comment.retweets,
            'replies_count': comment.replies,
            'parent_post_content': comment.parent_post_content
        }
    
    def detect_encoding(self, file_path: str) -> str:
        """íŒŒì¼ ì¸ì½”ë”© ê°ì§€"""
        with open(file_path, 'rb') as f:
            raw_data = f.read()
            result = chardet.detect(raw_data)
            return result['encoding'] or 'utf-8'
    
    def parse_email_file(self, file_path: str) -> Optional[email.message.Message]:
        """ì´ë©”ì¼ íŒŒì¼ íŒŒì‹±"""
        try:
            encoding = self.detect_encoding(file_path)
            with open(file_path, 'r', encoding=encoding) as f:
                return email.message_from_file(f)
        except Exception as e:
            logger.error(f"ì´ë©”ì¼ íŒŒì‹± ì˜¤ë¥˜ ({file_path}): {e}")
            return None
    
    def extract_email_content(self, msg: email.message.Message) -> str:
        """ì´ë©”ì¼ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        content = ""
        
        if msg.is_multipart():
            for part in msg.walk():
                if part.get_content_type() == "text/plain":
                    payload = part.get_payload(decode=True)
                    if payload:
                        try:
                            content += payload.decode('utf-8', errors='ignore')
                        except:
                            content += str(payload)
        else:
            payload = msg.get_payload(decode=True)
            if payload:
                try:
                    content = payload.decode('utf-8', errors='ignore')
                except:
                    content = str(payload)
        
        return content.strip()
    
    def parse_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        if not date_str:
            return ""
        
        try:
            parsed_date = email.utils.parsedate_to_datetime(date_str)
            return parsed_date.strftime('%Y-%m-%d %H:%M:%S')
        except:
            return date_str
    
    def extract_sender_info(self, msg: email.message.Message) -> Tuple[str, str]:
        """ë°œì‹ ì ì •ë³´ ì¶”ì¶œ"""
        from_header = msg.get('From', '')
        
        email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
        email_match = re.search(email_pattern, from_header)
        sender_email = email_match.group(1) if email_match else ''
        
        sender_name = from_header.replace(f'<{sender_email}>', '').strip()
        sender_name = sender_name.replace('"', '').strip()
        
        return sender_name, sender_email
    
    def analyze_email_with_ai(self, file_path: str) -> Optional[Dict]:
        """AIë¥¼ ì´ìš©í•œ ì´ë©”ì¼ ë¶„ì„"""
        msg = self.parse_email_file(file_path)
        if not msg:
            return None
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        subject = msg.get('Subject', '')
        content = self.extract_email_content(msg)
        received_date = self.parse_date(msg.get('Date', ''))
        sender_name, sender_email = self.extract_sender_info(msg)
        
        # AI ë¶„ì„ ìˆ˜í–‰
        logger.info(f"ì´ë©”ì¼ AI ë¶„ì„ ì‹œì‘: {os.path.basename(file_path)}")
        ai_result = self.ai_provider.analyze_content(content, subject, "email")
        
        return {
            'content_type': 'email',
            'source_file': os.path.basename(file_path),
            'content_id': '',
            'username': sender_name,
            'display_name': sender_name,
            'is_recommendation': 1 if ai_result.get('is_recommendation', False) else 0,
            'government_positions': ', '.join(ai_result.get('government_positions', [])),
            'ai_summary': ai_result.get('summary', ''),
            'received_date': received_date,
            'sender_email': sender_email,
            'ai_keywords': ', '.join(ai_result.get('keywords', [])),
            'recommendation_type': ai_result.get('recommendation_type', 'íƒ€ì¸ì¶”ì²œ'),
            'confidence_score': ai_result.get('confidence', 0),
            'ai_provider': self.config.ai_provider,
            'likes_count': 0,
            'retweets_count': 0,
            'replies_count': 0,
            'parent_post_content': subject
        }
    
    def save_to_database(self, analysis_result: Dict):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO content_analysis 
            (content_type, source_file, content_id, username, display_name,
             is_recommendation, government_positions, ai_summary, received_date,
             sender_email, ai_keywords, recommendation_type, confidence_score,
             ai_provider, likes_count, retweets_count, replies_count, parent_post_content)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            analysis_result['content_type'],
            analysis_result['source_file'],
            analysis_result['content_id'],
            analysis_result['username'],
            analysis_result['display_name'],
            analysis_result['is_recommendation'],
            analysis_result['government_positions'],
            analysis_result['ai_summary'],
            analysis_result['received_date'],
            analysis_result['sender_email'],
            analysis_result['ai_keywords'],
            analysis_result['recommendation_type'],
            analysis_result['confidence_score'],
            analysis_result['ai_provider'],
            analysis_result['likes_count'],
            analysis_result['retweets_count'],
            analysis_result['replies_count'],
            analysis_result['parent_post_content']
        ))
        
        conn.commit()
        conn.close()
    
    def process_all_emails(self):
        """ëª¨ë“  ì´ë©”ì¼ AI ë¶„ì„ ì²˜ë¦¬"""
        if not os.path.exists(self.email_folder):
            logger.warning(f"ì´ë©”ì¼ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.email_folder}")
            return 0
        
        # AI ì—°ê²° í…ŒìŠ¤íŠ¸
        if not self.ai_provider.test_connection():
            logger.error(f"{self.config.ai_provider} ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return 0
        
        email_files = []
        for ext in ['*.eml', '*.msg', '*.txt']:
            email_files.extend(Path(self.email_folder).glob(ext))
        
        logger.info(f"ì´ {len(email_files)}ê°œì˜ ì´ë©”ì¼ íŒŒì¼ì„ AIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
        
        processed = 0
        for file_path in email_files:
            try:
                analysis_result = self.analyze_email_with_ai(str(file_path))
                
                if analysis_result:
                    self.save_to_database(analysis_result)
                    processed += 1
                    logger.info(f"ì´ë©”ì¼ ì™„ë£Œ: {file_path.name} (ì‹ ë¢°ë„: {analysis_result.get('confidence_score', 0)})")
                
            except Exception as e:
                logger.error(f"ì´ë©”ì¼ ë¶„ì„ ì˜¤ë¥˜ ({file_path.name}): {e}")
        
        logger.info(f"ì´ {processed}ê°œì˜ ì´ë©”ì¼ì´ ì„±ê³µì ìœ¼ë¡œ AI ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return processed
    
    def process_all_twitter_data(self):
        """ëª¨ë“  íŠ¸ìœ„í„° ë°ì´í„° AI ë¶„ì„ ì²˜ë¦¬"""
        if not os.path.exists(self.twitter_folder):
            logger.warning(f"íŠ¸ìœ„í„° ë°ì´í„° í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.twitter_folder}")
            return 0
        
        # AI ì—°ê²° í…ŒìŠ¤íŠ¸
        if not self.ai_provider.test_connection():
            logger.error(f"{self.config.ai_provider} ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return 0
        
        # CSV ë° JSON íŒŒì¼ ì°¾ê¸°
        data_files = []
        data_files.extend(Path(self.twitter_folder).glob('*.csv'))
        data_files.extend(Path(self.twitter_folder).glob('*.json'))
        
        if not data_files:
            logger.warning("ë¶„ì„í•  íŠ¸ìœ„í„° ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        logger.info(f"ì´ {len(data_files)}ê°œì˜ íŠ¸ìœ„í„° ë°ì´í„° íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        logger.info(f"ì‚¬ìš©ì¤‘ì¸ AI: {self.config.ai_provider}")
        
        total_processed = 0
        
        for data_file in data_files:
            try:
                logger.info(f"íŠ¸ìœ„í„° ë°ì´í„° ë¡œë“œ ì¤‘: {data_file.name}")
                
                # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ë¡œë“œ
                if data_file.suffix.lower() == '.csv':
                    twitter_comments = self.load_twitter_data_from_csv(str(data_file))
                elif data_file.suffix.lower() == '.json':
                    twitter_comments = self.load_twitter_data_from_json(str(data_file))
                else:
                    continue
                
                # ê° ëŒ“ê¸€ ë¶„ì„
                processed = 0
                for comment in twitter_comments:
                    try:
                        analysis_result = self.analyze_twitter_comment(comment)
                        
                        if analysis_result:
                            self.save_to_database(analysis_result)
                            processed += 1
                            total_processed += 1
                            
                            logger.info(f"íŠ¸ìœ„í„° ì™„ë£Œ: @{comment.username} (ì‹ ë¢°ë„: {analysis_result.get('confidence_score', 0)})")
                    
                    except Exception as e:
                        logger.error(f"íŠ¸ìœ„í„° ëŒ“ê¸€ ë¶„ì„ ì˜¤ë¥˜ (@{comment.username}): {e}")
                
                logger.info(f"{data_file.name}ì—ì„œ {processed}ê°œ ëŒ“ê¸€ ë¶„ì„ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({data_file.name}): {e}")
        
        logger.info(f"ì´ {total_processed}ê°œì˜ íŠ¸ìœ„í„° ëŒ“ê¸€ì´ ì„±ê³µì ìœ¼ë¡œ AI ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return total_processed
    
    def configure_ai_settings(self):
        """AI ì„¤ì • êµ¬ì„±"""
        print("\n=== AI ì„¤ì • êµ¬ì„± ===")
        print("1. Ollama (ë¡œì»¬ AI)")
        print("2. OpenAI GPT")
        print("3. Claude")
        
        choice = input("AI ì œê³µì ì„ íƒ (1-3): ").strip()
        
        if choice == '1':
            self.config.ai_provider = "ollama"
            self.config.ollama_url = input(f"Ollama URL [{self.config.ollama_url}]: ").strip() or self.config.ollama_url
            self.config.ollama_model = input(f"Ollama ëª¨ë¸ [{self.config.ollama_model}]: ").strip() or self.config.ollama_model
            
        elif choice == '2':
            self.config.ai_provider = "openai"
            api_key = input("OpenAI API í‚¤: ").strip()
            if api_key:
                self.config.openai_api_key = api_key
            self.config.openai_model = input(f"OpenAI ëª¨ë¸ [{self.config.openai_model}]: ").strip() or self.config.openai_model
            
        elif choice == '3':
            self.config.ai_provider = "claude"
            api_key = input("Claude API í‚¤: ").strip()
            if api_key:
                self.config.claude_api_key = api_key
            self.config.claude_model = input(f"Claude ëª¨ë¸ [{self.config.claude_model}]: ").strip() or self.config.claude_model
        
        # ê³µí†µ ì„¤ì •
        temp_input = input(f"Temperature (0.0-1.0) [{self.config.temperature}]: ").strip()
        if temp_input:
            try:
                self.config.temperature = float(temp_input)
            except ValueError:
                pass
        
        tokens_input = input(f"Max Tokens [{self.config.max_tokens}]: ").strip()
        if tokens_input:
            try:
                self.config.max_tokens = int(tokens_input)
            except ValueError:
                pass
        
        # ì„¤ì • ì €ì¥
        self._save_config(self.config)
        
        # AI ì œê³µì ì¬ì´ˆê¸°í™”
        self.ai_provider = self._initialize_ai_provider()
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        if self.ai_provider.test_connection():
            print(f"âœ… {self.config.ai_provider} ì—°ê²° ì„±ê³µ!")
        else:
            print(f"âŒ {self.config.ai_provider} ì—°ê²° ì‹¤íŒ¨!")
    
    def get_integrated_statistics(self) -> Dict:
        """í†µí•© ë¶„ì„ í†µê³„ ì¡°íšŒ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì „ì²´ í†µê³„
        cursor.execute("SELECT COUNT(*) FROM content_analysis")
        total_content = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM content_analysis WHERE content_type = 'email'")
        total_emails = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM content_analysis WHERE content_type = 'twitter'")
        total_twitter = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM content_analysis WHERE is_recommendation = 1")
        recommendation_content = cursor.fetchone()[0]
        
        cursor.execute("SELECT AVG(confidence_score) FROM content_analysis")
        avg_confidence = cursor.fetchone()[0] or 0
        
        # ì¶”ì²œ ìœ í˜•ë³„ í†µê³„
        cursor.execute("""
            SELECT recommendation_type, COUNT(*) 
            FROM content_analysis 
            WHERE is_recommendation = 1 
            GROUP BY recommendation_type 
            ORDER BY COUNT(*) DESC
        """)
        recommendation_type_stats = cursor.fetchall()
        
        # ì •ë¶€ ì§ì±…ë³„ í†µê³„
        cursor.execute("""
            SELECT government_positions, COUNT(*) 
            FROM content_analysis 
            WHERE government_positions != '' 
            GROUP BY government_positions 
            ORDER BY COUNT(*) DESC
        """)
        position_stats = cursor.fetchall()
        
        # í”Œë«í¼ë³„ ì¶”ì²œ í†µê³„
        cursor.execute("""
            SELECT content_type, 
                   COUNT(*) as total,
                   SUM(is_recommendation) as recommendations
            FROM content_analysis 
            GROUP BY content_type
        """)
        platform_stats = cursor.fetchall()
        
        # íŠ¸ìœ„í„° ì¸ê¸°ë„ í†µê³„ (ì¶”ì²œëœ ëŒ“ê¸€ ì¤‘)
        cursor.execute("""
            SELECT AVG(likes_count), AVG(retweets_count), AVG(replies_count)
            FROM content_analysis 
            WHERE content_type = 'twitter' AND is_recommendation = 1
        """)
        twitter_engagement = cursor.fetchone()
        
        conn.close()
        
        return {
            'total_content': total_content,
            'total_emails': total_emails,
            'total_twitter': total_twitter,
            'recommendation_content': recommendation_content,
            'non_recommendation_content': total_content - recommendation_content,
            'average_confidence': round(avg_confidence, 2),
            'recommendation_type_statistics': recommendation_type_stats,
            'position_statistics': position_stats,
            'platform_statistics': platform_stats,
            'twitter_engagement': {
                'avg_likes': round(twitter_engagement[0] or 0, 1),
                'avg_retweets': round(twitter_engagement[1] or 0, 1),
                'avg_replies': round(twitter_engagement[2] or 0, 1)
            }
        }
    
    def export_to_csv(self, output_file: str = "integrated_analysis_results.csv"):
        """í†µí•© ë¶„ì„ ê²°ê³¼ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT content_type, source_file, content_id, username, display_name,
                   is_recommendation, government_positions, ai_summary, received_date,
                   sender_email, ai_keywords, recommendation_type, confidence_score,
                   ai_provider, likes_count, retweets_count, replies_count,
                   parent_post_content, created_at
            FROM content_analysis
            ORDER BY created_at DESC
        """)
        
        results = cursor.fetchall()
        conn.close()
        
        with open(output_file, 'w', encoding='utf-8-sig') as f:
            f.write("í”Œë«í¼,ì†ŒìŠ¤íŒŒì¼,ì»¨í…ì¸ ID,ì‚¬ìš©ìëª…,í‘œì‹œëª…,ì¶”ì²œì—¬ë¶€,ì •ë¶€ì§ì±…,AIìš”ì•½,ìˆ˜ì‹ ì¼ì,ì´ë©”ì¼,AIí‚¤ì›Œë“œ,ì¶”ì²œìœ í˜•,ì‹ ë¢°ë„,AIì œê³µì,ì¢‹ì•„ìš”,ë¦¬íŠ¸ìœ—,ëŒ“ê¸€ìˆ˜,ì›ê¸€ë‚´ìš©,ì²˜ë¦¬ì¼ì‹œ\n")
            
            for row in results:
                csv_row = []
                for field in row:
                    if field is None:
                        csv_row.append("")
                    else:
                        field_str = str(field).replace('"', '""')
                        if ',' in field_str or '\n' in field_str:
                            csv_row.append(f'"{field_str}"')
                        else:
                            csv_row.append(field_str)
                
                f.write(','.join(csv_row) + '\n')
        
        logger.info(f"í†µí•© ë¶„ì„ ê²°ê³¼ê°€ {output_file}ë¡œ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤.")


def create_sample_twitter_data():
    """ìƒ˜í”Œ íŠ¸ìœ„í„° ë°ì´í„° ìƒì„±"""
    print("\nğŸ¦ ìƒ˜í”Œ íŠ¸ìœ„í„° ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    
    # twitter_data í´ë” ìƒì„±
    os.makedirs("twitter_data", exist_ok=True)
    
    # ìƒ˜í”Œ íŠ¸ìœ„í„° ëŒ“ê¸€ ë°ì´í„°
    sample_twitter_csv = """comment_id,username,display_name,content,timestamp,likes,retweets,replies,parent_post_id,parent_post_content
1001,kim_ai_expert,ê¹€AIì „ë¬¸ê°€,"ì •ë¶€ AI ì •ì±…ê´€ ëª¨ì§‘ì— ì§€ì›í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ì„œìš¸ëŒ€ AIí•™ê³¼ ë°•ì‚¬ê³¼ì •ì´ê³  ì‚¼ì„±ì—ì„œ 3ë…„ê°„ AI ì—°êµ¬í–ˆìŠµë‹ˆë‹¤. #ì •ë¶€ì±„ìš© #AIì •ì±…",2025-06-13 10:30:00,15,3,2,post_001,"ì •ë¶€ì—ì„œ AI ì •ì±…ê´€ì„ ëª¨ì§‘í•©ë‹ˆë‹¤. ìê²©ì„ ê°–ì¶˜ ë¶„ë“¤ì˜ ë§ì€ ì§€ì› ë°”ëë‹ˆë‹¤."
1002,lee_professor,ì´êµìˆ˜,"ìš°ë¦¬ ì—°êµ¬ì‹¤ ë°•ì‚¬ê³¼ì • ë°•ì˜ìˆ˜ë‹˜ì„ ì¶”ì²œí•©ë‹ˆë‹¤. í™˜ê²½ê³µí•™ ì „ê³µìœ¼ë¡œ ê¸°í›„ë³€í™” ì •ì±… ì—°êµ¬ì— íƒì›”í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.",2025-06-13 11:15:00,28,5,1,post_002,"í™˜ê²½ë¶€ ê¸°í›„ë³€í™” ë‹´ë‹¹ê´€ ëª¨ì§‘ ê³µê³ "
1003,ordinary_citizen,ì¼ë°˜ì‹œë¯¼,"ì •ë¶€ì—ì„œ ì¼í•˜ëŠ” ê²ƒë„ ì¢‹ê² ë„¤ìš”. ê·¸ëŸ°ë° ìê²© ìš”ê±´ì´ ë„ˆë¬´ ê¹Œë‹¤ë¡œìš´ ê²ƒ ê°™ì•„ìš”.",2025-06-13 12:00:00,3,0,1,post_001,"ì •ë¶€ì—ì„œ AI ì •ì±…ê´€ì„ ëª¨ì§‘í•©ë‹ˆë‹¤. ìê²©ì„ ê°–ì¶˜ ë¶„ë“¤ì˜ ë§ì€ ì§€ì› ë°”ëë‹ˆë‹¤."
1004,data_scientist,ë°ì´í„°ê³¼í•™ìë°•ì‚¬,"ë¹…ë°ì´í„° ë¶„ì„ ê²½í—˜ 10ë…„, ì •ë¶€ ìë¬¸ìœ„ì› 3ë…„ ê²½ë ¥ ìˆìŠµë‹ˆë‹¤. ë””ì§€í„¸ì •ì±…ê´€ ì§€ì› í¬ë§í•©ë‹ˆë‹¤.",2025-06-13 14:20:00,12,2,0,post_003,"ë””ì§€í„¸ì •ë¶€ í˜ì‹ ì„ ìœ„í•œ ì •ì±…ê´€ ëª¨ì§‘"
1005,skeptical_user,íšŒì˜ì ì‚¬ìš©ì,"ë˜ ì •ë¶€ ì¼ìë¦¬ ë§Œë“¤ê¸°ë„¤ìš”. ì‹¤ì œë¡œ ì„±ê³¼ê°€ ìˆì„ê¹Œìš”?",2025-06-13 15:45:00,1,0,3,post_003,"ë””ì§€í„¸ì •ë¶€ í˜ì‹ ì„ ìœ„í•œ ì •ì±…ê´€ ëª¨ì§‘"
"""
    
    # CSV íŒŒì¼ ìƒì„±
    csv_file = os.path.join("twitter_data", "sample_twitter_comments.csv")
    with open(csv_file, 'w', encoding='utf-8') as f:
        f.write(sample_twitter_csv)
    
    # JSON íŒŒì¼ë„ ìƒì„±
    sample_twitter_json = [
        {
            "id": "1006",
            "user": {
                "username": "startup_ceo",
                "name": "ìŠ¤íƒ€íŠ¸ì—…ëŒ€í‘œ"
            },
            "text": "ì •ë¶€ ìŠ¤íƒ€íŠ¸ì—… ì •ì±…ê´€ì— ì§€ì›í•˜ê² ìŠµë‹ˆë‹¤. í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… 5ê°œ ì°½ì—… ê²½í—˜ê³¼ ì •ë¶€ R&D ê³¼ì œ ìˆ˜í–‰ ê²½í—˜ ìˆìŠµë‹ˆë‹¤.",
            "created_at": "2025-06-13T16:30:00Z",
            "public_metrics": {
                "like_count": 20,
                "retweet_count": 4,
                "reply_count": 2
            },
            "referenced_tweets": [
                {
                    "text": "ì •ë¶€ì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ í™œì„±í™”ë¥¼ ìœ„í•œ ì •ì±…ê´€ì„ ëª¨ì§‘í•©ë‹ˆë‹¤."
                }
            ]
        },
        {
            "id": "1007",
            "user": {
                "username": "policy_researcher",
                "name": "ì •ì±…ì—°êµ¬ì›"
            },
            "text": "êµ­ì •ì› ì¶œì‹  ë™ë£Œ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. ë³´ì•ˆ ì •ì±… ì „ë¬¸ê°€ë¡œ ì‚¬ì´ë²„ë³´ì•ˆ ë‹´ë‹¹ê´€ì— ì í•©í•©ë‹ˆë‹¤.",
            "created_at": "2025-06-13T17:15:00Z",
            "public_metrics": {
                "like_count": 8,
                "retweet_count": 1,
                "reply_count": 0
            },
            "referenced_tweets": [
                {
                    "text": "êµ­ê°€ ì‚¬ì´ë²„ë³´ì•ˆ ê°•í™”ë¥¼ ìœ„í•œ ì „ë¬¸ ë‹´ë‹¹ê´€ì„ ëª¨ì§‘í•©ë‹ˆë‹¤."
                }
            ]
        }
    ]
    
    json_file = os.path.join("twitter_data", "sample_twitter_comments.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(sample_twitter_json, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… CSV íŒŒì¼ ìƒì„±: {csv_file}")
    print(f"âœ… JSON íŒŒì¼ ìƒì„±: {json_file}")
    print(f"\nì´ 7ê°œì˜ ìƒ˜í”Œ íŠ¸ìœ„í„° ëŒ“ê¸€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ì´ì œ 'íŠ¸ìœ„í„° ëŒ“ê¸€ ë¶„ì„ ì‹¤í–‰' ë©”ë‰´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•´ë³´ì„¸ìš”!")


def create_sample_emails():
    """ìƒ˜í”Œ ì´ë©”ì¼ íŒŒì¼ ìƒì„±"""
    print("\nğŸ“§ ìƒ˜í”Œ ì´ë©”ì¼ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    
    # emails í´ë” ìƒì„±
    os.makedirs("emails", exist_ok=True)
    
    # ìƒ˜í”Œ ì´ë©”ì¼ ë°ì´í„°
    sample_emails = [
        {
            "filename": "recommendation_001.eml",
            "content": """From: ê¹€ì² ìˆ˜ <kim.cs@university.ac.kr>
To: talent@government.go.kr
Subject: ì •ë¶€ AI ì •ì±…ê´€ ì¶”ì²œ
Date: Mon, 13 Jun 2025 10:30:00 +0900

ì•ˆë…•í•˜ì„¸ìš”.

ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ ê¹€ì² ìˆ˜ êµìˆ˜ì…ë‹ˆë‹¤.
ì •ë¶€ì˜ AI ì •ì±…ê´€ ì§ì±…ì— ì €í¬ ì—°êµ¬ì‹¤ ë°•ì‚¬ê³¼ì • ì´ì˜í¬ ë‹˜ì„ ì¶”ì²œí•˜ê³ ì í•©ë‹ˆë‹¤.

ì´ì˜í¬ ë‹˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ìš°ìˆ˜í•œ ê²½ë ¥ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤:
- ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ ë°•ì‚¬ê³¼ì • (AI ì „ê³µ)
- ì‚¼ì„±ì „ì AI ì—°êµ¬ì†Œ ì¸í„´ 3ë…„ ê²½ë ¥
- ì •ë¶€ ë¹…ë°ì´í„° ì •ì±… ìë¬¸ìœ„ì› í™œë™
- AI ìœ¤ë¦¬ ê´€ë ¨ ë…¼ë¬¸ 10í¸ ë°œí‘œ
- í•œêµ­ì •ë³´ê³¼í•™íšŒ ìš°ìˆ˜ë…¼ë¬¸ìƒ ìˆ˜ìƒ

íŠ¹íˆ AI ì •ì±… ìˆ˜ë¦½ê³¼ ê¸°ìˆ ì  ì´í•´ë¥¼ ë™ì‹œì— ê°–ì¶˜ ì¸ì¬ë¡œì„œ ì •ë¶€ AI ì •ì±…ê´€ ì—…ë¬´ì— ë§¤ìš° ì í•©í•˜ë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤.

ì—°ë½ì²˜: 010-1234-5678
ì´ë©”ì¼: lee.yh@snu.ac.kr

ê°ì‚¬í•©ë‹ˆë‹¤.

ê¹€ì² ìˆ˜ êµìˆ˜
ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼""",
        },
        {
            "filename": "application_002.eml",
            "content": """From: ë°•ë¯¼ìˆ˜ <park.ms@korea.ac.kr>
To: recruit@ministry.go.kr  
Subject: í™˜ê²½ë¶€ ê¸°í›„ë³€í™” ë‹´ë‹¹ê´€ ì§€ì›
Date: Tue, 14 Jun 2025 14:15:00 +0900

í™˜ê²½ë¶€ ê¸°í›„ë³€í™” ë‹´ë‹¹ê´€ ëª¨ì§‘ì— ì§€ì›í•©ë‹ˆë‹¤.

ì €ëŠ” ê³ ë ¤ëŒ€í•™êµ í™˜ê²½ê³µí•™ê³¼ì—ì„œ ë°•ì‚¬í•™ìœ„ë¥¼ ì·¨ë“í•˜ì˜€ìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì€ ì „ë¬¸ì„±ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤:

í•™ë ¥:
- ê³ ë ¤ëŒ€í•™êµ í™˜ê²½ê³µí•™ê³¼ ë°•ì‚¬ (2023ë…„)
- ì—°ì„¸ëŒ€í•™êµ í™˜ê²½ê³µí•™ê³¼ ì„ì‚¬ (2020ë…„)

ê²½ë ¥:
- í•œêµ­í™˜ê²½ì—°êµ¬ì› ì—°êµ¬ì› 2ë…„
- ê¸°í›„ë³€í™”ì„¼í„° ì„ ì„ì—°êµ¬ì› 1ë…„
- UN ê¸°í›„ë³€í™”í˜‘ì•½ í•œêµ­ ëŒ€í‘œë‹¨ ì°¸ì—¬

ì£¼ìš” ì„±ê³¼:
- ê¸°í›„ë³€í™” ì ì‘ì •ì±… ì—°êµ¬ 5í¸
- íƒ„ì†Œì¤‘ë¦½ ì‹œë‚˜ë¦¬ì˜¤ ê°œë°œ ì°¸ì—¬
- í™˜ê²½ì˜í–¥í‰ê°€ ì „ë¬¸ê°€ ìê²©

ê¸°í›„ë³€í™” ëŒ€ì‘ ì •ì±… ìˆ˜ë¦½ì— ê¸°ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.

ë°•ë¯¼ìˆ˜
ì—°ë½ì²˜: 010-9876-5432""",
        }
    ]
    
    # ìƒ˜í”Œ íŒŒì¼ ìƒì„±
    for sample in sample_emails:
        file_path = os.path.join("emails", sample["filename"])
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(sample["content"])
        print(f"âœ… ìƒì„±ë¨: {sample['filename']}")
    
    print(f"\nì´ {len(sample_emails)}ê°œì˜ ìƒ˜í”Œ ì´ë©”ì¼ì´ 'emails' í´ë”ì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


def install_ollama_guide():
    """Ollama ì„¤ì¹˜ ê°€ì´ë“œ"""
    print("\n" + "="*60)
    print("Ollama ì„¤ì¹˜ ê°€ì´ë“œ (ë¡œì»¬ AI)")
    print("="*60)
    print("""
1. Ollama ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜:
   - Windows/Mac: https://ollama.ai/download
   - Linux: curl -fsSL https://ollama.ai/install.sh | sh

2. í•œêµ­ì–´ ì§€ì› ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:
   ollama pull llama3.2:latest
   ollama pull qwen2:7b
   ollama pull solar:10.7b

3. ì„œë¹„ìŠ¤ ì‹œì‘:
   ollama serve

4. ì—°ê²° í™•ì¸:
   http://localhost:11434

ê¶Œì¥ ëª¨ë¸:
- llama3.2:latest (ì˜ì–´/í•œêµ­ì–´ ê· í˜•)
- qwen2:7b (í•œêµ­ì–´ íŠ¹í™”)  
- solar:10.7b (ê³ ì„±ëŠ¥, ë” í° ë©”ëª¨ë¦¬ í•„ìš”)

ìµœì†Œ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­:
- RAM: 8GB ì´ìƒ
- GPU: ì„ íƒì‚¬í•­ (NVIDIA GPU ê¶Œì¥)
""")


def twitter_data_format_guide():
    """íŠ¸ìœ„í„° ë°ì´í„° í˜•ì‹ ê°€ì´ë“œ"""
    print("\n" + "="*70)
    print("ğŸ¦ íŠ¸ìœ„í„° ë°ì´í„° í˜•ì‹ ê°€ì´ë“œ")
    print("="*70)
    print("""
ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹:
1. CSV íŒŒì¼ (.csv)
2. JSON íŒŒì¼ (.json)

=== CSV íŒŒì¼ í˜•ì‹ ===
í•„ìˆ˜ ì»¬ëŸ¼:
- comment_id: ëŒ“ê¸€ ê³ ìœ  ID
- username: ì‚¬ìš©ìëª…
- display_name: í‘œì‹œëª…
- content: ëŒ“ê¸€ ë‚´ìš©
- timestamp: ì‘ì„± ì‹œê°„

ì„ íƒ ì»¬ëŸ¼:
- likes: ì¢‹ì•„ìš” ìˆ˜
- retweets: ë¦¬íŠ¸ìœ— ìˆ˜
- replies: ëŒ“ê¸€ ìˆ˜
- parent_post_content: ì›ê¸€ ë‚´ìš©

CSV ì˜ˆì‹œ:
comment_id,username,display_name,content,timestamp,likes,retweets,replies,parent_post_content
1001,user123,ì‚¬ìš©ìëª…,"ëŒ“ê¸€ ë‚´ìš©",2025-06-13 10:30:00,5,1,0,"ì›ë˜ ê¸€ ë‚´ìš©"

=== JSON íŒŒì¼ í˜•ì‹ ===
Twitter API v2 í˜•ì‹ ì§€ì›:
[
  {
    "id": "ëŒ“ê¸€ID",
    "user": {
      "username": "ì‚¬ìš©ìëª…",
      "name": "í‘œì‹œëª…"
    },
    "text": "ëŒ“ê¸€ ë‚´ìš©",
    "created_at": "2025-06-13T10:30:00Z",
    "public_metrics": {
      "like_count": 5,
      "retweet_count": 1,
      "reply_count": 0
    },
    "referenced_tweets": [
      {
        "text": "ì›ë˜ ê¸€ ë‚´ìš©"
      }
    ]
  }
]

=== ë°ì´í„° ìˆ˜ì§‘ ë°©ë²• ===
1. Twitter API ì‚¬ìš©
2. íƒ€ì‚¬ ë„êµ¬ (Tweepy, snscrape ë“±)
3. ìˆ˜ë™ ë°ì´í„° ì…ë ¥

íŒŒì¼ì€ 'twitter_data' í´ë”ì— ì €ì¥í•˜ì„¸ìš”.
""")


def requirements_guide():
    """í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ê°€ì´ë“œ"""
    print("\n" + "="*60)
    print("í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜")
    print("="*60)
    print("""
ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:

pip install requests chardet

ì„ íƒì  ì„¤ì¹˜ (ë” ë‚˜ì€ ì„±ëŠ¥):
pip install python-magic  # íŒŒì¼ íƒ€ì… ê°ì§€ ê°œì„ 
pip install beautifulsoup4  # HTML ì´ë©”ì¼ íŒŒì‹±
pip install pandas  # ë°ì´í„° ë¶„ì„
pip install tweepy  # Twitter API ì ‘ê·¼

ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹:
ì´ë©”ì¼: .eml, .msg, .txt
íŠ¸ìœ„í„°: .csv, .json
""")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ¤–ğŸ¦ í†µí•© ì†Œì…œë¯¸ë””ì–´ ì¸ì¬ ì¶”ì²œ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("Integrated Social Media Talent Recommendation Analysis System")
    print("ì´ë©”ì¼ + íŠ¸ìœ„í„° ëŒ“ê¸€ AI ë¶„ì„")
    print("=" * 80)
    
    # í†µí•© ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = IntegratedTalentAnalyzer()
    
    while True:
        print(f"\ní˜„ì¬ AI ì œê³µì: {analyzer.config.ai_provider.upper()}")
        print("\nğŸ“‹ ë©”ë‰´ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("=" * 50)
        print("ğŸ” ë¶„ì„ ì‹¤í–‰")
        print("1. ğŸ“§ ì´ë©”ì¼ ë¶„ì„ ì‹¤í–‰")
        print("2. ğŸ¦ íŠ¸ìœ„í„° ëŒ“ê¸€ ë¶„ì„ ì‹¤í–‰")
        print("3. ğŸ”„ í†µí•© ë¶„ì„ ì‹¤í–‰ (ì´ë©”ì¼ + íŠ¸ìœ„í„°)")
        print("\nâš™ï¸ ì„¤ì • ë° ê´€ë¦¬")
        print("4. ğŸ¤– AI ì„¤ì • êµ¬ì„±")
        print("5. ğŸ“Š í†µí•© í†µê³„ ì¡°íšŒ")
        print("6. ğŸ“„ ê²°ê³¼ CSV ë‚´ë³´ë‚´ê¸°")
        print("\nğŸ› ï¸ ë„êµ¬ ë° ê°€ì´ë“œ")
        print("7. ğŸ  Ollama ì„¤ì¹˜ ê°€ì´ë“œ")
        print("8. ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ê°€ì´ë“œ")
        print("9. ğŸ¦ íŠ¸ìœ„í„° ë°ì´í„° í˜•ì‹ ê°€ì´ë“œ")
        print("10. ğŸ”Œ AI ì—°ê²° í…ŒìŠ¤íŠ¸")
        print("\nğŸ“ ìƒ˜í”Œ ë°ì´í„°")
        print("11. ğŸ“§ ìƒ˜í”Œ ì´ë©”ì¼ ìƒì„±")
        print("12. ğŸ¦ ìƒ˜í”Œ íŠ¸ìœ„í„° ë°ì´í„° ìƒì„±")
        print("13. ğŸ“Š ì „ì²´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±")
        print("\n14. âŒ ì¢…ë£Œ")
        print("=" * 50)
        
        choice = input("ì„ íƒ (1-14): ").strip()
        
        if choice == '1':
            print(f"\nğŸ“§ {analyzer.config.ai_provider.upper()}ë¥¼ ì´ìš©í•œ ì´ë©”ì¼ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            processed = analyzer.process_all_emails()
            if processed > 0:
                print(f"âœ… {processed}ê°œì˜ ì´ë©”ì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        elif choice == '2':
            print(f"\nğŸ¦ {analyzer.config.ai_provider.upper()}ë¥¼ ì´ìš©í•œ íŠ¸ìœ„í„° ëŒ“ê¸€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            processed = analyzer.process_all_twitter_data()
            if processed > 0:
                print(f"âœ… {processed}ê°œì˜ íŠ¸ìœ„í„° ëŒ“ê¸€ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        elif choice == '3':
            print(f"\nğŸ”„ {analyzer.config.ai_provider.upper()}ë¥¼ ì´ìš©í•œ í†µí•© ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            email_processed = analyzer.process_all_emails()
            twitter_processed = analyzer.process_all_twitter_data()
            total_processed = email_processed + twitter_processed
            if total_processed > 0:
                print(f"âœ… ì´ {total_processed}ê°œ ì½˜í…ì¸ ê°€ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤! (ì´ë©”ì¼: {email_processed}, íŠ¸ìœ„í„°: {twitter_processed})")
            
        elif choice == '4':
            analyzer.configure_ai_settings()
            
        elif choice == '5':
            print("\nğŸ“Š í†µí•© ë¶„ì„ í†µê³„:")
            stats = analyzer.get_integrated_statistics()
            print(f"- ì „ì²´ ë¶„ì„ëœ ì½˜í…ì¸ : {stats['total_content']}ê±´")
            print(f"  â”” ì´ë©”ì¼: {stats['total_emails']}ê±´")
            print(f"  â”” íŠ¸ìœ„í„°: {stats['total_twitter']}ê±´")
            print(f"- AIê°€ ì¶”ì²œìœ¼ë¡œ ë¶„ë¥˜: {stats['recommendation_content']}ê±´")
            print(f"- AIê°€ ì¼ë°˜ìœ¼ë¡œ ë¶„ë¥˜: {stats['non_recommendation_content']}ê±´")
            print(f"- í‰ê·  ì‹ ë¢°ë„: {stats['average_confidence']}/10")
            
            if stats['recommendation_type_statistics']:
                print("\nì¶”ì²œ ìœ í˜•ë³„ í†µê³„:")
                for rec_type, count in stats['recommendation_type_statistics']:
                    print(f"  - {rec_type}: {count}ê±´")
            
            if stats['platform_statistics']:
                print("\ní”Œë«í¼ë³„ ë¶„ì„ í†µê³„:")
                for platform, total, recommendations in stats['platform_statistics']:
                    print(f"  - {platform.upper()}: {total}ê±´ (ì¶”ì²œ: {recommendations}ê±´)")
            
            if stats['twitter_engagement']['avg_likes'] > 0:
                engagement = stats['twitter_engagement']
                print(f"\níŠ¸ìœ„í„° ì¶”ì²œ ëŒ“ê¸€ í‰ê·  ì¸ê¸°ë„:")
                print(f"  - ì¢‹ì•„ìš”: {engagement['avg_likes']}ê°œ")
                print(f"  - ë¦¬íŠ¸ìœ—: {engagement['avg_retweets']}ê°œ")
                print(f"  - ëŒ“ê¸€: {engagement['avg_replies']}ê°œ")
            
            if stats['position_statistics']:
                print("\nì •ë¶€ ì§ì±…ë³„ í†µê³„ (ìƒìœ„ 10ê°œ):")
                for position, count in stats['position_statistics'][:10]:
                    if position:
                        print(f"  - {position}: {count}ê±´")
            
        elif choice == '6':
            output_file = input("CSV íŒŒì¼ëª… (ê¸°ë³¸ê°’: integrated_analysis_results.csv): ").strip()
            if not output_file:
                output_file = "integrated_analysis_results.csv"
            analyzer.export_to_csv(output_file)
            
        elif choice == '7':
            install_ollama_guide()
            
        elif choice == '8':
            requirements_guide()
            
        elif choice == '9':
            twitter_data_format_guide()
            
        elif choice == '10':
            print(f"\nğŸ”Œ {analyzer.config.ai_provider.upper()} ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
            if analyzer.ai_provider.test_connection():
                print("âœ… ì—°ê²° ì„±ê³µ!")
            else:
                print("âŒ ì—°ê²° ì‹¤íŒ¨! ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
        elif choice == '11':
            create_sample_emails()
            
        elif choice == '12':
            create_sample_twitter_data()
            
        elif choice == '13':
            print("\nğŸ“Š ì „ì²´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            create_sample_emails()
            create_sample_twitter_data()
            print("âœ… ëª¨ë“  ìƒ˜í”Œ ë°ì´í„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        elif choice == '14':
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
            break
            
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()